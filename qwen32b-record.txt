INFO 12-14 01:29:08 [__init__.py:36] Available plugins for group vllm.platform_plugins:
INFO 12-14 01:29:08 [__init__.py:38] - ascend -> vllm_ascend:register
INFO 12-14 01:29:08 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-14 01:29:08 [__init__.py:207] Platform plugin ascend is activated
[Rank 0 | Local Rank 0] 2025-12-14 01:29:09,612 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[main] Before run_ppo
ray init kwargs: {'num_cpus': None, 'runtime_env': {'env_vars': {'TOKENIZERS_PARALLELISM': 'true', 'NCCL_DEBUG': 'WARN', 'VLLM_ALLOW_RUNTIME_LORA_UPDATING': 'true', 'NCCL_CUMEM_ENABLE': '0'}, 'working_dir': None}}
[main_ppo] Before ray.get(runner.run.remote)
[36m(pid=2108541)[0m INFO 12-14 01:29:24 [__init__.py:36] Available plugins for group vllm.platform_plugins:
[36m(pid=2108541)[0m INFO 12-14 01:29:24 [__init__.py:38] - ascend -> vllm_ascend:register
[36m(pid=2108541)[0m INFO 12-14 01:29:24 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[36m(pid=2108541)[0m INFO 12-14 01:29:24 [__init__.py:207] Platform plugin ascend is activated
[36m(pid=2108541)[0m [Rank 0 | Local Rank 0] 2025-12-14 01:29:25,117 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(TaskRunner pid=2108541)[0m TaskRunner hostname: liteserver-hps-1f00-00001, PID: 2108541
[36m(TaskRunner pid=2108541)[0m {'actor_rollout_ref': {'actor': {'_target_': 'verl.workers.config.McoreActorConfig',
[36m(TaskRunner pid=2108541)[0m                                  'checkpoint': {'_target_': 'verl.trainer.config.CheckpointConfig',
[36m(TaskRunner pid=2108541)[0m                                                 'async_save': False,
[36m(TaskRunner pid=2108541)[0m                                                 'load_contents': ['model',
[36m(TaskRunner pid=2108541)[0m                                                                   'optimizer',
[36m(TaskRunner pid=2108541)[0m                                                                   'extra'],
[36m(TaskRunner pid=2108541)[0m                                                 'save_contents': ['model',
[36m(TaskRunner pid=2108541)[0m                                                                   'optimizer',
[36m(TaskRunner pid=2108541)[0m                                                                   'extra']},
[36m(TaskRunner pid=2108541)[0m                                  'clip_ratio': 0.2,
[36m(TaskRunner pid=2108541)[0m                                  'clip_ratio_c': 3.0,
[36m(TaskRunner pid=2108541)[0m                                  'clip_ratio_high': 0.2,
[36m(TaskRunner pid=2108541)[0m                                  'clip_ratio_low': 0.2,
[36m(TaskRunner pid=2108541)[0m                                  'data_loader_seed': None,
[36m(TaskRunner pid=2108541)[0m                                  'entropy_coeff': 0,
[36m(TaskRunner pid=2108541)[0m                                  'freeze_vision_tower': False,
[36m(TaskRunner pid=2108541)[0m                                  'kl_loss_coef': 0.001,
[36m(TaskRunner pid=2108541)[0m                                  'kl_loss_type': 'low_var_kl',
[36m(TaskRunner pid=2108541)[0m                                  'load_weight': True,
[36m(TaskRunner pid=2108541)[0m                                  'loss_agg_mode': 'token-mean',
[36m(TaskRunner pid=2108541)[0m                                  'megatron': {'_target_': 'verl.workers.config.McoreEngineConfig',
[36m(TaskRunner pid=2108541)[0m                                               'context_parallel_size': 1,
[36m(TaskRunner pid=2108541)[0m                                               'dist_checkpointing_path': None,
[36m(TaskRunner pid=2108541)[0m                                               'expert_model_parallel_size': 1,
[36m(TaskRunner pid=2108541)[0m                                               'expert_tensor_parallel_size': 1,
[36m(TaskRunner pid=2108541)[0m                                               'forward_only': False,
[36m(TaskRunner pid=2108541)[0m                                               'grad_offload': True,
[36m(TaskRunner pid=2108541)[0m                                               'optimizer_offload': False,
[36m(TaskRunner pid=2108541)[0m                                               'override_ddp_config': {},
[36m(TaskRunner pid=2108541)[0m                                               'override_mcore_model_config': {},
[36m(TaskRunner pid=2108541)[0m                                               'override_transformer_config': {'attention_backend': 'flash',
[36m(TaskRunner pid=2108541)[0m                                                                               'context_parallel_algo': 'megatron_cp_algo',
[36m(TaskRunner pid=2108541)[0m                                                                               'context_parallel_size': 1,
[36m(TaskRunner pid=2108541)[0m                                                                               'cp_window_size': 1,
[36m(TaskRunner pid=2108541)[0m                                                                               'recompute_granularity': 'full',
[36m(TaskRunner pid=2108541)[0m                                                                               'recompute_method': 'uniform',
[36m(TaskRunner pid=2108541)[0m                                                                               'recompute_modules': ['core_attn'],
[36m(TaskRunner pid=2108541)[0m                                                                               'recompute_num_layers': 1,
[36m(TaskRunner pid=2108541)[0m                                                                               'seq_length': 2048,
[36m(TaskRunner pid=2108541)[0m                                                                               'swap_optimizer': True,
[36m(TaskRunner pid=2108541)[0m                                                                               'use_cp_send_recv_overlap': True,
[36m(TaskRunner pid=2108541)[0m                                                                               'use_flash_attn': True,
[36m(TaskRunner pid=2108541)[0m                                                                               'use_fused_ring_attention_update': True,
[36m(TaskRunner pid=2108541)[0m                                                                               'use_fused_rotary_pos_emb': True,
[36m(TaskRunner pid=2108541)[0m                                                                               'use_fused_swiglu': True},
[36m(TaskRunner pid=2108541)[0m                                               'param_offload': True,
[36m(TaskRunner pid=2108541)[0m                                               'pipeline_model_parallel_size': 2,
[36m(TaskRunner pid=2108541)[0m                                               'seed': 42,
[36m(TaskRunner pid=2108541)[0m                                               'sequence_parallel': True,
[36m(TaskRunner pid=2108541)[0m                                               'tensor_model_parallel_size': 8,
[36m(TaskRunner pid=2108541)[0m                                               'use_dist_checkpointing': False,
[36m(TaskRunner pid=2108541)[0m                                               'use_distributed_optimizer': True,
[36m(TaskRunner pid=2108541)[0m                                               'use_mbridge': False,
[36m(TaskRunner pid=2108541)[0m                                               'virtual_pipeline_model_parallel_size': None},
[36m(TaskRunner pid=2108541)[0m                                  'optim': {'_target_': 'verl.workers.config.McoreOptimizerConfig',
[36m(TaskRunner pid=2108541)[0m                                            'betas': [0.9, 0.999],
[36m(TaskRunner pid=2108541)[0m                                            'clip_grad': 10000,
[36m(TaskRunner pid=2108541)[0m                                            'lr': 1e-06,
[36m(TaskRunner pid=2108541)[0m                                            'lr_decay_steps': None,
[36m(TaskRunner pid=2108541)[0m                                            'lr_decay_style': 'constant',
[36m(TaskRunner pid=2108541)[0m                                            'lr_warmup_init': 0.0,
[36m(TaskRunner pid=2108541)[0m                                            'lr_warmup_steps': -1,
[36m(TaskRunner pid=2108541)[0m                                            'lr_warmup_steps_ratio': 0.0,
[36m(TaskRunner pid=2108541)[0m                                            'lr_wsd_decay_steps': None,
[36m(TaskRunner pid=2108541)[0m                                            'lr_wsd_decay_style': 'exponential',
[36m(TaskRunner pid=2108541)[0m                                            'min_lr': 0.0,
[36m(TaskRunner pid=2108541)[0m                                            'optimizer': 'adam',
[36m(TaskRunner pid=2108541)[0m                                            'override_optimizer_config': {},
[36m(TaskRunner pid=2108541)[0m                                            'total_training_steps': -1,
[36m(TaskRunner pid=2108541)[0m                                            'use_checkpoint_opt_param_scheduler': False,
[36m(TaskRunner pid=2108541)[0m                                            'weight_decay': 0.01,
[36m(TaskRunner pid=2108541)[0m                                            'weight_decay_incr_style': 'constant'},
[36m(TaskRunner pid=2108541)[0m                                  'policy_loss': {'_target_': 'verl.workers.config.PolicyLossConfig',
[36m(TaskRunner pid=2108541)[0m                                                  'clip_cov_lb': 1.0,
[36m(TaskRunner pid=2108541)[0m                                                  'clip_cov_ratio': 0.0002,
[36m(TaskRunner pid=2108541)[0m                                                  'clip_cov_ub': 5.0,
[36m(TaskRunner pid=2108541)[0m                                                  'kl_cov_ratio': 0.0002,
[36m(TaskRunner pid=2108541)[0m                                                  'loss_mode': 'vanilla',
[36m(TaskRunner pid=2108541)[0m                                                  'ppo_kl_coef': 0.1},
[36m(TaskRunner pid=2108541)[0m                                  'ppo_epochs': 1,
[36m(TaskRunner pid=2108541)[0m                                  'ppo_max_token_len_per_gpu': 36864,
[36m(TaskRunner pid=2108541)[0m                                  'ppo_micro_batch_size': None,
[36m(TaskRunner pid=2108541)[0m                                  'ppo_micro_batch_size_per_gpu': 1,
[36m(TaskRunner pid=2108541)[0m                                  'ppo_mini_batch_size': 32,
[36m(TaskRunner pid=2108541)[0m                                  'profiler': {'_target_': 'verl.utils.profiler.ProfilerConfig',
[36m(TaskRunner pid=2108541)[0m                                               'all_ranks': False,
[36m(TaskRunner pid=2108541)[0m                                               'enable': False,
[36m(TaskRunner pid=2108541)[0m                                               'ranks': [],
[36m(TaskRunner pid=2108541)[0m                                               'save_path': 'outputs/profile',
[36m(TaskRunner pid=2108541)[0m                                               'tool': None,
[36m(TaskRunner pid=2108541)[0m                                               'tool_config': {'npu': {'_target_': 'verl.utils.profiler.config.NPUToolConfig',
[36m(TaskRunner pid=2108541)[0m                                                                       'analysis': True,
[36m(TaskRunner pid=2108541)[0m                                                                       'contents': [],
[36m(TaskRunner pid=2108541)[0m                                                                       'discrete': False,
[36m(TaskRunner pid=2108541)[0m                                                                       'level': 'level1'},
[36m(TaskRunner pid=2108541)[0m                                                               'nsys': {'_target_': 'verl.utils.profiler.config.NsightToolConfig',
[36m(TaskRunner pid=2108541)[0m                                                                        'discrete': False},
[36m(TaskRunner pid=2108541)[0m                                                               'torch': {'_target_': 'verl.utils.profiler.config.TorchProfilerToolConfig',
[36m(TaskRunner pid=2108541)[0m                                                                         'step_end': None,
[36m(TaskRunner pid=2108541)[0m                                                                         'step_start': 0},
[36m(TaskRunner pid=2108541)[0m                                                               'torch_memory': {'_target_': 'verl.utils.profiler.config.TorchMemoryToolConfig',
[36m(TaskRunner pid=2108541)[0m                                                                                'stack_depth': 32,
[36m(TaskRunner pid=2108541)[0m                                                                                'trace_alloc_max_entries': 100000}}},
[36m(TaskRunner pid=2108541)[0m                                  'recompute_old_log_prob': True,
[36m(TaskRunner pid=2108541)[0m                                  'shuffle': False,
[36m(TaskRunner pid=2108541)[0m                                  'strategy': 'megatron',
[36m(TaskRunner pid=2108541)[0m                                  'use_dynamic_bsz': True,
[36m(TaskRunner pid=2108541)[0m                                  'use_fused_kernels': False,
[36m(TaskRunner pid=2108541)[0m                                  'use_kl_loss': True,
[36m(TaskRunner pid=2108541)[0m                                  'use_torch_compile': True},
[36m(TaskRunner pid=2108541)[0m                        'hybrid_engine': True,
[36m(TaskRunner pid=2108541)[0m                        'model': {'custom_chat_template': None,
[36m(TaskRunner pid=2108541)[0m                                  'external_lib': None,
[36m(TaskRunner pid=2108541)[0m                                  'override_config': {'model_config': {},
[36m(TaskRunner pid=2108541)[0m                                                      'moe_config': {'freeze_moe_router': False}},
[36m(TaskRunner pid=2108541)[0m                                  'path': '/home/data/Qwen3-32B',
[36m(TaskRunner pid=2108541)[0m                                  'trust_remote_code': False,
[36m(TaskRunner pid=2108541)[0m                                  'use_fused_kernels': False,
[36m(TaskRunner pid=2108541)[0m                                  'use_remove_padding': False},
[36m(TaskRunner pid=2108541)[0m                        'nccl_timeout': 600,
[36m(TaskRunner pid=2108541)[0m                        'ref': {'load_weight': True,
[36m(TaskRunner pid=2108541)[0m                                'log_prob_max_token_len_per_gpu': 36864,
[36m(TaskRunner pid=2108541)[0m                                'log_prob_micro_batch_size': None,
[36m(TaskRunner pid=2108541)[0m                                'log_prob_micro_batch_size_per_gpu': 8,
[36m(TaskRunner pid=2108541)[0m                                'log_prob_use_dynamic_bsz': True,
[36m(TaskRunner pid=2108541)[0m                                'megatron': {'_target_': 'verl.workers.config.MegatronEngineConfig',
[36m(TaskRunner pid=2108541)[0m                                             'context_parallel_size': 1,
[36m(TaskRunner pid=2108541)[0m                                             'dist_checkpointing_path': None,
[36m(TaskRunner pid=2108541)[0m                                             'expert_model_parallel_size': 1,
[36m(TaskRunner pid=2108541)[0m                                             'expert_tensor_parallel_size': 1,
[36m(TaskRunner pid=2108541)[0m                                             'forward_only': False,
[36m(TaskRunner pid=2108541)[0m                                             'grad_offload': False,
[36m(TaskRunner pid=2108541)[0m                                             'optimizer_offload': False,
[36m(TaskRunner pid=2108541)[0m                                             'override_ddp_config': {},
[36m(TaskRunner pid=2108541)[0m                                             'override_mcore_model_config': {},
[36m(TaskRunner pid=2108541)[0m                                             'override_transformer_config': {'attention_backend': 'flash',
[36m(TaskRunner pid=2108541)[0m                                                                             'context_parallel_algo': 'megatron_cp_algo',
[36m(TaskRunner pid=2108541)[0m                                                                             'context_parallel_size': 1,
[36m(TaskRunner pid=2108541)[0m                                                                             'cp_window_size': 1,
[36m(TaskRunner pid=2108541)[0m                                                                             'recompute_granularity': 'full',
[36m(TaskRunner pid=2108541)[0m                                                                             'recompute_method': 'uniform',
[36m(TaskRunner pid=2108541)[0m                                                                             'recompute_modules': ['core_attn'],
[36m(TaskRunner pid=2108541)[0m                                                                             'recompute_num_layers': 1,
[36m(TaskRunner pid=2108541)[0m                                                                             'seq_length': 2048,
[36m(TaskRunner pid=2108541)[0m                                                                             'swap_optimizer': True,
[36m(TaskRunner pid=2108541)[0m                                                                             'use_cp_send_recv_overlap': True,
[36m(TaskRunner pid=2108541)[0m                                                                             'use_flash_attn': True,
[36m(TaskRunner pid=2108541)[0m                                                                             'use_fused_ring_attention_update': True,
[36m(TaskRunner pid=2108541)[0m                                                                             'use_fused_rotary_pos_emb': True,
[36m(TaskRunner pid=2108541)[0m                                                                             'use_fused_swiglu': True},
[36m(TaskRunner pid=2108541)[0m                                             'param_offload': True,
[36m(TaskRunner pid=2108541)[0m                                             'pipeline_model_parallel_size': 1,
[36m(TaskRunner pid=2108541)[0m                                             'seed': 42,
[36m(TaskRunner pid=2108541)[0m                                             'sequence_parallel': True,
[36m(TaskRunner pid=2108541)[0m                                             'tensor_model_parallel_size': 1,
[36m(TaskRunner pid=2108541)[0m                                             'use_dist_checkpointing': False,
[36m(TaskRunner pid=2108541)[0m                                             'use_distributed_optimizer': True,
[36m(TaskRunner pid=2108541)[0m                                             'use_mbridge': False,
[36m(TaskRunner pid=2108541)[0m                                             'virtual_pipeline_model_parallel_size': None},
[36m(TaskRunner pid=2108541)[0m                                'profiler': {'_target_': 'verl.utils.profiler.ProfilerConfig',
[36m(TaskRunner pid=2108541)[0m                                             'all_ranks': False,
[36m(TaskRunner pid=2108541)[0m                                             'enable': False,
[36m(TaskRunner pid=2108541)[0m                                             'ranks': [],
[36m(TaskRunner pid=2108541)[0m                                             'save_path': 'outputs/profile',
[36m(TaskRunner pid=2108541)[0m                                             'tool': None,
[36m(TaskRunner pid=2108541)[0m                                             'tool_config': {'npu': {'_target_': 'verl.utils.profiler.config.NPUToolConfig',
[36m(TaskRunner pid=2108541)[0m                                                                     'analysis': True,
[36m(TaskRunner pid=2108541)[0m                                                                     'contents': [],
[36m(TaskRunner pid=2108541)[0m                                                                     'discrete': False,
[36m(TaskRunner pid=2108541)[0m                                                                     'level': 'level1'},
[36m(TaskRunner pid=2108541)[0m                                                             'nsys': {'_target_': 'verl.utils.profiler.config.NsightToolConfig',
[36m(TaskRunner pid=2108541)[0m                                                                      'discrete': False},
[36m(TaskRunner pid=2108541)[0m                                                             'torch': {'_target_': 'verl.utils.profiler.config.TorchProfilerToolConfig',
[36m(TaskRunner pid=2108541)[0m                                                                       'step_end': None,
[36m(TaskRunner pid=2108541)[0m                                                                       'step_start': 0},
[36m(TaskRunner pid=2108541)[0m                                                             'torch_memory': {'_target_': 'verl.utils.profiler.config.TorchMemoryToolConfig',
[36m(TaskRunner pid=2108541)[0m                                                                              'stack_depth': 32,
[36m(TaskRunner pid=2108541)[0m                                                                              'trace_alloc_max_entries': 100000}}},
[36m(TaskRunner pid=2108541)[0m                                'strategy': 'megatron',
[36m(TaskRunner pid=2108541)[0m                                'use_torch_compile': True},
[36m(TaskRunner pid=2108541)[0m                        'rollout': {'_target_': 'verl.workers.config.RolloutConfig',
[36m(TaskRunner pid=2108541)[0m                                    'agent': {'_target_': 'verl.workers.config.AgentLoopConfig',
[36m(TaskRunner pid=2108541)[0m                                              'agent_loop_config_path': None,
[36m(TaskRunner pid=2108541)[0m                                              'custom_async_server': {'_target_': 'verl.workers.config.CustomAsyncServerConfig',
[36m(TaskRunner pid=2108541)[0m                                                                      'name': None,
[36m(TaskRunner pid=2108541)[0m                                                                      'path': None},
[36m(TaskRunner pid=2108541)[0m                                              'default_agent_loop': 'single_turn_agent',
[36m(TaskRunner pid=2108541)[0m                                              'num_workers': 8},
[36m(TaskRunner pid=2108541)[0m                                    'calculate_log_probs': False,
[36m(TaskRunner pid=2108541)[0m                                    'cudagraph_capture_sizes': None,
[36m(TaskRunner pid=2108541)[0m                                    'data_parallel_size': 1,
[36m(TaskRunner pid=2108541)[0m                                    'disable_log_stats': True,
[36m(TaskRunner pid=2108541)[0m                                    'do_sample': True,
[36m(TaskRunner pid=2108541)[0m                                    'dtype': 'bfloat16',
[36m(TaskRunner pid=2108541)[0m                                    'enable_chunked_prefill': True,
[36m(TaskRunner pid=2108541)[0m                                    'enable_prefix_caching': True,
[36m(TaskRunner pid=2108541)[0m                                    'enforce_eager': True,
[36m(TaskRunner pid=2108541)[0m                                    'engine_kwargs': {'sglang': {},
[36m(TaskRunner pid=2108541)[0m                                                      'vllm': {'speculative_config': {'method': 'sam',
[36m(TaskRunner pid=2108541)[0m                                                                                      'num_speculative_tokens': 3}}},
[36m(TaskRunner pid=2108541)[0m                                    'expert_parallel_size': 1,
[36m(TaskRunner pid=2108541)[0m                                    'free_cache_engine': True,
[36m(TaskRunner pid=2108541)[0m                                    'gpu_memory_utilization': 0.85,
[36m(TaskRunner pid=2108541)[0m                                    'ignore_eos': False,
[36m(TaskRunner pid=2108541)[0m                                    'layer_name_map': {'gate_proj_layer_name': 'gate_up',
[36m(TaskRunner pid=2108541)[0m                                                       'qkv_layer_name': 'qkv'},
[36m(TaskRunner pid=2108541)[0m                                    'load_format': 'dummy',
[36m(TaskRunner pid=2108541)[0m                                    'log_prob_max_token_len_per_gpu': 36864,
[36m(TaskRunner pid=2108541)[0m                                    'log_prob_micro_batch_size': None,
[36m(TaskRunner pid=2108541)[0m                                    'log_prob_micro_batch_size_per_gpu': None,
[36m(TaskRunner pid=2108541)[0m                                    'log_prob_use_dynamic_bsz': True,
[36m(TaskRunner pid=2108541)[0m                                    'max_model_len': None,
[36m(TaskRunner pid=2108541)[0m                                    'max_num_batched_tokens': 36864,
[36m(TaskRunner pid=2108541)[0m                                    'max_num_seqs': 128,
[36m(TaskRunner pid=2108541)[0m                                    'mode': 'sync',
[36m(TaskRunner pid=2108541)[0m                                    'multi_stage_wake_up': False,
[36m(TaskRunner pid=2108541)[0m                                    'multi_turn': {'_target_': 'verl.workers.config.MultiTurnConfig',
[36m(TaskRunner pid=2108541)[0m                                                   'enable': False,
[36m(TaskRunner pid=2108541)[0m                                                   'format': 'hermes',
[36m(TaskRunner pid=2108541)[0m                                                   'interaction_config_path': None,
[36m(TaskRunner pid=2108541)[0m                                                   'max_assistant_turns': None,
[36m(TaskRunner pid=2108541)[0m                                                   'max_parallel_calls': 1,
[36m(TaskRunner pid=2108541)[0m                                                   'max_tool_response_length': 256,
[36m(TaskRunner pid=2108541)[0m                                                   'max_user_turns': None,
[36m(TaskRunner pid=2108541)[0m                                                   'num_repeat_rollouts': None,
[36m(TaskRunner pid=2108541)[0m                                                   'tokenization_sanity_check_mode': 'strict',
[36m(TaskRunner pid=2108541)[0m                                                   'tool_config_path': None,
[36m(TaskRunner pid=2108541)[0m                                                   'tool_response_truncate_side': 'middle',
[36m(TaskRunner pid=2108541)[0m                                                   'use_inference_chat_template': False},
[36m(TaskRunner pid=2108541)[0m                                    'n': 16,
[36m(TaskRunner pid=2108541)[0m                                    'name': 'vllm',
[36m(TaskRunner pid=2108541)[0m                                    'over_sample_rate': 0,
[36m(TaskRunner pid=2108541)[0m                                    'pipeline_model_parallel_size': 1,
[36m(TaskRunner pid=2108541)[0m                                    'profiler': {'_target_': 'verl.utils.profiler.ProfilerConfig',
[36m(TaskRunner pid=2108541)[0m                                                 'all_ranks': False,
[36m(TaskRunner pid=2108541)[0m                                                 'enable': False,
[36m(TaskRunner pid=2108541)[0m                                                 'ranks': [],
[36m(TaskRunner pid=2108541)[0m                                                 'save_path': 'outputs/profile',
[36m(TaskRunner pid=2108541)[0m                                                 'tool': None,
[36m(TaskRunner pid=2108541)[0m                                                 'tool_config': {'npu': {'_target_': 'verl.utils.profiler.config.NPUToolConfig',
[36m(TaskRunner pid=2108541)[0m                                                                         'analysis': True,
[36m(TaskRunner pid=2108541)[0m                                                                         'contents': [],
[36m(TaskRunner pid=2108541)[0m                                                                         'discrete': False,
[36m(TaskRunner pid=2108541)[0m                                                                         'level': 'level1'},
[36m(TaskRunner pid=2108541)[0m                                                                 'nsys': {'_target_': 'verl.utils.profiler.config.NsightToolConfig',
[36m(TaskRunner pid=2108541)[0m                                                                          'discrete': False},
[36m(TaskRunner pid=2108541)[0m                                                                 'torch': {'_target_': 'verl.utils.profiler.config.TorchProfilerToolConfig',
[36m(TaskRunner pid=2108541)[0m                                                                           'step_end': None,
[36m(TaskRunner pid=2108541)[0m                                                                           'step_start': 0},
[36m(TaskRunner pid=2108541)[0m                                                                 'torch_memory': {'_target_': 'verl.utils.profiler.config.TorchMemoryToolConfig',
[36m(TaskRunner pid=2108541)[0m                                                                                  'stack_depth': 32,
[36m(TaskRunner pid=2108541)[0m                                                                                  'trace_alloc_max_entries': 100000}}},
[36m(TaskRunner pid=2108541)[0m                                    'prompt_length': 2048,
[36m(TaskRunner pid=2108541)[0m                                    'response_length': 34816,
[36m(TaskRunner pid=2108541)[0m                                    'skip_dump_dir': '/tmp/rollout_dump',
[36m(TaskRunner pid=2108541)[0m                                    'skip_rollout': False,
[36m(TaskRunner pid=2108541)[0m                                    'skip_tokenizer_init': True,
[36m(TaskRunner pid=2108541)[0m                                    'temperature': 0.9,
[36m(TaskRunner pid=2108541)[0m                                    'tensor_model_parallel_size': 8,
[36m(TaskRunner pid=2108541)[0m                                    'top_k': -1,
[36m(TaskRunner pid=2108541)[0m                                    'top_p': 1.0,
[36m(TaskRunner pid=2108541)[0m                                    'trace': {'_target_': 'verl.workers.config.TraceConfig',
[36m(TaskRunner pid=2108541)[0m                                              'backend': None,
[36m(TaskRunner pid=2108541)[0m                                              'token2text': False},
[36m(TaskRunner pid=2108541)[0m                                    'update_weights_bucket_megabytes': 512,
[36m(TaskRunner pid=2108541)[0m                                    'val_kwargs': {'_target_': 'verl.workers.config.SamplingConfig',
[36m(TaskRunner pid=2108541)[0m                                                   'do_sample': False,
[36m(TaskRunner pid=2108541)[0m                                                   'n': 1,
[36m(TaskRunner pid=2108541)[0m                                                   'temperature': 0,
[36m(TaskRunner pid=2108541)[0m                                                   'top_k': -1,
[36m(TaskRunner pid=2108541)[0m                                                   'top_p': 1.0}}},
[36m(TaskRunner pid=2108541)[0m  'algorithm': {'_target_': 'verl.trainer.config.AlgoConfig',
[36m(TaskRunner pid=2108541)[0m                'adv_estimator': 'grpo',
[36m(TaskRunner pid=2108541)[0m                'gamma': 1.0,
[36m(TaskRunner pid=2108541)[0m                'kl_ctrl': {'_target_': 'verl.trainer.config.KLControlConfig',
[36m(TaskRunner pid=2108541)[0m                            'horizon': 10000,
[36m(TaskRunner pid=2108541)[0m                            'kl_coef': 0.001,
[36m(TaskRunner pid=2108541)[0m                            'target_kl': 0.1,
[36m(TaskRunner pid=2108541)[0m                            'type': 'fixed'},
[36m(TaskRunner pid=2108541)[0m                'kl_penalty': 'kl',
[36m(TaskRunner pid=2108541)[0m                'lam': 1.0,
[36m(TaskRunner pid=2108541)[0m                'norm_adv_by_std_in_grpo': True,
[36m(TaskRunner pid=2108541)[0m                'pf_ppo': {'reweight_method': 'pow', 'weight_pow': 2.0},
[36m(TaskRunner pid=2108541)[0m                'rollout_is': False,
[36m(TaskRunner pid=2108541)[0m                'rollout_is_level': 'token',
[36m(TaskRunner pid=2108541)[0m                'rollout_is_mode': 'truncate',
[36m(TaskRunner pid=2108541)[0m                'rollout_is_threshold': None,
[36m(TaskRunner pid=2108541)[0m                'rollout_is_threshold_lower': None,
[36m(TaskRunner pid=2108541)[0m                'rollout_is_veto_threshold': 0.0001,
[36m(TaskRunner pid=2108541)[0m                'use_kl_in_reward': False,
[36m(TaskRunner pid=2108541)[0m                'use_pf_ppo': False},
[36m(TaskRunner pid=2108541)[0m  'critic': {'_target_': 'verl.workers.config.McoreCriticConfig',
[36m(TaskRunner pid=2108541)[0m             'checkpoint': {'_target_': 'verl.trainer.config.CheckpointConfig',
[36m(TaskRunner pid=2108541)[0m                            'async_save': False,
[36m(TaskRunner pid=2108541)[0m                            'load_contents': ['model', 'optimizer', 'extra'],
[36m(TaskRunner pid=2108541)[0m                            'save_contents': ['model', 'optimizer', 'extra']},
[36m(TaskRunner pid=2108541)[0m             'cliprange_value': 0.5,
[36m(TaskRunner pid=2108541)[0m             'data_loader_seed': None,
[36m(TaskRunner pid=2108541)[0m             'enable': None,
[36m(TaskRunner pid=2108541)[0m             'forward_max_token_len_per_gpu': 32768,
[36m(TaskRunner pid=2108541)[0m             'load_weight': True,
[36m(TaskRunner pid=2108541)[0m             'loss_agg_mode': 'token-mean',
[36m(TaskRunner pid=2108541)[0m             'megatron': {'_target_': 'verl.workers.config.McoreEngineConfig',
[36m(TaskRunner pid=2108541)[0m                          'context_parallel_size': 1,
[36m(TaskRunner pid=2108541)[0m                          'dist_checkpointing_path': None,
[36m(TaskRunner pid=2108541)[0m                          'expert_model_parallel_size': 1,
[36m(TaskRunner pid=2108541)[0m                          'expert_tensor_parallel_size': 1,
[36m(TaskRunner pid=2108541)[0m                          'forward_only': False,
[36m(TaskRunner pid=2108541)[0m                          'grad_offload': False,
[36m(TaskRunner pid=2108541)[0m                          'optimizer_offload': False,
[36m(TaskRunner pid=2108541)[0m                          'override_ddp_config': {},
[36m(TaskRunner pid=2108541)[0m                          'override_mcore_model_config': {},
[36m(TaskRunner pid=2108541)[0m                          'override_transformer_config': {'attention_backend': 'flash',
[36m(TaskRunner pid=2108541)[0m                                                          'recompute_granularity': None,
[36m(TaskRunner pid=2108541)[0m                                                          'recompute_method': None,
[36m(TaskRunner pid=2108541)[0m                                                          'recompute_modules': ['core_attn'],
[36m(TaskRunner pid=2108541)[0m                                                          'recompute_num_layers': None},
[36m(TaskRunner pid=2108541)[0m                          'param_offload': False,
[36m(TaskRunner pid=2108541)[0m                          'pipeline_model_parallel_size': 1,
[36m(TaskRunner pid=2108541)[0m                          'seed': 42,
[36m(TaskRunner pid=2108541)[0m                          'sequence_parallel': True,
[36m(TaskRunner pid=2108541)[0m                          'tensor_model_parallel_size': 1,
[36m(TaskRunner pid=2108541)[0m                          'use_dist_checkpointing': False,
[36m(TaskRunner pid=2108541)[0m                          'use_distributed_optimizer': True,
[36m(TaskRunner pid=2108541)[0m                          'use_mbridge': False,
[36m(TaskRunner pid=2108541)[0m                          'virtual_pipeline_model_parallel_size': None},
[36m(TaskRunner pid=2108541)[0m             'model': {'_target_': 'verl.trainer.config.BaseModelConfig',
[36m(TaskRunner pid=2108541)[0m                       'external_lib': None,
[36m(TaskRunner pid=2108541)[0m                       'override_config': {'model_config': {},
[36m(TaskRunner pid=2108541)[0m                                           'moe_config': {'freeze_moe_router': False}},
[36m(TaskRunner pid=2108541)[0m                       'path': '~/models/deepseek-llm-7b-chat',
[36m(TaskRunner pid=2108541)[0m                       'tokenizer_path': '/home/data/Qwen3-32B',
[36m(TaskRunner pid=2108541)[0m                       'trust_remote_code': False},
[36m(TaskRunner pid=2108541)[0m             'nccl_timeout': 600,
[36m(TaskRunner pid=2108541)[0m             'optim': {'_target_': 'verl.workers.config.McoreOptimizerConfig',
[36m(TaskRunner pid=2108541)[0m                       'betas': [0.9, 0.999],
[36m(TaskRunner pid=2108541)[0m                       'clip_grad': 1.0,
[36m(TaskRunner pid=2108541)[0m                       'lr': 1e-05,
[36m(TaskRunner pid=2108541)[0m                       'lr_decay_steps': None,
[36m(TaskRunner pid=2108541)[0m                       'lr_decay_style': 'constant',
[36m(TaskRunner pid=2108541)[0m                       'lr_warmup_init': 0.0,
[36m(TaskRunner pid=2108541)[0m                       'lr_warmup_steps': -1,
[36m(TaskRunner pid=2108541)[0m                       'lr_warmup_steps_ratio': 0.0,
[36m(TaskRunner pid=2108541)[0m                       'lr_wsd_decay_steps': None,
[36m(TaskRunner pid=2108541)[0m                       'lr_wsd_decay_style': 'exponential',
[36m(TaskRunner pid=2108541)[0m                       'min_lr': 0.0,
[36m(TaskRunner pid=2108541)[0m                       'optimizer': 'adam',
[36m(TaskRunner pid=2108541)[0m                       'override_optimizer_config': {},
[36m(TaskRunner pid=2108541)[0m                       'total_training_steps': -1,
[36m(TaskRunner pid=2108541)[0m                       'use_checkpoint_opt_param_scheduler': False,
[36m(TaskRunner pid=2108541)[0m                       'weight_decay': 0.01,
[36m(TaskRunner pid=2108541)[0m                       'weight_decay_incr_style': 'constant'},
[36m(TaskRunner pid=2108541)[0m             'ppo_epochs': 1,
[36m(TaskRunner pid=2108541)[0m             'ppo_max_token_len_per_gpu': 32768,
[36m(TaskRunner pid=2108541)[0m             'ppo_micro_batch_size': None,
[36m(TaskRunner pid=2108541)[0m             'ppo_micro_batch_size_per_gpu': None,
[36m(TaskRunner pid=2108541)[0m             'ppo_mini_batch_size': 32,
[36m(TaskRunner pid=2108541)[0m             'profiler': {'_target_': 'verl.utils.profiler.ProfilerConfig',
[36m(TaskRunner pid=2108541)[0m                          'all_ranks': False,
[36m(TaskRunner pid=2108541)[0m                          'enable': False,
[36m(TaskRunner pid=2108541)[0m                          'ranks': [],
[36m(TaskRunner pid=2108541)[0m                          'save_path': 'outputs/profile',
[36m(TaskRunner pid=2108541)[0m                          'tool': None,
[36m(TaskRunner pid=2108541)[0m                          'tool_config': {'npu': {'_target_': 'verl.utils.profiler.config.NPUToolConfig',
[36m(TaskRunner pid=2108541)[0m                                                  'analysis': True,
[36m(TaskRunner pid=2108541)[0m                                                  'contents': [],
[36m(TaskRunner pid=2108541)[0m                                                  'discrete': False,
[36m(TaskRunner pid=2108541)[0m                                                  'level': 'level1'},
[36m(TaskRunner pid=2108541)[0m                                          'nsys': {'_target_': 'verl.utils.profiler.config.NsightToolConfig',
[36m(TaskRunner pid=2108541)[0m                                                   'discrete': False},
[36m(TaskRunner pid=2108541)[0m                                          'torch': {'_target_': 'verl.utils.profiler.config.TorchProfilerToolConfig',
[36m(TaskRunner pid=2108541)[0m                                                    'step_end': None,
[36m(TaskRunner pid=2108541)[0m                                                    'step_start': 0},
[36m(TaskRunner pid=2108541)[0m                                          'torch_memory': {'_target_': 'verl.utils.profiler.config.TorchMemoryToolConfig',
[36m(TaskRunner pid=2108541)[0m                                                           'stack_depth': 32,
[36m(TaskRunner pid=2108541)[0m                                                           'trace_alloc_max_entries': 100000}}},
[36m(TaskRunner pid=2108541)[0m             'rollout_n': 16,
[36m(TaskRunner pid=2108541)[0m             'shuffle': False,
[36m(TaskRunner pid=2108541)[0m             'strategy': 'megatron',
[36m(TaskRunner pid=2108541)[0m             'use_dynamic_bsz': True},
[36m(TaskRunner pid=2108541)[0m  'custom_reward_function': {'name': 'compute_score', 'path': 'deepscaler.py'},
[36m(TaskRunner pid=2108541)[0m  'data': {'apply_chat_template_kwargs': {},
[36m(TaskRunner pid=2108541)[0m           'custom_cls': {'name': None, 'path': None},
[36m(TaskRunner pid=2108541)[0m           'datagen': {'name': None, 'path': None},
[36m(TaskRunner pid=2108541)[0m           'dataloader_num_workers': 8,
[36m(TaskRunner pid=2108541)[0m           'dataset_fraction': 0.002,
[36m(TaskRunner pid=2108541)[0m           'filter_overlong_prompts': True,
[36m(TaskRunner pid=2108541)[0m           'filter_overlong_prompts_workers': 1,
[36m(TaskRunner pid=2108541)[0m           'image_key': 'images',
[36m(TaskRunner pid=2108541)[0m           'max_prompt_length': 2048,
[36m(TaskRunner pid=2108541)[0m           'max_response_length': 34816,
[36m(TaskRunner pid=2108541)[0m           'prompt_key': 'prompt',
[36m(TaskRunner pid=2108541)[0m           'return_full_prompt': False,
[36m(TaskRunner pid=2108541)[0m           'return_multi_modal_inputs': True,
[36m(TaskRunner pid=2108541)[0m           'return_raw_chat': False,
[36m(TaskRunner pid=2108541)[0m           'return_raw_input_ids': False,
[36m(TaskRunner pid=2108541)[0m           'reward_fn_key': 'data_source',
[36m(TaskRunner pid=2108541)[0m           'sampler': {'class_name': None, 'class_path': None},
[36m(TaskRunner pid=2108541)[0m           'shuffle': False,
[36m(TaskRunner pid=2108541)[0m           'tokenizer': None,
[36m(TaskRunner pid=2108541)[0m           'train_batch_size': 32,
[36m(TaskRunner pid=2108541)[0m           'train_files': '/workspace/data/deepscaler/train.parquet',
[36m(TaskRunner pid=2108541)[0m           'truncation': 'error',
[36m(TaskRunner pid=2108541)[0m           'trust_remote_code': False,
[36m(TaskRunner pid=2108541)[0m           'use_shm': False,
[36m(TaskRunner pid=2108541)[0m           'val_batch_size': None,
[36m(TaskRunner pid=2108541)[0m           'val_files': '/workspace/data/deepscaler/test.parquet',
[36m(TaskRunner pid=2108541)[0m           'validation_shuffle': False,
[36m(TaskRunner pid=2108541)[0m           'video_key': 'videos'},
[36m(TaskRunner pid=2108541)[0m  'global_profiler': {'_target_': 'verl.utils.profiler.ProfilerConfig',
[36m(TaskRunner pid=2108541)[0m                      'global_tool_config': {'nsys': {'controller_nsight_options': {'cuda-graph-trace': 'graph',
[36m(TaskRunner pid=2108541)[0m                                                                                    'cuda-memory-usage': 'true',
[36m(TaskRunner pid=2108541)[0m                                                                                    'trace': 'cuda,nvtx,cublas,ucx'},
[36m(TaskRunner pid=2108541)[0m                                                      'discrete': False,
[36m(TaskRunner pid=2108541)[0m                                                      'worker_nsight_options': {'capture-range': 'cudaProfilerApi',
[36m(TaskRunner pid=2108541)[0m                                                                                'capture-range-end': None,
[36m(TaskRunner pid=2108541)[0m                                                                                'cuda-graph-trace': 'graph',
[36m(TaskRunner pid=2108541)[0m                                                                                'cuda-memory-usage': 'true',
[36m(TaskRunner pid=2108541)[0m                                                                                'kill': 'none',
[36m(TaskRunner pid=2108541)[0m                                                                                'trace': 'cuda,nvtx,cublas,ucx'}},
[36m(TaskRunner pid=2108541)[0m                                             'torch_memory': {'context': 'all',
[36m(TaskRunner pid=2108541)[0m                                                              'kw_args': {},
[36m(TaskRunner pid=2108541)[0m                                                              'stack_depth': 32,
[36m(TaskRunner pid=2108541)[0m                                                              'stacks': 'all',
[36m(TaskRunner pid=2108541)[0m                                                              'trace_alloc_max_entries': 100000}},
[36m(TaskRunner pid=2108541)[0m                      'profile_continuous_steps': False,
[36m(TaskRunner pid=2108541)[0m                      'save_path': 'outputs/profile',
[36m(TaskRunner pid=2108541)[0m                      'steps': None,
[36m(TaskRunner pid=2108541)[0m                      'tool': None},
[36m(TaskRunner pid=2108541)[0m  'ray_kwargs': {'ray_init': {'num_cpus': None}, 'timeline_json_file': None},
[36m(TaskRunner pid=2108541)[0m  'reward_model': {'enable': False,
[36m(TaskRunner pid=2108541)[0m                   'enable_resource_pool': False,
[36m(TaskRunner pid=2108541)[0m                   'forward_max_token_len_per_gpu': 32768,
[36m(TaskRunner pid=2108541)[0m                   'launch_reward_fn_async': False,
[36m(TaskRunner pid=2108541)[0m                   'load_weight': True,
[36m(TaskRunner pid=2108541)[0m                   'max_length': None,
[36m(TaskRunner pid=2108541)[0m                   'megatron': {'_target_': 'verl.workers.config.MegatronEngineConfig',
[36m(TaskRunner pid=2108541)[0m                                'context_parallel_size': 1,
[36m(TaskRunner pid=2108541)[0m                                'dist_checkpointing_path': None,
[36m(TaskRunner pid=2108541)[0m                                'expert_model_parallel_size': 1,
[36m(TaskRunner pid=2108541)[0m                                'expert_tensor_parallel_size': 1,
[36m(TaskRunner pid=2108541)[0m                                'override_transformer_config': {'attention_backend': 'flash',
[36m(TaskRunner pid=2108541)[0m                                                                'context_parallel_algo': 'megatron_cp_algo',
[36m(TaskRunner pid=2108541)[0m                                                                'context_parallel_size': 1,
[36m(TaskRunner pid=2108541)[0m                                                                'cp_window_size': 1,
[36m(TaskRunner pid=2108541)[0m                                                                'recompute_granularity': 'full',
[36m(TaskRunner pid=2108541)[0m                                                                'recompute_method': 'uniform',
[36m(TaskRunner pid=2108541)[0m                                                                'recompute_modules': ['core_attn'],
[36m(TaskRunner pid=2108541)[0m                                                                'recompute_num_layers': 1,
[36m(TaskRunner pid=2108541)[0m                                                                'seq_length': 2048,
[36m(TaskRunner pid=2108541)[0m                                                                'swap_optimizer': True,
[36m(TaskRunner pid=2108541)[0m                                                                'use_cp_send_recv_overlap': True,
[36m(TaskRunner pid=2108541)[0m                                                                'use_flash_attn': True,
[36m(TaskRunner pid=2108541)[0m                                                                'use_fused_ring_attention_update': True,
[36m(TaskRunner pid=2108541)[0m                                                                'use_fused_rotary_pos_emb': True,
[36m(TaskRunner pid=2108541)[0m                                                                'use_fused_swiglu': True},
[36m(TaskRunner pid=2108541)[0m                                'param_offload': False,
[36m(TaskRunner pid=2108541)[0m                                'pipeline_model_parallel_size': 1,
[36m(TaskRunner pid=2108541)[0m                                'seed': 42,
[36m(TaskRunner pid=2108541)[0m                                'sequence_parallel': True,
[36m(TaskRunner pid=2108541)[0m                                'tensor_model_parallel_size': 1,
[36m(TaskRunner pid=2108541)[0m                                'use_dist_checkpointing': False,
[36m(TaskRunner pid=2108541)[0m                                'use_distributed_optimizer': False,
[36m(TaskRunner pid=2108541)[0m                                'use_mbridge': False,
[36m(TaskRunner pid=2108541)[0m                                'virtual_pipeline_model_parallel_size': None},
[36m(TaskRunner pid=2108541)[0m                   'micro_batch_size': None,
[36m(TaskRunner pid=2108541)[0m                   'micro_batch_size_per_gpu': None,
[36m(TaskRunner pid=2108541)[0m                   'model': {'external_lib': None,
[36m(TaskRunner pid=2108541)[0m                             'input_tokenizer': '/home/data/Qwen3-32B',
[36m(TaskRunner pid=2108541)[0m                             'path': '~/models/FsfairX-LLaMA3-RM-v0.1',
[36m(TaskRunner pid=2108541)[0m                             'trust_remote_code': False},
[36m(TaskRunner pid=2108541)[0m                   'n_gpus_per_node': 0,
[36m(TaskRunner pid=2108541)[0m                   'nccl_timeout': 600,
[36m(TaskRunner pid=2108541)[0m                   'nnodes': 0,
[36m(TaskRunner pid=2108541)[0m                   'profiler': {'_target_': 'verl.utils.profiler.ProfilerConfig',
[36m(TaskRunner pid=2108541)[0m                                'all_ranks': False,
[36m(TaskRunner pid=2108541)[0m                                'enable': False,
[36m(TaskRunner pid=2108541)[0m                                'ranks': [],
[36m(TaskRunner pid=2108541)[0m                                'save_path': 'outputs/profile',
[36m(TaskRunner pid=2108541)[0m                                'tool': None,
[36m(TaskRunner pid=2108541)[0m                                'tool_config': {'npu': {'_target_': 'verl.utils.profiler.config.NPUToolConfig',
[36m(TaskRunner pid=2108541)[0m                                                        'analysis': True,
[36m(TaskRunner pid=2108541)[0m                                                        'contents': [],
[36m(TaskRunner pid=2108541)[0m                                                        'discrete': False,
[36m(TaskRunner pid=2108541)[0m                                                        'level': 'level1'},
[36m(TaskRunner pid=2108541)[0m                                                'nsys': {'_target_': 'verl.utils.profiler.config.NsightToolConfig',
[36m(TaskRunner pid=2108541)[0m                                                         'discrete': False},
[36m(TaskRunner pid=2108541)[0m                                                'torch': {'_target_': 'verl.utils.profiler.config.TorchProfilerToolConfig',
[36m(TaskRunner pid=2108541)[0m                                                          'step_end': None,
[36m(TaskRunner pid=2108541)[0m                                                          'step_start': 0},
[36m(TaskRunner pid=2108541)[0m                                                'torch_memory': {'_target_': 'verl.utils.profiler.config.TorchMemoryToolConfig',
[36m(TaskRunner pid=2108541)[0m                                                                 'stack_depth': 32,
[36m(TaskRunner pid=2108541)[0m                                                                 'trace_alloc_max_entries': 100000}}},
[36m(TaskRunner pid=2108541)[0m                   'reward_manager': 'naive',
[36m(TaskRunner pid=2108541)[0m                   'sandbox_fusion': {'max_concurrent': 64,
[36m(TaskRunner pid=2108541)[0m                                      'memory_limit_mb': 1024,
[36m(TaskRunner pid=2108541)[0m                                      'url': None},
[36m(TaskRunner pid=2108541)[0m                   'strategy': 'megatron',
[36m(TaskRunner pid=2108541)[0m                   'use_dynamic_bsz': True},
[36m(TaskRunner pid=2108541)[0m  'trainer': {'balance_batch': False,
[36m(TaskRunner pid=2108541)[0m              'critic_warmup': 0,
[36m(TaskRunner pid=2108541)[0m              'default_hdfs_dir': None,
[36m(TaskRunner pid=2108541)[0m              'default_local_dir': 'checkpoints/verl_grpo_example_deepscaler/qwen3_32b_verl_true_weights',
[36m(TaskRunner pid=2108541)[0m              'del_local_ckpt_after_load': False,
[36m(TaskRunner pid=2108541)[0m              'device': 'npu',
[36m(TaskRunner pid=2108541)[0m              'esi_redundant_time': 0,
[36m(TaskRunner pid=2108541)[0m              'experiment_name': 'qwen3_32b_verl_true_weights',
[36m(TaskRunner pid=2108541)[0m              'log_val_generations': 0,
[36m(TaskRunner pid=2108541)[0m              'logger': ['console', 'tensorboard'],
[36m(TaskRunner pid=2108541)[0m              'max_actor_ckpt_to_keep': None,
[36m(TaskRunner pid=2108541)[0m              'max_critic_ckpt_to_keep': None,
[36m(TaskRunner pid=2108541)[0m              'n_gpus_per_node': 16,
[36m(TaskRunner pid=2108541)[0m              'nnodes': 1,
[36m(TaskRunner pid=2108541)[0m              'project_name': 'verl_grpo_example_deepscaler',
[36m(TaskRunner pid=2108541)[0m              'ray_wait_register_center_timeout': 300,
[36m(TaskRunner pid=2108541)[0m              'resume_from_path': None,
[36m(TaskRunner pid=2108541)[0m              'resume_mode': 'auto',
[36m(TaskRunner pid=2108541)[0m              'rollout_data_dir': '/workspace/data/dump',
[36m(TaskRunner pid=2108541)[0m              'rollout_length_dir': '/workspace/data/dump',
[36m(TaskRunner pid=2108541)[0m              'save_freq': -1,
[36m(TaskRunner pid=2108541)[0m              'test_freq': -1,
[36m(TaskRunner pid=2108541)[0m              'total_epochs': 10,
[36m(TaskRunner pid=2108541)[0m              'total_training_steps': None,
[36m(TaskRunner pid=2108541)[0m              'val_before_train': False}}
[36m(TaskRunner pid=2108541)[0m WARNING 12-14 01:29:26 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(TaskRunner pid=2108541)[0m [validate_config] All configuration checks passed successfully!
[36m(TaskRunner pid=2108541)[0m using customized reward function 'compute_score' from '/workspace/cann-recipes-train/llm_rl/qwen3/deepscaler.py'
[36m(TaskRunner pid=2108541)[0m using customized reward function 'compute_score' from '/workspace/cann-recipes-train/llm_rl/qwen3/deepscaler.py'
[36m(TaskRunner pid=2108541)[0m Using dataset class: RLHFDataset
[36m(TaskRunner pid=2108541)[0m dataset len: 36283
[36m(TaskRunner pid=2108541)[0m Sampled dataset len: 72 (fraction: 0.002)
[36m(TaskRunner pid=2108541)[0m filter dataset len: 72
[36m(TaskRunner pid=2108541)[0m Using dataset class: RLHFDataset
[36m(TaskRunner pid=2108541)[0m dataset len: 4032
[36m(TaskRunner pid=2108541)[0m Sampled dataset len: 8 (fraction: 0.002)
[36m(TaskRunner pid=2108541)[0m filter dataset len: 8
[36m(TaskRunner pid=2108541)[0m Size of train dataloader: 2, Size of val dataloader: 1
[36m(TaskRunner pid=2108541)[0m Total training steps: 20
[36m(TaskRunner pid=2108541)[0m colocated worker base class <class 'verl.workers.megatron_workers.MegatronWorker'>
[36m(pid=2136241)[0m [Rank 0 | Local Rank 0] 2025-12-14 01:29:39,503 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(pid=2136244)[0m [Rank 1 | Local Rank 0] 2025-12-14 01:29:40,206 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(pid=2136247)[0m [Rank 2 | Local Rank 0] 2025-12-14 01:29:40,519 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(pid=2136248)[0m [Rank 3 | Local Rank 0] 2025-12-14 01:29:40,709 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(pid=2136249)[0m [Rank 11 | Local Rank 0] 2025-12-14 01:29:40,830 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(pid=2136241)[0m INFO 12-14 01:29:40 [__init__.py:36] Available plugins for group vllm.platform_plugins:
[36m(pid=2136241)[0m INFO 12-14 01:29:40 [__init__.py:38] - ascend -> vllm_ascend:register
[36m(pid=2136241)[0m INFO 12-14 01:29:40 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[36m(pid=2136241)[0m INFO 12-14 01:29:40 [__init__.py:207] Platform plugin ascend is activated
[36m(pid=2136254)[0m [Rank 5 | Local Rank 0] 2025-12-14 01:29:41,086 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(pid=2136256)[0m [Rank 7 | Local Rank 0] 2025-12-14 01:29:41,339 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(pid=2136255)[0m [Rank 6 | Local Rank 0] 2025-12-14 01:29:41,339 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(pid=2136260)[0m [Rank 12 | Local Rank 0] 2025-12-14 01:29:41,420 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(pid=2136258)[0m [Rank 9 | Local Rank 0] 2025-12-14 01:29:41,499 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(pid=2136244)[0m INFO 12-14 01:29:41 [__init__.py:36] Available plugins for group vllm.platform_plugins:
[36m(pid=2136244)[0m INFO 12-14 01:29:41 [__init__.py:38] - ascend -> vllm_ascend:register
[36m(pid=2136244)[0m INFO 12-14 01:29:41 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[36m(pid=2136244)[0m INFO 12-14 01:29:41 [__init__.py:207] Platform plugin ascend is activated
[36m(pid=2136247)[0m INFO 12-14 01:29:41 [__init__.py:36] Available plugins for group vllm.platform_plugins:
[36m(pid=2136247)[0m INFO 12-14 01:29:41 [__init__.py:38] - ascend -> vllm_ascend:register
[36m(pid=2136247)[0m INFO 12-14 01:29:41 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[36m(pid=2136261)[0m [Rank 13 | Local Rank 0] 2025-12-14 01:29:41,842 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(pid=2136247)[0m INFO 12-14 01:29:41 [__init__.py:207] Platform plugin ascend is activated
[36m(pid=2136253)[0m [Rank 4 | Local Rank 0] 2025-12-14 01:29:41,926 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(pid=2136248)[0m INFO 12-14 01:29:42 [__init__.py:36] Available plugins for group vllm.platform_plugins:
[36m(pid=2136248)[0m INFO 12-14 01:29:42 [__init__.py:38] - ascend -> vllm_ascend:register
[36m(pid=2136248)[0m INFO 12-14 01:29:42 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[36m(pid=2136248)[0m INFO 12-14 01:29:42 [__init__.py:207] Platform plugin ascend is activated
[36m(pid=2136241)[0m WARNING 12-14 01:29:42 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(pid=2136269)[0m [Rank 14 | Local Rank 0] 2025-12-14 01:29:42,216 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(pid=2136249)[0m INFO 12-14 01:29:42 [__init__.py:36] Available plugins for group vllm.platform_plugins:
[36m(pid=2136249)[0m INFO 12-14 01:29:42 [__init__.py:38] - ascend -> vllm_ascend:register
[36m(pid=2136249)[0m INFO 12-14 01:29:42 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[36m(pid=2136249)[0m INFO 12-14 01:29:42 [__init__.py:207] Platform plugin ascend is activated
[36m(pid=2136254)[0m INFO 12-14 01:29:42 [__init__.py:36] Available plugins for group vllm.platform_plugins:
[36m(pid=2136254)[0m INFO 12-14 01:29:42 [__init__.py:38] - ascend -> vllm_ascend:register
[36m(pid=2136254)[0m INFO 12-14 01:29:42 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[36m(pid=2136254)[0m INFO 12-14 01:29:42 [__init__.py:207] Platform plugin ascend is activated
[36m(pid=2136257)[0m [Rank 8 | Local Rank 0] 2025-12-14 01:29:42,319 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(pid=2136265)[0m [Rank 15 | Local Rank 0] 2025-12-14 01:29:42,383 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(pid=2136259)[0m [Rank 10 | Local Rank 0] 2025-12-14 01:29:42,470 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(pid=2136255)[0m INFO 12-14 01:29:42 [__init__.py:36] Available plugins for group vllm.platform_plugins:
[36m(pid=2136255)[0m INFO 12-14 01:29:42 [__init__.py:38] - ascend -> vllm_ascend:register
[36m(pid=2136255)[0m INFO 12-14 01:29:42 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[36m(pid=2136255)[0m INFO 12-14 01:29:42 [__init__.py:207] Platform plugin ascend is activated
[36m(pid=2136260)[0m INFO 12-14 01:29:42 [__init__.py:36] Available plugins for group vllm.platform_plugins:
[36m(pid=2136260)[0m INFO 12-14 01:29:42 [__init__.py:38] - ascend -> vllm_ascend:register
[36m(pid=2136260)[0m INFO 12-14 01:29:42 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[36m(pid=2136260)[0m INFO 12-14 01:29:42 [__init__.py:207] Platform plugin ascend is activated
[36m(pid=2136256)[0m INFO 12-14 01:29:42 [__init__.py:36] Available plugins for group vllm.platform_plugins:
[36m(pid=2136256)[0m INFO 12-14 01:29:42 [__init__.py:38] - ascend -> vllm_ascend:register
[36m(pid=2136256)[0m INFO 12-14 01:29:42 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[36m(pid=2136256)[0m INFO 12-14 01:29:42 [__init__.py:207] Platform plugin ascend is activated
[36m(pid=2136258)[0m INFO 12-14 01:29:42 [__init__.py:36] Available plugins for group vllm.platform_plugins:
[36m(pid=2136258)[0m INFO 12-14 01:29:42 [__init__.py:38] - ascend -> vllm_ascend:register
[36m(pid=2136258)[0m INFO 12-14 01:29:42 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[36m(pid=2136258)[0m INFO 12-14 01:29:42 [__init__.py:207] Platform plugin ascend is activated
[36m(pid=2136244)[0m WARNING 12-14 01:29:42 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(pid=2136247)[0m WARNING 12-14 01:29:43 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(WorkerDict pid=2136241)[0m [Rank 0 | Local Rank 0] 2025-12-14 01:29:43,172 WARNING [mindspeed.args_utils:70] => Failed from megatron.training import get_args, use mindspeed arguments.
[36m(pid=2136248)[0m WARNING 12-14 01:29:43 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(pid=2136261)[0m INFO 12-14 01:29:43 [__init__.py:36] Available plugins for group vllm.platform_plugins:
[36m(pid=2136261)[0m INFO 12-14 01:29:43 [__init__.py:38] - ascend -> vllm_ascend:register
[36m(pid=2136261)[0m INFO 12-14 01:29:43 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[36m(pid=2136261)[0m INFO 12-14 01:29:43 [__init__.py:207] Platform plugin ascend is activated
[36m(pid=2136253)[0m INFO 12-14 01:29:43 [__init__.py:36] Available plugins for group vllm.platform_plugins:
[36m(pid=2136253)[0m INFO 12-14 01:29:43 [__init__.py:38] - ascend -> vllm_ascend:register
[36m(pid=2136253)[0m INFO 12-14 01:29:43 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[36m(pid=2136253)[0m INFO 12-14 01:29:43 [__init__.py:207] Platform plugin ascend is activated
[36m(pid=2136249)[0m WARNING 12-14 01:29:43 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(pid=2136254)[0m WARNING 12-14 01:29:43 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(pid=2136257)[0m INFO 12-14 01:29:43 [__init__.py:36] Available plugins for group vllm.platform_plugins:
[36m(pid=2136257)[0m INFO 12-14 01:29:43 [__init__.py:38] - ascend -> vllm_ascend:register
[36m(pid=2136257)[0m INFO 12-14 01:29:43 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[36m(pid=2136257)[0m INFO 12-14 01:29:43 [__init__.py:207] Platform plugin ascend is activated
[36m(pid=2136269)[0m INFO 12-14 01:29:43 [__init__.py:36] Available plugins for group vllm.platform_plugins:
[36m(pid=2136269)[0m INFO 12-14 01:29:43 [__init__.py:38] - ascend -> vllm_ascend:register
[36m(pid=2136269)[0m INFO 12-14 01:29:43 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[36m(pid=2136269)[0m INFO 12-14 01:29:43 [__init__.py:207] Platform plugin ascend is activated
[36m(WorkerDict pid=2136244)[0m [Rank 1 | Local Rank 0] 2025-12-14 01:29:44,015 WARNING [mindspeed.args_utils:70] => Failed from megatron.training import get_args, use mindspeed arguments.
[36m(WorkerDict pid=2136247)[0m [Rank 2 | Local Rank 0] 2025-12-14 01:29:44,045 WARNING [mindspeed.args_utils:70] => Failed from megatron.training import get_args, use mindspeed arguments.
[36m(pid=2136256)[0m WARNING 12-14 01:29:44 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(pid=2136255)[0m WARNING 12-14 01:29:43 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(pid=2136260)[0m WARNING 12-14 01:29:43 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(pid=2136259)[0m INFO 12-14 01:29:44 [__init__.py:36] Available plugins for group vllm.platform_plugins:
[36m(pid=2136259)[0m INFO 12-14 01:29:44 [__init__.py:38] - ascend -> vllm_ascend:register
[36m(pid=2136259)[0m INFO 12-14 01:29:44 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[36m(pid=2136259)[0m INFO 12-14 01:29:44 [__init__.py:207] Platform plugin ascend is activated
[36m(pid=2136265)[0m INFO 12-14 01:29:43 [__init__.py:36] Available plugins for group vllm.platform_plugins:
[36m(pid=2136265)[0m INFO 12-14 01:29:43 [__init__.py:38] - ascend -> vllm_ascend:register
[36m(pid=2136265)[0m INFO 12-14 01:29:43 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[36m(pid=2136265)[0m INFO 12-14 01:29:43 [__init__.py:207] Platform plugin ascend is activated
[36m(pid=2136258)[0m WARNING 12-14 01:29:44 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(WorkerDict pid=2136248)[0m [Rank 3 | Local Rank 0] 2025-12-14 01:29:44,274 WARNING [mindspeed.args_utils:70] => Failed from megatron.training import get_args, use mindspeed arguments.
[36m(WorkerDict pid=2136254)[0m [Rank 5 | Local Rank 0] 2025-12-14 01:29:44,497 WARNING [mindspeed.args_utils:70] => Failed from megatron.training import get_args, use mindspeed arguments.
[36m(pid=2136261)[0m WARNING 12-14 01:29:44 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(WorkerDict pid=2136249)[0m [Rank 11 | Local Rank 0] 2025-12-14 01:29:44,606 WARNING [mindspeed.args_utils:70] => Failed from megatron.training import get_args, use mindspeed arguments.
[36m(pid=2136253)[0m WARNING 12-14 01:29:44 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(pid=2136257)[0m WARNING 12-14 01:29:44 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(WorkerDict pid=2136256)[0m [Rank 7 | Local Rank 0] 2025-12-14 01:29:44,943 WARNING [mindspeed.args_utils:70] => Failed from megatron.training import get_args, use mindspeed arguments.
[36m(WorkerDict pid=2136255)[0m [Rank 6 | Local Rank 0] 2025-12-14 01:29:44,939 WARNING [mindspeed.args_utils:70] => Failed from megatron.training import get_args, use mindspeed arguments.
[36m(WorkerDict pid=2136260)[0m [Rank 12 | Local Rank 0] 2025-12-14 01:29:44,980 WARNING [mindspeed.args_utils:70] => Failed from megatron.training import get_args, use mindspeed arguments.
[36m(pid=2136269)[0m WARNING 12-14 01:29:45 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(WorkerDict pid=2136258)[0m [Rank 9 | Local Rank 0] 2025-12-14 01:29:45,087 WARNING [mindspeed.args_utils:70] => Failed from megatron.training import get_args, use mindspeed arguments.
[36m(pid=2136265)[0m WARNING 12-14 01:29:45 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(pid=2136259)[0m WARNING 12-14 01:29:45 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(WorkerDict pid=2136261)[0m [Rank 13 | Local Rank 0] 2025-12-14 01:29:45,480 WARNING [mindspeed.args_utils:70] => Failed from megatron.training import get_args, use mindspeed arguments.
[36m(WorkerDict pid=2136253)[0m [Rank 4 | Local Rank 0] 2025-12-14 01:29:45,711 WARNING [mindspeed.args_utils:70] => Failed from megatron.training import get_args, use mindspeed arguments.
[36m(WorkerDict pid=2136257)[0m [Rank 8 | Local Rank 0] 2025-12-14 01:29:45,768 WARNING [mindspeed.args_utils:70] => Failed from megatron.training import get_args, use mindspeed arguments.
[36m(WorkerDict pid=2136269)[0m [Rank 14 | Local Rank 0] 2025-12-14 01:29:46,041 WARNING [mindspeed.args_utils:70] => Failed from megatron.training import get_args, use mindspeed arguments.
[36m(WorkerDict pid=2136265)[0m [Rank 15 | Local Rank 0] 2025-12-14 01:29:46,155 WARNING [mindspeed.args_utils:70] => Failed from megatron.training import get_args, use mindspeed arguments.
[36m(WorkerDict pid=2136259)[0m [Rank 10 | Local Rank 0] 2025-12-14 01:29:46,324 WARNING [mindspeed.args_utils:70] => Failed from megatron.training import get_args, use mindspeed arguments.
[36m(WorkerDict pid=2136257)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff34265ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff340dbe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff340dbe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='331', worker_launch_time_ms='1765646970817', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='537201117', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136258)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xfffef8561ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xfffef83d7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xfffef83d7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='332', worker_launch_time_ms='1765646970842', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='1618725680', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136244)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff38275ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff380ebe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff380ebe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='323', worker_launch_time_ms='1765646970634', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='2131114654', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136249)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff2c745ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff2c5b3e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff2c5b3e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='326', worker_launch_time_ms='1765646970702', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='1153430005', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136248)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff30265ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff300dbe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff300dbe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='325', worker_launch_time_ms='1765646970680', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='-769289954', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136256)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff30165ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff0c6afe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff0c6afe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='330', worker_launch_time_ms='1765646970792', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='1715011496', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136269)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff30171ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff186bbe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff186bbe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='337', worker_launch_time_ms='1765646970983', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='-1558909396', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136254)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff2835dee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff281d3e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff281d3e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='328', worker_launch_time_ms='1765646970746', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='-1053958217', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136253)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xfffefc561ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xfffefc3d7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xfffefc3d7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='327', worker_launch_time_ms='1765646970724', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='-624206571', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136257)[0m  > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 2047931392
[36m(WorkerDict pid=2136257)[0m load ref weight start
[36m(WorkerDict pid=2136257)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=2136261)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xfffefc465ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xfffefc2dbe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xfffefc2dbe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='335', worker_launch_time_ms='1765646970935', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='1948273534', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136244)[0m  > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 2047926272
[36m(WorkerDict pid=2136244)[0m load ref weight start
[36m(WorkerDict pid=2136244)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=2136247)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff30561ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff303d7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff303d7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='324', worker_launch_time_ms='1765646970657', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='1550268023', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136248)[0m  > number of parameters on (tensor, pipeline) model parallel rank (3, 0): 2047926272
[36m(WorkerDict pid=2136248)[0m load ref weight start
[36m(WorkerDict pid=2136248)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=2136249)[0m  > number of parameters on (tensor, pipeline) model parallel rank (3, 1): 2047931392
[36m(WorkerDict pid=2136249)[0m load ref weight start
[36m(WorkerDict pid=2136249)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=2136255)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xfffee4639ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xfffee44abe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xfffee44abe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='329', worker_launch_time_ms='1765646970769', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='-1486323873', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136258)[0m  > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 2047931392
[36m(WorkerDict pid=2136258)[0m load ref weight start
[36m(WorkerDict pid=2136258)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=2136260)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff2c171ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff086bbe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff086bbe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='334', worker_launch_time_ms='1765646970913', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='-1925062186', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136256)[0m  > number of parameters on (tensor, pipeline) model parallel rank (7, 0): 2047926272
[36m(WorkerDict pid=2136256)[0m load ref weight start
[36m(WorkerDict pid=2136256)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=2136254)[0m  > number of parameters on (tensor, pipeline) model parallel rank (5, 0): 2047926272
[36m(WorkerDict pid=2136254)[0m load ref weight start
[36m(WorkerDict pid=2136254)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=2136261)[0m  > number of parameters on (tensor, pipeline) model parallel rank (5, 1): 2047931392
[36m(WorkerDict pid=2136269)[0m  > number of parameters on (tensor, pipeline) model parallel rank (6, 1): 2047931392
[36m(WorkerDict pid=2136269)[0m load ref weight start
[36m(WorkerDict pid=2136269)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=2136261)[0m load ref weight start
[36m(WorkerDict pid=2136261)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=2136253)[0m  > number of parameters on (tensor, pipeline) model parallel rank (4, 0): 2047926272
[36m(WorkerDict pid=2136247)[0m  > number of parameters on (tensor, pipeline) model parallel rank (2, 0): 2047926272
[36m(WorkerDict pid=2136247)[0m load ref weight start
[36m(WorkerDict pid=2136247)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=2136253)[0m load ref weight start
[36m(WorkerDict pid=2136253)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=2136255)[0m  > number of parameters on (tensor, pipeline) model parallel rank (6, 0): 2047926272
[36m(WorkerDict pid=2136255)[0m load ref weight start
[36m(WorkerDict pid=2136255)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=2136260)[0m  > number of parameters on (tensor, pipeline) model parallel rank (4, 1): 2047931392
[36m(WorkerDict pid=2136260)[0m load ref weight start
[36m(WorkerDict pid=2136260)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=2136259)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff38561ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff383d7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff383d7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='333', worker_launch_time_ms='1765646970868', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='957585563', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136241)[0m Model config after override: Qwen3Config {
[36m(WorkerDict pid=2136241)[0m   "architectures": [
[36m(WorkerDict pid=2136241)[0m     "Qwen3ForCausalLM"
[36m(WorkerDict pid=2136241)[0m   ],
[36m(WorkerDict pid=2136241)[0m   "attention_bias": false,
[36m(WorkerDict pid=2136241)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=2136241)[0m   "eos_token_id": 151645,
[36m(WorkerDict pid=2136241)[0m   "head_dim": 128,
[36m(WorkerDict pid=2136241)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=2136241)[0m   "hidden_size": 5120,
[36m(WorkerDict pid=2136241)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=2136241)[0m   "intermediate_size": 25600,
[36m(WorkerDict pid=2136241)[0m   "layer_types": [
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention"
[36m(WorkerDict pid=2136241)[0m   ],
[36m(WorkerDict pid=2136241)[0m   "max_position_embeddings": 40960,
[36m(WorkerDict pid=2136241)[0m   "max_window_layers": 64,
[36m(WorkerDict pid=2136241)[0m   "model_type": "qwen3",
[36m(WorkerDict pid=2136241)[0m   "num_attention_heads": 64,
[36m(WorkerDict pid=2136241)[0m   "num_hidden_layers": 64,
[36m(WorkerDict pid=2136241)[0m   "num_key_value_heads": 8,
[36m(WorkerDict pid=2136241)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=2136241)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=2136241)[0m   "rope_scaling": null,
[36m(WorkerDict pid=2136241)[0m   "rope_theta": 1000000,
[36m(WorkerDict pid=2136241)[0m   "sliding_window": null,
[36m(WorkerDict pid=2136241)[0m   "tie_word_embeddings": false,
[36m(WorkerDict pid=2136241)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=2136241)[0m   "transformers_version": "4.55.0",
[36m(WorkerDict pid=2136241)[0m   "use_cache": true,
[36m(WorkerDict pid=2136241)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=2136241)[0m   "vocab_size": 151936
[36m(WorkerDict pid=2136241)[0m }
[36m(WorkerDict pid=2136241)[0m 
[36m(WorkerDict pid=2136241)[0m Overridden TransformerConfig init config: {'num_layers': 64, 'hidden_size': 5120, 'num_attention_heads': 64, 'num_query_groups': 8, 'ffn_hidden_size': 25600, 'attention_dropout': 0.0, 'hidden_dropout': 0.0, 'kv_channels': 128, 'layernorm_epsilon': 1e-06, 'add_bias_linear': False, 'activation_func': <function silu at 0xffff04265ee0>, 'normalization': 'RMSNorm', 'gated_linear_unit': True, 'pipeline_dtype': torch.bfloat16, 'params_dtype': torch.bfloat16, 'bf16': True, 'tensor_model_parallel_size': 8, 'pipeline_model_parallel_size': 2, 'expert_model_parallel_size': 1, 'expert_tensor_parallel_size': 1, 'virtual_pipeline_model_parallel_size': None, 'context_parallel_size': 1, 'overlap_p2p_comm': False, 'batch_p2p_comm': False, 'sequence_parallel': True, 'variable_seq_lengths': True, 'masked_softmax_fusion': True, 'moe_token_dispatcher_type': 'alltoall', 'use_cpu_initialization': False, 'add_qkv_bias': False, 'qk_layernorm': True, 'recompute_granularity': 'full', 'recompute_modules': ['core_attn'], 'recompute_method': 'uniform', 'recompute_num_layers': 1, 'attention_backend': <AttnBackend.flash: 1>}
[36m(WorkerDict pid=2136241)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff04265ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff040dbe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff040dbe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='322', worker_launch_time_ms='1765646970603', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='-1442360010', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136259)[0m  > number of parameters on (tensor, pipeline) model parallel rank (2, 1): 2047931392
[36m(WorkerDict pid=2136259)[0m load ref weight start
[36m(WorkerDict pid=2136259)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=2136265)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff04741ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff045b3e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff045b3e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='336', worker_launch_time_ms='1765646970959', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='-75012150', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136241)[0m  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 2047926272
[36m(WorkerDict pid=2136241)[0m load ref weight start
[36m(WorkerDict pid=2136241)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=2136265)[0m  > number of parameters on (tensor, pipeline) model parallel rank (7, 1): 2047931392
[36m(WorkerDict pid=2136265)[0m load ref weight start
[36m(WorkerDict pid=2136265)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=2136241)[0m loading embeddings...
[36m(WorkerDict pid=2136241)[0m loading layer #0, with layer_name model.layers.0...
[36m(WorkerDict pid=2136241)[0m loading layer #1, with layer_name model.layers.1...
[36m(WorkerDict pid=2136241)[0m loading layer #2, with layer_name model.layers.2...
[36m(WorkerDict pid=2136241)[0m loading layer #3, with layer_name model.layers.3...
[36m(WorkerDict pid=2136241)[0m loading layer #4, with layer_name model.layers.4...
[36m(WorkerDict pid=2136241)[0m loading layer #5, with layer_name model.layers.5...
[36m(WorkerDict pid=2136241)[0m loading layer #6, with layer_name model.layers.6...
[36m(WorkerDict pid=2136241)[0m loading layer #7, with layer_name model.layers.7...
[36m(WorkerDict pid=2136241)[0m loading layer #8, with layer_name model.layers.8...
[36m(WorkerDict pid=2136241)[0m loading layer #9, with layer_name model.layers.9...
[36m(WorkerDict pid=2136241)[0m loading layer #10, with layer_name model.layers.10...
[36m(WorkerDict pid=2136241)[0m loading layer #11, with layer_name model.layers.11...
[36m(WorkerDict pid=2136241)[0m loading layer #12, with layer_name model.layers.12...
[36m(WorkerDict pid=2136241)[0m loading layer #13, with layer_name model.layers.13...
[36m(WorkerDict pid=2136241)[0m loading layer #14, with layer_name model.layers.14...
[36m(WorkerDict pid=2136241)[0m loading layer #15, with layer_name model.layers.15...
[36m(WorkerDict pid=2136241)[0m loading layer #16, with layer_name model.layers.16...
[36m(WorkerDict pid=2136241)[0m loading layer #17, with layer_name model.layers.17...
[36m(WorkerDict pid=2136241)[0m loading layer #18, with layer_name model.layers.18...
[36m(WorkerDict pid=2136241)[0m loading layer #19, with layer_name model.layers.19...
[36m(WorkerDict pid=2136241)[0m loading layer #20, with layer_name model.layers.20...
[36m(WorkerDict pid=2136241)[0m loading layer #21, with layer_name model.layers.21...
[36m(WorkerDict pid=2136241)[0m loading layer #22, with layer_name model.layers.22...
[36m(WorkerDict pid=2136241)[0m loading layer #23, with layer_name model.layers.23...
[36m(WorkerDict pid=2136241)[0m loading layer #24, with layer_name model.layers.24...
[36m(WorkerDict pid=2136241)[0m loading layer #25, with layer_name model.layers.25...
[36m(WorkerDict pid=2136241)[0m loading layer #26, with layer_name model.layers.26...
[36m(WorkerDict pid=2136241)[0m loading layer #27, with layer_name model.layers.27...
[36m(WorkerDict pid=2136241)[0m loading layer #28, with layer_name model.layers.28...
[36m(WorkerDict pid=2136241)[0m loading layer #29, with layer_name model.layers.29...
[36m(WorkerDict pid=2136241)[0m loading layer #30, with layer_name model.layers.30...
[36m(WorkerDict pid=2136241)[0m loading layer #31, with layer_name model.layers.31...
[36m(WorkerDict pid=2136241)[0m loading layer #32, with layer_name model.layers.32...
[36m(WorkerDict pid=2136241)[0m loading layer #33, with layer_name model.layers.33...
[36m(WorkerDict pid=2136241)[0m loading layer #34, with layer_name model.layers.34...
[36m(WorkerDict pid=2136241)[0m loading layer #35, with layer_name model.layers.35...
[36m(WorkerDict pid=2136241)[0m loading layer #36, with layer_name model.layers.36...
[36m(WorkerDict pid=2136241)[0m loading layer #37, with layer_name model.layers.37...
[36m(WorkerDict pid=2136241)[0m loading layer #38, with layer_name model.layers.38...
[36m(WorkerDict pid=2136241)[0m loading layer #39, with layer_name model.layers.39...
[36m(WorkerDict pid=2136241)[0m loading layer #40, with layer_name model.layers.40...
[36m(WorkerDict pid=2136241)[0m loading layer #41, with layer_name model.layers.41...
[36m(WorkerDict pid=2136241)[0m loading layer #42, with layer_name model.layers.42...
[36m(WorkerDict pid=2136241)[0m loading layer #43, with layer_name model.layers.43...
[36m(WorkerDict pid=2136241)[0m loading layer #44, with layer_name model.layers.44...
[36m(WorkerDict pid=2136241)[0m loading layer #45, with layer_name model.layers.45...
[36m(WorkerDict pid=2136241)[0m loading layer #46, with layer_name model.layers.46...
[36m(WorkerDict pid=2136241)[0m loading layer #47, with layer_name model.layers.47...
[36m(WorkerDict pid=2136241)[0m loading layer #48, with layer_name model.layers.48...
[36m(WorkerDict pid=2136241)[0m loading layer #49, with layer_name model.layers.49...
[36m(WorkerDict pid=2136241)[0m loading layer #50, with layer_name model.layers.50...
[36m(WorkerDict pid=2136241)[0m loading layer #51, with layer_name model.layers.51...
[36m(WorkerDict pid=2136241)[0m loading layer #52, with layer_name model.layers.52...
[36m(WorkerDict pid=2136241)[0m loading layer #53, with layer_name model.layers.53...
[36m(WorkerDict pid=2136241)[0m loading layer #54, with layer_name model.layers.54...
[36m(WorkerDict pid=2136241)[0m loading layer #55, with layer_name model.layers.55...
[36m(WorkerDict pid=2136241)[0m loading layer #56, with layer_name model.layers.56...
[36m(WorkerDict pid=2136241)[0m loading layer #57, with layer_name model.layers.57...
[36m(WorkerDict pid=2136241)[0m loading layer #58, with layer_name model.layers.58...
[36m(WorkerDict pid=2136241)[0m loading layer #59, with layer_name model.layers.59...
[36m(WorkerDict pid=2136241)[0m loading layer #60, with layer_name model.layers.60...
[36m(WorkerDict pid=2136241)[0m loading layer #61, with layer_name model.layers.61...
[36m(WorkerDict pid=2136241)[0m loading layer #62, with layer_name model.layers.62...
[36m(WorkerDict pid=2136241)[0m loading layer #63, with layer_name model.layers.63...
[36m(WorkerDict pid=2136241)[0m loading final layernorm...
[36m(WorkerDict pid=2136241)[0m loading lm_head...
[36m(WorkerDict pid=2136244)[0m [Warining] Because actor tp size == 1, set sp to False
[36m(WorkerDict pid=2136244)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff38275ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff380ebe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff380ebe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='323', worker_launch_time_ms='1765646970634', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='2131114654', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136247)[0m [Warining] Because actor tp size == 1, set sp to False
[36m(WorkerDict pid=2136247)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff30561ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff303d7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff303d7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='324', worker_launch_time_ms='1765646970657', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='1550268023', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136248)[0m [Warining] Because actor tp size == 1, set sp to False
[36m(WorkerDict pid=2136248)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff30265ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff300dbe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff300dbe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='325', worker_launch_time_ms='1765646970680', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='-769289954', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136253)[0m [Warining] Because actor tp size == 1, set sp to False
[36m(WorkerDict pid=2136253)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xfffefc561ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xfffefc3d7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xfffefc3d7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='327', worker_launch_time_ms='1765646970724', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='-624206571', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136249)[0m [Warining] Because actor tp size == 1, set sp to False
[36m(WorkerDict pid=2136249)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff2c745ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff2c5b3e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff2c5b3e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='326', worker_launch_time_ms='1765646970702', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='1153430005', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136254)[0m [Warining] Because actor tp size == 1, set sp to False
[36m(WorkerDict pid=2136254)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff2835dee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff281d3e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff281d3e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='328', worker_launch_time_ms='1765646970746', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='-1053958217', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136256)[0m [Warining] Because actor tp size == 1, set sp to False
[36m(WorkerDict pid=2136256)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff30165ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff0c6afe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff0c6afe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='330', worker_launch_time_ms='1765646970792', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='1715011496', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136255)[0m [Warining] Because actor tp size == 1, set sp to False
[36m(WorkerDict pid=2136257)[0m [Warining] Because actor tp size == 1, set sp to False
[36m(WorkerDict pid=2136257)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff34265ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff340dbe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff340dbe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='331', worker_launch_time_ms='1765646970817', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='537201117', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136258)[0m [Warining] Because actor tp size == 1, set sp to False
[36m(WorkerDict pid=2136258)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xfffef8561ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xfffef83d7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xfffef83d7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='332', worker_launch_time_ms='1765646970842', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='1618725680', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136261)[0m [Warining] Because actor tp size == 1, set sp to False
[36m(WorkerDict pid=2136261)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xfffefc465ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xfffefc2dbe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xfffefc2dbe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='335', worker_launch_time_ms='1765646970935', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='1948273534', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136260)[0m [Warining] Because actor tp size == 1, set sp to False
[36m(WorkerDict pid=2136260)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff2c171ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff086bbe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff086bbe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='334', worker_launch_time_ms='1765646970913', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='-1925062186', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136259)[0m [Warining] Because actor tp size == 1, set sp to False
[36m(WorkerDict pid=2136259)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff38561ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff383d7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff383d7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='333', worker_launch_time_ms='1765646970868', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='957585563', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136265)[0m [Warining] Because actor tp size == 1, set sp to False
[36m(WorkerDict pid=2136265)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff04741ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff045b3e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff045b3e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='336', worker_launch_time_ms='1765646970959', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='-75012150', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136269)[0m [Warining] Because actor tp size == 1, set sp to False
[36m(WorkerDict pid=2136269)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff30171ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff186bbe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff186bbe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='337', worker_launch_time_ms='1765646970983', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='-1558909396', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136255)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xfffee4639ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xfffee44abe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xfffee44abe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='329', worker_launch_time_ms='1765646970769', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='-1486323873', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136241)[0m loading megatron ckpt done, time elapsed 22.844329357147217s
[36m(WorkerDict pid=2136241)[0m [Warining] Because actor tp size == 1, set sp to False
[36m(WorkerDict pid=2136241)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff04265ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff040dbe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff040dbe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='322', worker_launch_time_ms='1765646970603', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='-1442360010', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136248)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff30265ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff300dbe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff300dbe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='325', worker_launch_time_ms='1765646970680', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='-769289954', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136257)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff34265ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff340dbe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff340dbe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='331', worker_launch_time_ms='1765646970817', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='537201117', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136258)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xfffef8561ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xfffef83d7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xfffef83d7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='332', worker_launch_time_ms='1765646970842', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='1618725680', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136244)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff38275ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff380ebe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff380ebe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='323', worker_launch_time_ms='1765646970634', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='2131114654', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136249)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff2c745ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff2c5b3e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff2c5b3e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='326', worker_launch_time_ms='1765646970702', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='1153430005', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136256)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff30165ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff0c6afe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff0c6afe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='330', worker_launch_time_ms='1765646970792', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='1715011496', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136269)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff30171ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff186bbe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff186bbe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='337', worker_launch_time_ms='1765646970983', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='-1558909396', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136241)[0m Model config after override: Qwen3Config {
[36m(WorkerDict pid=2136241)[0m   "architectures": [
[36m(WorkerDict pid=2136241)[0m     "Qwen3ForCausalLM"
[36m(WorkerDict pid=2136241)[0m   ],
[36m(WorkerDict pid=2136241)[0m   "attention_bias": false,
[36m(WorkerDict pid=2136241)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=2136241)[0m   "eos_token_id": 151645,
[36m(WorkerDict pid=2136241)[0m   "head_dim": 128,
[36m(WorkerDict pid=2136241)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=2136241)[0m   "hidden_size": 5120,
[36m(WorkerDict pid=2136241)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=2136241)[0m   "intermediate_size": 25600,
[36m(WorkerDict pid=2136241)[0m   "layer_types": [
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention",
[36m(WorkerDict pid=2136241)[0m     "full_attention"
[36m(WorkerDict pid=2136241)[0m   ],
[36m(WorkerDict pid=2136241)[0m   "max_position_embeddings": 40960,
[36m(WorkerDict pid=2136241)[0m   "max_window_layers": 64,
[36m(WorkerDict pid=2136241)[0m   "model_type": "qwen3",
[36m(WorkerDict pid=2136241)[0m   "num_attention_heads": 64,
[36m(WorkerDict pid=2136241)[0m   "num_hidden_layers": 64,
[36m(WorkerDict pid=2136241)[0m   "num_key_value_heads": 8,
[36m(WorkerDict pid=2136241)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=2136241)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=2136241)[0m   "rope_scaling": null,
[36m(WorkerDict pid=2136241)[0m   "rope_theta": 1000000,
[36m(WorkerDict pid=2136241)[0m   "sliding_window": null,
[36m(WorkerDict pid=2136241)[0m   "tie_word_embeddings": false,
[36m(WorkerDict pid=2136241)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=2136241)[0m   "transformers_version": "4.55.0",
[36m(WorkerDict pid=2136241)[0m   "use_cache": true,
[36m(WorkerDict pid=2136241)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=2136241)[0m   "vocab_size": 151936
[36m(WorkerDict pid=2136241)[0m }
[36m(WorkerDict pid=2136241)[0m 
[36m(WorkerDict pid=2136241)[0m Overridden TransformerConfig init config: {'num_layers': 64, 'hidden_size': 5120, 'num_attention_heads': 64, 'num_query_groups': 8, 'ffn_hidden_size': 25600, 'attention_dropout': 0.0, 'hidden_dropout': 0.0, 'kv_channels': 128, 'layernorm_epsilon': 1e-06, 'add_bias_linear': False, 'activation_func': <function silu at 0xffff04265ee0>, 'normalization': 'RMSNorm', 'gated_linear_unit': True, 'pipeline_dtype': torch.bfloat16, 'params_dtype': torch.bfloat16, 'bf16': True, 'tensor_model_parallel_size': 8, 'pipeline_model_parallel_size': 2, 'expert_model_parallel_size': 1, 'expert_tensor_parallel_size': 1, 'virtual_pipeline_model_parallel_size': None, 'context_parallel_size': 1, 'overlap_p2p_comm': False, 'batch_p2p_comm': False, 'sequence_parallel': True, 'variable_seq_lengths': True, 'masked_softmax_fusion': True, 'moe_token_dispatcher_type': 'alltoall', 'use_cpu_initialization': False, 'add_qkv_bias': False, 'qk_layernorm': True, 'recompute_granularity': 'full', 'recompute_modules': ['core_attn'], 'recompute_method': 'uniform', 'recompute_num_layers': 1, 'attention_backend': <AttnBackend.flash: 1>}
[36m(WorkerDict pid=2136241)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff04265ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff040dbe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff040dbe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='322', worker_launch_time_ms='1765646970603', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='-1442360010', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136247)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff30561ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff303d7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff303d7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='324', worker_launch_time_ms='1765646970657', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='1550268023', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136254)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff2835dee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff281d3e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff281d3e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='328', worker_launch_time_ms='1765646970746', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='-1053958217', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136261)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xfffefc465ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xfffefc2dbe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xfffefc2dbe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='335', worker_launch_time_ms='1765646970935', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='1948273534', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136260)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff2c171ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff086bbe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff086bbe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='334', worker_launch_time_ms='1765646970913', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='-1925062186', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136265)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff04741ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff045b3e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff045b3e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='336', worker_launch_time_ms='1765646970959', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='-75012150', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136244)[0m  > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 2047926272
[36m(WorkerDict pid=2136248)[0m  > number of parameters on (tensor, pipeline) model parallel rank (3, 0): 2047926272
[36m(WorkerDict pid=2136253)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xfffefc561ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xfffefc3d7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xfffefc3d7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='327', worker_launch_time_ms='1765646970724', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='-624206571', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136249)[0m  > number of parameters on (tensor, pipeline) model parallel rank (3, 1): 2047931392
[36m(WorkerDict pid=2136256)[0m  > number of parameters on (tensor, pipeline) model parallel rank (7, 0): 2047926272
[36m(WorkerDict pid=2136255)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xfffee4639ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xfffee44abe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xfffee44abe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='329', worker_launch_time_ms='1765646970769', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='-1486323873', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136257)[0m  > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 2047931392
[36m(WorkerDict pid=2136258)[0m  > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 2047931392
[36m(WorkerDict pid=2136259)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff38561ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff383d7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff383d7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='333', worker_launch_time_ms='1765646970868', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='957585563', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136269)[0m  > number of parameters on (tensor, pipeline) model parallel rank (6, 1): 2047931392
[36m(WorkerDict pid=2136247)[0m  > number of parameters on (tensor, pipeline) model parallel rank (2, 0): 2047926272
[36m(WorkerDict pid=2136257)[0m [Rank 8 | Local Rank 0] 2025-12-14 01:30:31,034 INFO [megatron.core.distributed.param_and_grad_buffer:553] => Number of buckets for gradient all-reduce / reduce-scatter: 1
[36m(WorkerDict pid=2136257)[0m Params for bucket 1 (2047931392 elements, 2047931392 padded size):
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.12.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.9.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.29.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.25.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.21.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.17.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.15.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.14.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.13.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.11.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.10.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.9.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.6.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.1.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.26.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.30.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.29.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.22.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.28.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.25.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.21.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.20.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.18.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.11.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.25.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.23.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.26.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.30.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.31.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.29.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.27.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.22.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.21.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.19.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.16.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.8.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.3.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.13.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.final_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.28.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.24.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.20.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.15.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.14.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.11.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.12.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.5.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.16.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.10.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.6.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.2.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.9.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.output_layer.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.30.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.26.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.22.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.18.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.18.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.16.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.15.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.14.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.13.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.7.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.6.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.3.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.10.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.4.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.0.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.26.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.30.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.31.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.29.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.23.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.27.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.25.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.22.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.21.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.19.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.7.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.7.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.1.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.4.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.0.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.26.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.23.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.24.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.30.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.31.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.28.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.27.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.22.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.20.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.19.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.0.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.4.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.29.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.25.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.21.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.18.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.16.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.15.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.12.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.11.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.1.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.17.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.9.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.0.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.3.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.31.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.27.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.23.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.19.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.18.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.17.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.16.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.15.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.14.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.13.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.10.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.6.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.4.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.2.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.1.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.0.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.0.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.17.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.11.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.7.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.2.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.26.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.23.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.30.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.31.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.28.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.27.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.24.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.22.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.20.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.19.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.14.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.9.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.8.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.4.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.7.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.11.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.5.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.25.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.23.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.27.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.31.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.29.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.24.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.28.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.21.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.20.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.19.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.8.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.6.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.4.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.1.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.8.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.5.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.30.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.26.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.22.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.17.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.16.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.18.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.13.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.12.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.8.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.5.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.3.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.2.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.28.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.24.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.20.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.19.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.17.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.16.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.14.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.13.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.12.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.10.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.3.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.10.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.1.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.23.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.27.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.29.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.31.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.24.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.28.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.25.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.21.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.20.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.19.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.15.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.7.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.2.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.18.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.12.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.8.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.25.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.22.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.26.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.30.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.29.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.24.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.28.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.21.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.20.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.18.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.11.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.2.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.15.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.9.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.5.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.3.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.8.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.23.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.31.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.27.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.17.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.16.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.14.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.13.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.12.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.10.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.6.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.9.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136257)[0m 	module.decoder.layers.5.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136241)[0m  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 2047926272
[36m(WorkerDict pid=2136241)[0m [Rank 0 | Local Rank 0] 2025-12-14 01:30:31,068 INFO [megatron.core.distributed.distributed_data_parallel:532] => Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=True, overlap_grad_reduce=False, overlap_param_gather=False, align_param_gather=False, use_distributed_optimizer=True, num_distributed_optimizer_instances=1, check_for_nan_in_grad=False, check_for_large_grads=False, bucket_size=None, pad_buckets_for_high_nccl_busbw=False, average_in_collective=False, fp8_param_gather=False, use_custom_fsdp=False, data_parallel_sharding_strategy='no_shard', gradient_reduce_div_fusion=True, suggested_communication_unit_size=None, preserve_fp32_weights=True, keep_fp8_transpose_cache_when_using_custom_fsdp=False)
[36m(WorkerDict pid=2136241)[0m [Rank 0 | Local Rank 0] 2025-12-14 01:30:31,154 INFO [megatron.core.distributed.param_and_grad_buffer:553] => Number of buckets for gradient all-reduce / reduce-scatter: 1
[36m(WorkerDict pid=2136241)[0m Params for bucket 1 (2047926272 elements, 2047926272 padded size):
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.8.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.5.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.30.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.31.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.28.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.22.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.26.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.27.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.18.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.15.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.14.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.9.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.2.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.2.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.9.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.8.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.27.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.29.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.31.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.28.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.24.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.20.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.16.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.13.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.12.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.5.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.4.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.9.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.23.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.24.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.30.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.27.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.25.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.21.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.20.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.19.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.17.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.16.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.13.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.7.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.1.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.5.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.4.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.0.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.1.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.25.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.22.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.24.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.28.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.21.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.20.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.18.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.17.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.16.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.14.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.11.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.12.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.7.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.29.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.31.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.28.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.23.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.27.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.19.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.15.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.11.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.9.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.6.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.11.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.3.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.28.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.30.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.29.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.25.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.26.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.21.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.17.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.13.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.13.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.11.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.8.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.2.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.8.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.1.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.13.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.2.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.9.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.22.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.31.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.25.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.21.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.20.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.18.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.17.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.16.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.14.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.14.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.1.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.5.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.22.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.25.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.23.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.29.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.26.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.21.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.19.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.18.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.17.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.15.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.14.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.10.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.1.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.8.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.12.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.3.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.28.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.30.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.29.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.26.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.24.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.20.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.16.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.11.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.11.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.10.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.6.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.7.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.12.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.3.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.8.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.6.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.2.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.27.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.31.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.30.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.29.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.22.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.18.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.14.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.10.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.7.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.12.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.2.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.4.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.23.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.28.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.26.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.25.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.22.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.21.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.19.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.18.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.15.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.10.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.9.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.6.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.14.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.23.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.24.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.30.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.26.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.26.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.22.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.20.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.19.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.18.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.16.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.7.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.5.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.3.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.5.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.0.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.10.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.31.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.30.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.29.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.25.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.27.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.21.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.17.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.12.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.8.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.4.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.12.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.15.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.7.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.0.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.4.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.27.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.23.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.30.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.31.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.28.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.19.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.15.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.10.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.11.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.10.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.3.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136241)[0m 	module.embedding.word_embeddings.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.0.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.13.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.4.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.6.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.23.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.29.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.26.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.24.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.22.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.20.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.19.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.18.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.16.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.15.self_attention.linear_qkv.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.9.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.3.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.0.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.1.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.13.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.0.self_attention.q_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.6.mlp.linear_fc2.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.25.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.23.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.24.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.31.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.27.mlp.linear_fc1.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.21.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.20.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.19.self_attention.linear_proj.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.17.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.16.self_attention.k_layernorm.weight
[36m(WorkerDict pid=2136241)[0m 	module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=2136244)[0m actor_module: 1
[36m(WorkerDict pid=2136244)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=2136248)[0m actor_module: 1
[36m(WorkerDict pid=2136248)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=2136253)[0m  > number of parameters on (tensor, pipeline) model parallel rank (4, 0): 2047926272
[36m(WorkerDict pid=2136249)[0m actor_module: 1
[36m(WorkerDict pid=2136249)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=2136254)[0m  > number of parameters on (tensor, pipeline) model parallel rank (5, 0): 2047926272
[36m(WorkerDict pid=2136256)[0m actor_module: 1
[36m(WorkerDict pid=2136256)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=2136257)[0m actor_module: 1
[36m(WorkerDict pid=2136257)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=2136258)[0m actor_module: 1
[36m(WorkerDict pid=2136258)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=2136261)[0m  > number of parameters on (tensor, pipeline) model parallel rank (5, 1): 2047931392
[36m(WorkerDict pid=2136260)[0m  > number of parameters on (tensor, pipeline) model parallel rank (4, 1): 2047931392
[36m(WorkerDict pid=2136265)[0m  > number of parameters on (tensor, pipeline) model parallel rank (7, 1): 2047931392
[36m(WorkerDict pid=2136269)[0m actor_module: 1
[36m(WorkerDict pid=2136269)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=2136241)[0m actor_module: 1
[36m(WorkerDict pid=2136241)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=2136247)[0m actor_module: 1
[36m(WorkerDict pid=2136247)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=2136254)[0m actor_module: 1
[36m(WorkerDict pid=2136254)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=2136255)[0m  > number of parameters on (tensor, pipeline) model parallel rank (6, 0): 2047926272
[36m(WorkerDict pid=2136261)[0m actor_module: 1
[36m(WorkerDict pid=2136261)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=2136260)[0m actor_module: 1
[36m(WorkerDict pid=2136260)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=2136259)[0m  > number of parameters on (tensor, pipeline) model parallel rank (2, 1): 2047931392
[36m(WorkerDict pid=2136265)[0m actor_module: 1
[36m(WorkerDict pid=2136265)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=2136253)[0m actor_module: 1
[36m(WorkerDict pid=2136253)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=2136255)[0m actor_module: 1
[36m(WorkerDict pid=2136255)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=2136259)[0m actor_module: 1
[36m(WorkerDict pid=2136259)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=2136241)[0m loading embeddings...
[36m(WorkerDict pid=2136241)[0m loading layer #0, with layer_name model.layers.0...
[36m(WorkerDict pid=2136241)[0m loading layer #1, with layer_name model.layers.1...
[36m(WorkerDict pid=2136241)[0m loading layer #2, with layer_name model.layers.2...
[36m(WorkerDict pid=2136241)[0m loading layer #3, with layer_name model.layers.3...
[36m(WorkerDict pid=2136241)[0m loading layer #4, with layer_name model.layers.4...
[36m(WorkerDict pid=2136241)[0m loading layer #5, with layer_name model.layers.5...
[36m(WorkerDict pid=2136241)[0m loading layer #6, with layer_name model.layers.6...
[36m(WorkerDict pid=2136241)[0m loading layer #7, with layer_name model.layers.7...
[36m(WorkerDict pid=2136241)[0m loading layer #8, with layer_name model.layers.8...
[36m(WorkerDict pid=2136241)[0m loading layer #9, with layer_name model.layers.9...
[36m(WorkerDict pid=2136241)[0m loading layer #10, with layer_name model.layers.10...
[36m(WorkerDict pid=2136241)[0m loading layer #11, with layer_name model.layers.11...
[36m(WorkerDict pid=2136241)[0m loading layer #12, with layer_name model.layers.12...
[36m(WorkerDict pid=2136241)[0m loading layer #13, with layer_name model.layers.13...
[36m(WorkerDict pid=2136241)[0m loading layer #14, with layer_name model.layers.14...
[36m(WorkerDict pid=2136241)[0m loading layer #15, with layer_name model.layers.15...
[36m(WorkerDict pid=2136241)[0m loading layer #16, with layer_name model.layers.16...
[36m(WorkerDict pid=2136241)[0m loading layer #17, with layer_name model.layers.17...
[36m(WorkerDict pid=2136241)[0m loading layer #18, with layer_name model.layers.18...
[36m(WorkerDict pid=2136241)[0m loading layer #19, with layer_name model.layers.19...
[36m(WorkerDict pid=2136241)[0m loading layer #20, with layer_name model.layers.20...
[36m(WorkerDict pid=2136241)[0m loading layer #21, with layer_name model.layers.21...
[36m(WorkerDict pid=2136241)[0m loading layer #22, with layer_name model.layers.22...
[36m(WorkerDict pid=2136241)[0m loading layer #23, with layer_name model.layers.23...
[36m(WorkerDict pid=2136241)[0m loading layer #24, with layer_name model.layers.24...
[36m(WorkerDict pid=2136241)[0m loading layer #25, with layer_name model.layers.25...
[36m(WorkerDict pid=2136241)[0m loading layer #26, with layer_name model.layers.26...
[36m(WorkerDict pid=2136241)[0m loading layer #27, with layer_name model.layers.27...
[36m(WorkerDict pid=2136241)[0m loading layer #28, with layer_name model.layers.28...
[36m(WorkerDict pid=2136241)[0m loading layer #29, with layer_name model.layers.29...
[36m(WorkerDict pid=2136241)[0m loading layer #30, with layer_name model.layers.30...
[36m(WorkerDict pid=2136241)[0m loading layer #31, with layer_name model.layers.31...
[36m(WorkerDict pid=2136241)[0m loading layer #32, with layer_name model.layers.32...
[36m(WorkerDict pid=2136241)[0m loading layer #33, with layer_name model.layers.33...
[36m(WorkerDict pid=2136241)[0m loading layer #34, with layer_name model.layers.34...
[36m(WorkerDict pid=2136241)[0m loading layer #35, with layer_name model.layers.35...
[36m(WorkerDict pid=2136241)[0m loading layer #36, with layer_name model.layers.36...
[36m(WorkerDict pid=2136241)[0m loading layer #37, with layer_name model.layers.37...
[36m(WorkerDict pid=2136241)[0m loading layer #38, with layer_name model.layers.38...
[36m(WorkerDict pid=2136241)[0m loading layer #39, with layer_name model.layers.39...
[36m(WorkerDict pid=2136241)[0m loading layer #40, with layer_name model.layers.40...
[36m(WorkerDict pid=2136241)[0m loading layer #41, with layer_name model.layers.41...
[36m(WorkerDict pid=2136241)[0m loading layer #42, with layer_name model.layers.42...
[36m(WorkerDict pid=2136241)[0m loading layer #43, with layer_name model.layers.43...
[36m(WorkerDict pid=2136241)[0m loading layer #44, with layer_name model.layers.44...
[36m(WorkerDict pid=2136241)[0m loading layer #45, with layer_name model.layers.45...
[36m(WorkerDict pid=2136241)[0m loading layer #46, with layer_name model.layers.46...
[36m(WorkerDict pid=2136241)[0m loading layer #47, with layer_name model.layers.47...
[36m(WorkerDict pid=2136241)[0m loading layer #48, with layer_name model.layers.48...
[36m(WorkerDict pid=2136241)[0m loading layer #49, with layer_name model.layers.49...
[36m(WorkerDict pid=2136241)[0m loading layer #50, with layer_name model.layers.50...
[36m(WorkerDict pid=2136241)[0m loading layer #51, with layer_name model.layers.51...
[36m(WorkerDict pid=2136241)[0m loading layer #52, with layer_name model.layers.52...
[36m(WorkerDict pid=2136241)[0m loading layer #53, with layer_name model.layers.53...
[36m(WorkerDict pid=2136241)[0m loading layer #54, with layer_name model.layers.54...
[36m(WorkerDict pid=2136241)[0m loading layer #55, with layer_name model.layers.55...
[36m(WorkerDict pid=2136241)[0m loading layer #56, with layer_name model.layers.56...
[36m(WorkerDict pid=2136241)[0m loading layer #57, with layer_name model.layers.57...
[36m(WorkerDict pid=2136241)[0m loading layer #58, with layer_name model.layers.58...
[36m(WorkerDict pid=2136241)[0m loading layer #59, with layer_name model.layers.59...
[36m(WorkerDict pid=2136241)[0m loading layer #60, with layer_name model.layers.60...
[36m(WorkerDict pid=2136241)[0m loading layer #61, with layer_name model.layers.61...
[36m(WorkerDict pid=2136241)[0m loading layer #62, with layer_name model.layers.62...
[36m(WorkerDict pid=2136241)[0m loading layer #63, with layer_name model.layers.63...
[36m(WorkerDict pid=2136241)[0m loading final layernorm...
[36m(WorkerDict pid=2136241)[0m loading lm_head...
[36m(WorkerDict pid=2136241)[0m loading megatron ckpt done, time elapsed 18.551682472229004s
[36m(WorkerDict pid=2136244)[0m [Rank 0] swap optimizer: 2047926272 (23436.65625 MB)/2047926272
[36m(WorkerDict pid=2136247)[0m [Rank 0] swap optimizer: 2047926272 (23436.65625 MB)/2047926272
[36m(WorkerDict pid=2136248)[0m [Rank 0] swap optimizer: 2047926272 (23436.65625 MB)/2047926272
[36m(WorkerDict pid=2136253)[0m [Rank 0] swap optimizer: 2047926272 (23436.65625 MB)/2047926272
[36m(WorkerDict pid=2136249)[0m [Rank 0] swap optimizer: 2047931392 (23436.71484375 MB)/2047931392
[36m(WorkerDict pid=2136254)[0m [Rank 0] swap optimizer: 2047926272 (23436.65625 MB)/2047926272
[36m(WorkerDict pid=2136256)[0m [Rank 0] swap optimizer: 2047926272 (23436.65625 MB)/2047926272
[36m(WorkerDict pid=2136257)[0m [Rank 0] swap optimizer: 2047931392 (23436.71484375 MB)/2047931392
[36m(WorkerDict pid=2136258)[0m [Rank 0] swap optimizer: 2047931392 (23436.71484375 MB)/2047931392
[36m(WorkerDict pid=2136261)[0m [Rank 0] swap optimizer: 2047931392 (23436.71484375 MB)/2047931392
[36m(WorkerDict pid=2136260)[0m [Rank 0] swap optimizer: 2047931392 (23436.71484375 MB)/2047931392
[36m(WorkerDict pid=2136265)[0m [Rank 0] swap optimizer: 2047931392 (23436.71484375 MB)/2047931392
[36m(WorkerDict pid=2136269)[0m [Rank 0] swap optimizer: 2047931392 (23436.71484375 MB)/2047931392
[36m(WorkerDict pid=2136255)[0m [Rank 0] swap optimizer: 2047926272 (23436.65625 MB)/2047926272
[36m(WorkerDict pid=2136259)[0m [Rank 0] swap optimizer: 2047931392 (23436.71484375 MB)/2047931392
[36m(WorkerDict pid=2136241)[0m DistributedDataParallel contains 2.05B parameters
[36m(WorkerDict pid=2136241)[0m optimizer config after override: {'optimizer': 'adam', 'lr': 1e-06, 'min_lr': 0.0, 'clip_grad': 10000, 'weight_decay': 0.01, 'bf16': True, 'params_dtype': torch.bfloat16, 'use_distributed_optimizer': True}
[36m(WorkerDict pid=2136241)[0m [Rank 0 | Local Rank 0] 2025-12-14 01:30:53,385 INFO [megatron.core.optimizer:532] => Setting up optimizer with config OptimizerConfig(optimizer='adam', lr=1e-06, min_lr=0.0, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=True, params_dtype=torch.bfloat16, use_precision_aware_optimizer=False, main_grads_dtype=torch.float32, main_params_dtype=torch.float32, exp_avg_dtype=torch.float32, exp_avg_sq_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=True, overlap_param_gather_with_optimizer_step=False, optimizer_cpu_offload=False, optimizer_offload_fraction=0.0, use_torch_optimizer_for_cpu_offload=False, overlap_cpu_optimizer_d2h_h2d=False, pin_cpu_grads=True, pin_cpu_params=True, clip_grad=10000, log_num_zeros_in_grad=False, barrier_with_L1_time=False, timers=None, config_logger_dir='')
[36m(WorkerDict pid=2136258)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xfffef8561ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xfffef83d7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xfffef83d7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='332', worker_launch_time_ms='1765646970842', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='1618725680', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136241)[0m [Rank 0] swap optimizer: 2047926272 (23436.65625 MB)/2047926272
[36m(WorkerDict pid=2136241)[0m [Rank 0 | Local Rank 0] 2025-12-14 01:30:53,910 INFO [megatron.core.optimizer_param_scheduler:532] => > learning rate decay style: constant
[36m(WorkerDict pid=2136257)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff34265ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff340dbe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff340dbe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='331', worker_launch_time_ms='1765646970817', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='537201117', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136248)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff30265ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff300dbe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff300dbe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='325', worker_launch_time_ms='1765646970680', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='-769289954', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136258)[0m INFO 12-14 01:30:54 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
[36m(WorkerDict pid=2136265)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff04741ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff045b3e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff045b3e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='336', worker_launch_time_ms='1765646970959', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='-75012150', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136269)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff30171ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff186bbe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff186bbe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='337', worker_launch_time_ms='1765646970983', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='-1558909396', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136254)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff2835dee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff281d3e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff281d3e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='328', worker_launch_time_ms='1765646970746', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='-1053958217', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136257)[0m INFO 12-14 01:30:54 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
[36m(WorkerDict pid=2136258)[0m WARNING 12-14 01:30:54 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136258)[0m WARNING 12-14 01:30:54 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136258)[0m INFO 12-14 01:30:54 [parallel_state.py:1047] Adjusting world_size=2 rank=9 distributed_init_method=tcp://127.0.0.1:0 for DP
[36m(WorkerDict pid=2136257)[0m WARNING 12-14 01:30:54 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136257)[0m WARNING 12-14 01:30:54 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136257)[0m INFO 12-14 01:30:54 [parallel_state.py:1047] Adjusting world_size=2 rank=8 distributed_init_method=tcp://127.0.0.1:0 for DP
[36m(WorkerDict pid=2136248)[0m INFO 12-14 01:30:55 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
[36m(WorkerDict pid=2136248)[0m WARNING 12-14 01:30:55 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136248)[0m WARNING 12-14 01:30:55 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136248)[0m INFO 12-14 01:30:55 [parallel_state.py:1047] Adjusting world_size=2 rank=3 distributed_init_method=tcp://127.0.0.1:0 for DP
[36m(WorkerDict pid=2136265)[0m INFO 12-14 01:30:55 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
[36m(WorkerDict pid=2136265)[0m WARNING 12-14 01:30:55 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136265)[0m WARNING 12-14 01:30:55 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136265)[0m INFO 12-14 01:30:55 [parallel_state.py:1047] Adjusting world_size=2 rank=15 distributed_init_method=tcp://127.0.0.1:0 for DP
[36m(WorkerDict pid=2136269)[0m INFO 12-14 01:30:55 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
[36m(WorkerDict pid=2136254)[0m INFO 12-14 01:30:55 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
[36m(WorkerDict pid=2136269)[0m WARNING 12-14 01:30:55 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136269)[0m WARNING 12-14 01:30:55 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136269)[0m INFO 12-14 01:30:55 [parallel_state.py:1047] Adjusting world_size=2 rank=14 distributed_init_method=tcp://127.0.0.1:0 for DP
[36m(WorkerDict pid=2136254)[0m WARNING 12-14 01:30:55 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136254)[0m WARNING 12-14 01:30:55 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136254)[0m INFO 12-14 01:30:55 [parallel_state.py:1047] Adjusting world_size=2 rank=5 distributed_init_method=tcp://127.0.0.1:0 for DP
[36m(WorkerDict pid=2136244)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff38275ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff380ebe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff380ebe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='323', worker_launch_time_ms='1765646970634', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='2131114654', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136256)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff30165ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff0c6afe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff0c6afe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='330', worker_launch_time_ms='1765646970792', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='1715011496', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136247)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff30561ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff303d7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff303d7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='324', worker_launch_time_ms='1765646970657', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='1550268023', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136253)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xfffefc561ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xfffefc3d7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xfffefc3d7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='327', worker_launch_time_ms='1765646970724', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='-624206571', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136249)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff2c745ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff2c5b3e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff2c5b3e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='326', worker_launch_time_ms='1765646970702', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='1153430005', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136255)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xfffee4639ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xfffee44abe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xfffee44abe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='329', worker_launch_time_ms='1765646970769', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='-1486323873', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136261)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xfffefc465ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xfffefc2dbe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xfffefc2dbe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='335', worker_launch_time_ms='1765646970935', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='1948273534', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136260)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff2c171ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff086bbe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff086bbe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='334', worker_launch_time_ms='1765646970913', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='-1925062186', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136259)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff38561ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff383d7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff383d7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='333', worker_launch_time_ms='1765646970868', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='957585563', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136244)[0m INFO 12-14 01:30:57 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
[36m(WorkerDict pid=2136244)[0m WARNING 12-14 01:30:57 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136244)[0m WARNING 12-14 01:30:57 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136244)[0m INFO 12-14 01:30:57 [parallel_state.py:1047] Adjusting world_size=2 rank=1 distributed_init_method=tcp://127.0.0.1:0 for DP
[36m(WorkerDict pid=2136256)[0m INFO 12-14 01:30:57 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
[36m(WorkerDict pid=2136256)[0m WARNING 12-14 01:30:57 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136256)[0m WARNING 12-14 01:30:57 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136256)[0m INFO 12-14 01:30:57 [parallel_state.py:1047] Adjusting world_size=2 rank=7 distributed_init_method=tcp://127.0.0.1:0 for DP
[36m(WorkerDict pid=2136247)[0m INFO 12-14 01:30:57 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
[36m(WorkerDict pid=2136249)[0m INFO 12-14 01:30:57 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
[36m(WorkerDict pid=2136247)[0m WARNING 12-14 01:30:57 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136247)[0m WARNING 12-14 01:30:57 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136247)[0m INFO 12-14 01:30:57 [parallel_state.py:1047] Adjusting world_size=2 rank=2 distributed_init_method=tcp://127.0.0.1:0 for DP
[36m(WorkerDict pid=2136253)[0m INFO 12-14 01:30:57 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
[36m(WorkerDict pid=2136253)[0m WARNING 12-14 01:30:57 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136253)[0m WARNING 12-14 01:30:57 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136253)[0m INFO 12-14 01:30:57 [parallel_state.py:1047] Adjusting world_size=2 rank=4 distributed_init_method=tcp://127.0.0.1:0 for DP
[36m(WorkerDict pid=2136249)[0m WARNING 12-14 01:30:57 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136249)[0m WARNING 12-14 01:30:57 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136249)[0m INFO 12-14 01:30:57 [parallel_state.py:1047] Adjusting world_size=2 rank=11 distributed_init_method=tcp://127.0.0.1:0 for DP
[36m(WorkerDict pid=2136261)[0m INFO 12-14 01:30:57 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
[36m(WorkerDict pid=2136260)[0m INFO 12-14 01:30:57 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
[36m(WorkerDict pid=2136261)[0m WARNING 12-14 01:30:57 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136261)[0m WARNING 12-14 01:30:57 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136261)[0m INFO 12-14 01:30:57 [parallel_state.py:1047] Adjusting world_size=2 rank=13 distributed_init_method=tcp://127.0.0.1:0 for DP
[36m(WorkerDict pid=2136260)[0m WARNING 12-14 01:30:57 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136260)[0m WARNING 12-14 01:30:57 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136260)[0m INFO 12-14 01:30:57 [parallel_state.py:1047] Adjusting world_size=2 rank=12 distributed_init_method=tcp://127.0.0.1:0 for DP
[36m(WorkerDict pid=2136259)[0m INFO 12-14 01:30:57 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
[36m(WorkerDict pid=2136259)[0m WARNING 12-14 01:30:57 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136259)[0m WARNING 12-14 01:30:57 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136259)[0m INFO 12-14 01:30:57 [parallel_state.py:1047] Adjusting world_size=2 rank=10 distributed_init_method=tcp://127.0.0.1:0 for DP
[36m(WorkerDict pid=2136255)[0m INFO 12-14 01:30:57 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
[36m(WorkerDict pid=2136255)[0m WARNING 12-14 01:30:57 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136255)[0m WARNING 12-14 01:30:57 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136255)[0m INFO 12-14 01:30:57 [parallel_state.py:1047] Adjusting world_size=2 rank=6 distributed_init_method=tcp://127.0.0.1:0 for DP
[36m(WorkerDict pid=2136241)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff04265ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff040dbe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff040dbe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='43229', object_store_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-14_01-28-46_675047_2088733/sockets/raylet', redis_address='None', metrics_agent_port='57752', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='57767', gcs_address='192.168.0.163:29445', session_name='session_2025-12-14_01-28-46_675047_2088733', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='945ebedbdfca6db3d0c8081e9c93cd0705200a9cae37102e98b5f003', startup_token='322', worker_launch_time_ms='1765646970603', node_id='2697be1e17e709405a646c5d2c0ce25b1d692cb8029ddcdb3640dc2e', runtime_env_hash='-1442360010', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=2136241)[0m INFO 12-14 01:30:58 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
[36m(WorkerDict pid=2136241)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136241)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136241)[0m INFO 12-14 01:30:59 [parallel_state.py:1047] Adjusting world_size=2 rank=0 distributed_init_method=tcp://127.0.0.1:0 for DP
[36m(WorkerDict pid=2136241)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136241)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136241)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136241)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136241)[0m INFO 12-14 01:30:59 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer_handle=(7, 4194304, 6, 'psm_f0c46527'), local_subscribe_addr='ipc:///tmp/8beeefe1-d3df-481f-993d-295e61b5b201', remote_subscribe_addr=None, remote_addr_ipv6=False)
[36m(WorkerDict pid=2136241)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136241)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136241)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136241)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136241)[0m INFO 12-14 01:30:59 [parallel_state.py:1208] rank 0 in world size 16 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[36m(WorkerDict pid=2136244)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136244)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136244)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136244)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136244)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136244)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136244)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136244)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136244)[0m INFO 12-14 01:30:59 [parallel_state.py:1208] rank 1 in world size 16 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
[36m(WorkerDict pid=2136247)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136247)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136247)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136247)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136247)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136247)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136247)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136247)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136247)[0m INFO 12-14 01:30:59 [parallel_state.py:1208] rank 2 in world size 16 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
[36m(WorkerDict pid=2136248)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136248)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136248)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136248)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136248)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136248)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136248)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136248)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136248)[0m INFO 12-14 01:30:59 [parallel_state.py:1208] rank 3 in world size 16 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
[36m(WorkerDict pid=2136253)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136253)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136253)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136253)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136253)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136253)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136253)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136253)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136253)[0m INFO 12-14 01:30:59 [parallel_state.py:1208] rank 4 in world size 16 is assigned as DP rank 0, PP rank 0, TP rank 4, EP rank 4
[36m(WorkerDict pid=2136249)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136249)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136249)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136249)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136249)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136249)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136249)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136249)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136249)[0m INFO 12-14 01:30:59 [parallel_state.py:1208] rank 11 in world size 16 is assigned as DP rank 1, PP rank 0, TP rank 3, EP rank 11
[36m(WorkerDict pid=2136254)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136254)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136254)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136254)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136254)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136254)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136254)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136254)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136254)[0m INFO 12-14 01:30:59 [parallel_state.py:1208] rank 5 in world size 16 is assigned as DP rank 0, PP rank 0, TP rank 5, EP rank 5
[36m(WorkerDict pid=2136256)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136256)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136256)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136256)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136256)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136256)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136256)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136256)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136256)[0m INFO 12-14 01:30:59 [parallel_state.py:1208] rank 7 in world size 16 is assigned as DP rank 0, PP rank 0, TP rank 7, EP rank 7
[36m(WorkerDict pid=2136255)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136255)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136255)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136255)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136255)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136255)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136255)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136255)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136255)[0m INFO 12-14 01:30:59 [parallel_state.py:1208] rank 6 in world size 16 is assigned as DP rank 0, PP rank 0, TP rank 6, EP rank 6
[36m(WorkerDict pid=2136257)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136257)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136257)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136257)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136257)[0m INFO 12-14 01:30:59 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer_handle=(7, 4194304, 6, 'psm_06d0e1ba'), local_subscribe_addr='ipc:///tmp/2a1c6a28-e3dc-4028-a711-7b297c9d4521', remote_subscribe_addr=None, remote_addr_ipv6=False)
[36m(WorkerDict pid=2136257)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136257)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136257)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136257)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136257)[0m INFO 12-14 01:30:59 [parallel_state.py:1208] rank 8 in world size 16 is assigned as DP rank 1, PP rank 0, TP rank 0, EP rank 8
[36m(WorkerDict pid=2136258)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136258)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136258)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136258)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136258)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136258)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136258)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136258)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136258)[0m INFO 12-14 01:30:59 [parallel_state.py:1208] rank 9 in world size 16 is assigned as DP rank 1, PP rank 0, TP rank 1, EP rank 9
[36m(WorkerDict pid=2136261)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136261)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136261)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136261)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136261)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136261)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136261)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136261)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136261)[0m INFO 12-14 01:30:59 [parallel_state.py:1208] rank 13 in world size 16 is assigned as DP rank 1, PP rank 0, TP rank 5, EP rank 13
[36m(WorkerDict pid=2136260)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136260)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136260)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136260)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136260)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136260)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136260)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136260)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136260)[0m INFO 12-14 01:30:59 [parallel_state.py:1208] rank 12 in world size 16 is assigned as DP rank 1, PP rank 0, TP rank 4, EP rank 12
[36m(WorkerDict pid=2136259)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136259)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136259)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136259)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136259)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136259)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136259)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136259)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136259)[0m INFO 12-14 01:30:59 [parallel_state.py:1208] rank 10 in world size 16 is assigned as DP rank 1, PP rank 0, TP rank 2, EP rank 10
[36m(WorkerDict pid=2136265)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136265)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136265)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136265)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136265)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136265)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136265)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136265)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136265)[0m INFO 12-14 01:30:59 [parallel_state.py:1208] rank 15 in world size 16 is assigned as DP rank 1, PP rank 0, TP rank 7, EP rank 15
[36m(WorkerDict pid=2136269)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136269)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136269)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136269)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136269)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136269)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136269)[0m WARNING 12-14 01:30:59 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=2136269)[0m WARNING 12-14 01:30:59 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=2136269)[0m INFO 12-14 01:30:59 [parallel_state.py:1208] rank 14 in world size 16 is assigned as DP rank 1, PP rank 0, TP rank 6, EP rank 14
[36m(WorkerDict pid=2136241)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(WorkerDict pid=2136241)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(WorkerDict pid=2136241)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(WorkerDict pid=2136241)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(WorkerDict pid=2136241)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
[36m(WorkerDict pid=2136241)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=2136241)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=2136241)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
[36m(WorkerDict pid=2136241)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
[36m(WorkerDict pid=2136241)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(WorkerDict pid=2136241)[0m INFO 12-14 01:30:59 [utils.py:233] non-default args: {'load_format': 'dummy', 'dtype': 'bfloat16', 'seed': 0, 'max_model_len': 36864, 'distributed_executor_backend': 'external_launcher', 'tensor_parallel_size': 8, 'enable_prefix_caching': False, 'gpu_memory_utilization': 0.85, 'max_num_batched_tokens': 36864, 'max_num_seqs': 128, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'speculative_config': {'method': 'sam', 'num_speculative_tokens': 3}, 'additional_config': {'torchair_graph_config': {'enabled': False}, 'ascend_scheduler_config': {'enabled': True, 'enable_chunked_prefill': False}, 'refresh': True, 'dynamic_eplb': False, 'num_iterations_eplb_update': 400, 'gate_eplb': True, 'num_wait_worker_iterations': 30}, 'model': '/home/data/Qwen3-32B'}
[36m(WorkerDict pid=2136241)[0m INFO 12-14 01:30:59 [model.py:547] Resolved architecture: Qwen3ForCausalLM
[36m(WorkerDict pid=2136241)[0m INFO 12-14 01:30:59 [model.py:1510] Using max model len 36864
[36m(WorkerDict pid=2136241)[0m INFO 12-14 01:30:59 [arg_utils.py:1215] Using ray runtime env: {'env_vars': {'MASTER_ADDR': '192.168.0.163', 'MASTER_PORT': '56741', 'NCCL_CUMEM_ENABLE': '0', 'NCCL_DEBUG': 'WARN', 'RANK': '0', 'RAY_LOCAL_WORLD_SIZE': '16', 'TOKENIZERS_PARALLELISM': 'true', 'VLLM_ALLOW_RUNTIME_LORA_UPDATING': 'true', 'WG_BACKEND': 'ray', 'WG_PREFIX': 'jWrpjg', 'WORLD_SIZE': '16'}}
[36m(WorkerDict pid=2136241)[0m INFO 12-14 01:30:59 [parallel.py:380] Using external launcher for distributed inference.
[36m(WorkerDict pid=2136241)[0m INFO 12-14 01:30:59 [parallel.py:420] Disabling V1 multiprocessing for external launcher.
[36m(WorkerDict pid=2136241)[0m INFO 12-14 01:30:59 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=36864.
[36m(WorkerDict pid=2136241)[0m INFO 12-14 01:30:59 [__init__.py:381] Cudagraph is disabled under eager mode
[36m(WorkerDict pid=2136241)[0m INFO 12-14 01:30:59 [platform.py:141] Non-MLA LLMs forcibly disable the chunked prefill feature,as the performance of operators supporting this feature functionality is currently suboptimal.
[36m(WorkerDict pid=2136241)[0m INFO 12-14 01:30:59 [platform.py:179] Compilation disabled, using eager mode by default
[36m(WorkerDict pid=2136244)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(WorkerDict pid=2136244)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(WorkerDict pid=2136244)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(WorkerDict pid=2136244)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(WorkerDict pid=2136244)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
[36m(WorkerDict pid=2136244)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=2136244)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=2136244)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
[36m(WorkerDict pid=2136244)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
[36m(WorkerDict pid=2136244)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(WorkerDict pid=2136244)[0m INFO 12-14 01:30:59 [utils.py:233] non-default args: {'load_format': 'dummy', 'dtype': 'bfloat16', 'seed': 0, 'max_model_len': 36864, 'distributed_executor_backend': 'external_launcher', 'tensor_parallel_size': 8, 'enable_prefix_caching': False, 'gpu_memory_utilization': 0.85, 'max_num_batched_tokens': 36864, 'max_num_seqs': 128, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'speculative_config': {'method': 'sam', 'num_speculative_tokens': 3}, 'additional_config': {'torchair_graph_config': {'enabled': False}, 'ascend_scheduler_config': {'enabled': True, 'enable_chunked_prefill': False}, 'refresh': True, 'dynamic_eplb': False, 'num_iterations_eplb_update': 400, 'gate_eplb': True, 'num_wait_worker_iterations': 30}, 'model': '/home/data/Qwen3-32B'}
[36m(WorkerDict pid=2136244)[0m INFO 12-14 01:30:59 [model.py:547] Resolved architecture: Qwen3ForCausalLM
[36m(WorkerDict pid=2136244)[0m INFO 12-14 01:30:59 [model.py:1510] Using max model len 36864
[36m(WorkerDict pid=2136244)[0m INFO 12-14 01:30:59 [arg_utils.py:1215] Using ray runtime env: {'env_vars': {'MASTER_ADDR': '192.168.0.163', 'MASTER_PORT': '56741', 'NCCL_CUMEM_ENABLE': '0', 'NCCL_DEBUG': 'WARN', 'RANK': '1', 'RAY_LOCAL_WORLD_SIZE': '16', 'TOKENIZERS_PARALLELISM': 'true', 'VLLM_ALLOW_RUNTIME_LORA_UPDATING': 'true', 'WG_BACKEND': 'ray', 'WG_PREFIX': 'jWrpjg', 'WORLD_SIZE': '16'}}
[36m(WorkerDict pid=2136244)[0m INFO 12-14 01:30:59 [parallel.py:380] Using external launcher for distributed inference.
[36m(WorkerDict pid=2136244)[0m INFO 12-14 01:30:59 [parallel.py:420] Disabling V1 multiprocessing for external launcher.
[36m(WorkerDict pid=2136244)[0m INFO 12-14 01:30:59 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=36864.
[36m(WorkerDict pid=2136244)[0m INFO 12-14 01:30:59 [__init__.py:381] Cudagraph is disabled under eager mode
[36m(WorkerDict pid=2136244)[0m INFO 12-14 01:30:59 [platform.py:141] Non-MLA LLMs forcibly disable the chunked prefill feature,as the performance of operators supporting this feature functionality is currently suboptimal.
[36m(WorkerDict pid=2136244)[0m INFO 12-14 01:30:59 [platform.py:179] Compilation disabled, using eager mode by default
[36m(WorkerDict pid=2136247)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(WorkerDict pid=2136247)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(WorkerDict pid=2136247)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(WorkerDict pid=2136247)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(WorkerDict pid=2136247)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
[36m(WorkerDict pid=2136247)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=2136247)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=2136247)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
[36m(WorkerDict pid=2136247)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
[36m(WorkerDict pid=2136247)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(WorkerDict pid=2136247)[0m INFO 12-14 01:30:59 [utils.py:233] non-default args: {'load_format': 'dummy', 'dtype': 'bfloat16', 'seed': 0, 'max_model_len': 36864, 'distributed_executor_backend': 'external_launcher', 'tensor_parallel_size': 8, 'enable_prefix_caching': False, 'gpu_memory_utilization': 0.85, 'max_num_batched_tokens': 36864, 'max_num_seqs': 128, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'speculative_config': {'method': 'sam', 'num_speculative_tokens': 3}, 'additional_config': {'torchair_graph_config': {'enabled': False}, 'ascend_scheduler_config': {'enabled': True, 'enable_chunked_prefill': False}, 'refresh': True, 'dynamic_eplb': False, 'num_iterations_eplb_update': 400, 'gate_eplb': True, 'num_wait_worker_iterations': 30}, 'model': '/home/data/Qwen3-32B'}
[36m(WorkerDict pid=2136247)[0m INFO 12-14 01:30:59 [model.py:547] Resolved architecture: Qwen3ForCausalLM
[36m(WorkerDict pid=2136247)[0m INFO 12-14 01:30:59 [model.py:1510] Using max model len 36864
[36m(WorkerDict pid=2136247)[0m INFO 12-14 01:30:59 [arg_utils.py:1215] Using ray runtime env: {'env_vars': {'MASTER_ADDR': '192.168.0.163', 'MASTER_PORT': '56741', 'NCCL_CUMEM_ENABLE': '0', 'NCCL_DEBUG': 'WARN', 'RANK': '2', 'RAY_LOCAL_WORLD_SIZE': '16', 'TOKENIZERS_PARALLELISM': 'true', 'VLLM_ALLOW_RUNTIME_LORA_UPDATING': 'true', 'WG_BACKEND': 'ray', 'WG_PREFIX': 'jWrpjg', 'WORLD_SIZE': '16'}}
[36m(WorkerDict pid=2136247)[0m INFO 12-14 01:30:59 [parallel.py:380] Using external launcher for distributed inference.
[36m(WorkerDict pid=2136247)[0m INFO 12-14 01:30:59 [parallel.py:420] Disabling V1 multiprocessing for external launcher.
[36m(WorkerDict pid=2136247)[0m INFO 12-14 01:30:59 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=36864.
[36m(WorkerDict pid=2136247)[0m INFO 12-14 01:30:59 [__init__.py:381] Cudagraph is disabled under eager mode
[36m(WorkerDict pid=2136247)[0m INFO 12-14 01:30:59 [platform.py:141] Non-MLA LLMs forcibly disable the chunked prefill feature,as the performance of operators supporting this feature functionality is currently suboptimal.
[36m(WorkerDict pid=2136247)[0m INFO 12-14 01:30:59 [platform.py:179] Compilation disabled, using eager mode by default
[36m(WorkerDict pid=2136248)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(WorkerDict pid=2136248)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(WorkerDict pid=2136248)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(WorkerDict pid=2136248)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(WorkerDict pid=2136248)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
[36m(WorkerDict pid=2136248)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=2136248)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=2136248)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
[36m(WorkerDict pid=2136248)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
[36m(WorkerDict pid=2136248)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(WorkerDict pid=2136248)[0m INFO 12-14 01:30:59 [utils.py:233] non-default args: {'load_format': 'dummy', 'dtype': 'bfloat16', 'seed': 0, 'max_model_len': 36864, 'distributed_executor_backend': 'external_launcher', 'tensor_parallel_size': 8, 'enable_prefix_caching': False, 'gpu_memory_utilization': 0.85, 'max_num_batched_tokens': 36864, 'max_num_seqs': 128, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'speculative_config': {'method': 'sam', 'num_speculative_tokens': 3}, 'additional_config': {'torchair_graph_config': {'enabled': False}, 'ascend_scheduler_config': {'enabled': True, 'enable_chunked_prefill': False}, 'refresh': True, 'dynamic_eplb': False, 'num_iterations_eplb_update': 400, 'gate_eplb': True, 'num_wait_worker_iterations': 30}, 'model': '/home/data/Qwen3-32B'}
[36m(WorkerDict pid=2136248)[0m INFO 12-14 01:30:59 [model.py:547] Resolved architecture: Qwen3ForCausalLM
[36m(WorkerDict pid=2136248)[0m INFO 12-14 01:30:59 [model.py:1510] Using max model len 36864
[36m(WorkerDict pid=2136248)[0m INFO 12-14 01:30:59 [arg_utils.py:1215] Using ray runtime env: {'env_vars': {'MASTER_ADDR': '192.168.0.163', 'MASTER_PORT': '56741', 'NCCL_CUMEM_ENABLE': '0', 'NCCL_DEBUG': 'WARN', 'RANK': '3', 'RAY_LOCAL_WORLD_SIZE': '16', 'TOKENIZERS_PARALLELISM': 'true', 'VLLM_ALLOW_RUNTIME_LORA_UPDATING': 'true', 'WG_BACKEND': 'ray', 'WG_PREFIX': 'jWrpjg', 'WORLD_SIZE': '16'}}
[36m(WorkerDict pid=2136248)[0m INFO 12-14 01:30:59 [parallel.py:380] Using external launcher for distributed inference.
[36m(WorkerDict pid=2136248)[0m INFO 12-14 01:30:59 [parallel.py:420] Disabling V1 multiprocessing for external launcher.
[36m(WorkerDict pid=2136248)[0m INFO 12-14 01:30:59 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=36864.
[36m(WorkerDict pid=2136248)[0m INFO 12-14 01:30:59 [__init__.py:381] Cudagraph is disabled under eager mode
[36m(WorkerDict pid=2136248)[0m INFO 12-14 01:30:59 [platform.py:141] Non-MLA LLMs forcibly disable the chunked prefill feature,as the performance of operators supporting this feature functionality is currently suboptimal.
[36m(WorkerDict pid=2136248)[0m INFO 12-14 01:30:59 [platform.py:179] Compilation disabled, using eager mode by default
[36m(WorkerDict pid=2136253)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(WorkerDict pid=2136253)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(WorkerDict pid=2136253)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(WorkerDict pid=2136253)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(WorkerDict pid=2136253)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
[36m(WorkerDict pid=2136253)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=2136253)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=2136253)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
[36m(WorkerDict pid=2136253)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
[36m(WorkerDict pid=2136253)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(WorkerDict pid=2136253)[0m INFO 12-14 01:30:59 [utils.py:233] non-default args: {'load_format': 'dummy', 'dtype': 'bfloat16', 'seed': 0, 'max_model_len': 36864, 'distributed_executor_backend': 'external_launcher', 'tensor_parallel_size': 8, 'enable_prefix_caching': False, 'gpu_memory_utilization': 0.85, 'max_num_batched_tokens': 36864, 'max_num_seqs': 128, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'speculative_config': {'method': 'sam', 'num_speculative_tokens': 3}, 'additional_config': {'torchair_graph_config': {'enabled': False}, 'ascend_scheduler_config': {'enabled': True, 'enable_chunked_prefill': False}, 'refresh': True, 'dynamic_eplb': False, 'num_iterations_eplb_update': 400, 'gate_eplb': True, 'num_wait_worker_iterations': 30}, 'model': '/home/data/Qwen3-32B'}
[36m(WorkerDict pid=2136253)[0m INFO 12-14 01:30:59 [model.py:547] Resolved architecture: Qwen3ForCausalLM
[36m(WorkerDict pid=2136253)[0m INFO 12-14 01:30:59 [model.py:1510] Using max model len 36864
[36m(WorkerDict pid=2136253)[0m INFO 12-14 01:30:59 [arg_utils.py:1215] Using ray runtime env: {'env_vars': {'MASTER_ADDR': '192.168.0.163', 'MASTER_PORT': '56741', 'NCCL_CUMEM_ENABLE': '0', 'NCCL_DEBUG': 'WARN', 'RANK': '4', 'RAY_LOCAL_WORLD_SIZE': '16', 'TOKENIZERS_PARALLELISM': 'true', 'VLLM_ALLOW_RUNTIME_LORA_UPDATING': 'true', 'WG_BACKEND': 'ray', 'WG_PREFIX': 'jWrpjg', 'WORLD_SIZE': '16'}}
[36m(WorkerDict pid=2136253)[0m INFO 12-14 01:30:59 [parallel.py:380] Using external launcher for distributed inference.
[36m(WorkerDict pid=2136253)[0m INFO 12-14 01:30:59 [parallel.py:420] Disabling V1 multiprocessing for external launcher.
[36m(WorkerDict pid=2136253)[0m INFO 12-14 01:30:59 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=36864.
[36m(WorkerDict pid=2136253)[0m INFO 12-14 01:30:59 [__init__.py:381] Cudagraph is disabled under eager mode
[36m(WorkerDict pid=2136253)[0m INFO 12-14 01:30:59 [platform.py:141] Non-MLA LLMs forcibly disable the chunked prefill feature,as the performance of operators supporting this feature functionality is currently suboptimal.
[36m(WorkerDict pid=2136253)[0m INFO 12-14 01:30:59 [platform.py:179] Compilation disabled, using eager mode by default
[36m(WorkerDict pid=2136249)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(WorkerDict pid=2136249)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(WorkerDict pid=2136249)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(WorkerDict pid=2136249)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(WorkerDict pid=2136249)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
[36m(WorkerDict pid=2136249)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=2136249)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=2136249)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
[36m(WorkerDict pid=2136249)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
[36m(WorkerDict pid=2136249)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(WorkerDict pid=2136249)[0m INFO 12-14 01:30:59 [utils.py:233] non-default args: {'load_format': 'dummy', 'dtype': 'bfloat16', 'seed': 0, 'max_model_len': 36864, 'distributed_executor_backend': 'external_launcher', 'tensor_parallel_size': 8, 'enable_prefix_caching': False, 'gpu_memory_utilization': 0.85, 'max_num_batched_tokens': 36864, 'max_num_seqs': 128, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'speculative_config': {'method': 'sam', 'num_speculative_tokens': 3}, 'additional_config': {'torchair_graph_config': {'enabled': False}, 'ascend_scheduler_config': {'enabled': True, 'enable_chunked_prefill': False}, 'refresh': True, 'dynamic_eplb': False, 'num_iterations_eplb_update': 400, 'gate_eplb': True, 'num_wait_worker_iterations': 30}, 'model': '/home/data/Qwen3-32B'}
[36m(WorkerDict pid=2136249)[0m INFO 12-14 01:30:59 [model.py:547] Resolved architecture: Qwen3ForCausalLM
[36m(WorkerDict pid=2136249)[0m INFO 12-14 01:30:59 [model.py:1510] Using max model len 36864
[36m(WorkerDict pid=2136249)[0m INFO 12-14 01:30:59 [arg_utils.py:1215] Using ray runtime env: {'env_vars': {'MASTER_ADDR': '192.168.0.163', 'MASTER_PORT': '56741', 'NCCL_CUMEM_ENABLE': '0', 'NCCL_DEBUG': 'WARN', 'RANK': '11', 'RAY_LOCAL_WORLD_SIZE': '16', 'TOKENIZERS_PARALLELISM': 'true', 'VLLM_ALLOW_RUNTIME_LORA_UPDATING': 'true', 'WG_BACKEND': 'ray', 'WG_PREFIX': 'jWrpjg', 'WORLD_SIZE': '16'}}
[36m(WorkerDict pid=2136249)[0m INFO 12-14 01:30:59 [parallel.py:380] Using external launcher for distributed inference.
[36m(WorkerDict pid=2136249)[0m INFO 12-14 01:30:59 [parallel.py:420] Disabling V1 multiprocessing for external launcher.
[36m(WorkerDict pid=2136249)[0m INFO 12-14 01:30:59 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=36864.
[36m(WorkerDict pid=2136249)[0m INFO 12-14 01:30:59 [__init__.py:381] Cudagraph is disabled under eager mode
[36m(WorkerDict pid=2136249)[0m INFO 12-14 01:30:59 [platform.py:141] Non-MLA LLMs forcibly disable the chunked prefill feature,as the performance of operators supporting this feature functionality is currently suboptimal.
[36m(WorkerDict pid=2136249)[0m INFO 12-14 01:30:59 [platform.py:179] Compilation disabled, using eager mode by default
[36m(WorkerDict pid=2136254)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(WorkerDict pid=2136254)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(WorkerDict pid=2136254)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(WorkerDict pid=2136254)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(WorkerDict pid=2136254)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
[36m(WorkerDict pid=2136254)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=2136254)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=2136254)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
[36m(WorkerDict pid=2136254)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
[36m(WorkerDict pid=2136254)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(WorkerDict pid=2136254)[0m INFO 12-14 01:30:59 [utils.py:233] non-default args: {'load_format': 'dummy', 'dtype': 'bfloat16', 'seed': 0, 'max_model_len': 36864, 'distributed_executor_backend': 'external_launcher', 'tensor_parallel_size': 8, 'enable_prefix_caching': False, 'gpu_memory_utilization': 0.85, 'max_num_batched_tokens': 36864, 'max_num_seqs': 128, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'speculative_config': {'method': 'sam', 'num_speculative_tokens': 3}, 'additional_config': {'torchair_graph_config': {'enabled': False}, 'ascend_scheduler_config': {'enabled': True, 'enable_chunked_prefill': False}, 'refresh': True, 'dynamic_eplb': False, 'num_iterations_eplb_update': 400, 'gate_eplb': True, 'num_wait_worker_iterations': 30}, 'model': '/home/data/Qwen3-32B'}
[36m(WorkerDict pid=2136254)[0m INFO 12-14 01:30:59 [model.py:547] Resolved architecture: Qwen3ForCausalLM
[36m(WorkerDict pid=2136254)[0m INFO 12-14 01:30:59 [model.py:1510] Using max model len 36864
[36m(WorkerDict pid=2136254)[0m INFO 12-14 01:30:59 [arg_utils.py:1215] Using ray runtime env: {'env_vars': {'MASTER_ADDR': '192.168.0.163', 'MASTER_PORT': '56741', 'NCCL_CUMEM_ENABLE': '0', 'NCCL_DEBUG': 'WARN', 'RANK': '5', 'RAY_LOCAL_WORLD_SIZE': '16', 'TOKENIZERS_PARALLELISM': 'true', 'VLLM_ALLOW_RUNTIME_LORA_UPDATING': 'true', 'WG_BACKEND': 'ray', 'WG_PREFIX': 'jWrpjg', 'WORLD_SIZE': '16'}}
[36m(WorkerDict pid=2136254)[0m INFO 12-14 01:30:59 [parallel.py:380] Using external launcher for distributed inference.
[36m(WorkerDict pid=2136254)[0m INFO 12-14 01:30:59 [parallel.py:420] Disabling V1 multiprocessing for external launcher.
[36m(WorkerDict pid=2136254)[0m INFO 12-14 01:30:59 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=36864.
[36m(WorkerDict pid=2136254)[0m INFO 12-14 01:30:59 [__init__.py:381] Cudagraph is disabled under eager mode
[36m(WorkerDict pid=2136254)[0m INFO 12-14 01:30:59 [platform.py:141] Non-MLA LLMs forcibly disable the chunked prefill feature,as the performance of operators supporting this feature functionality is currently suboptimal.
[36m(WorkerDict pid=2136254)[0m INFO 12-14 01:30:59 [platform.py:179] Compilation disabled, using eager mode by default
[36m(WorkerDict pid=2136256)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(WorkerDict pid=2136256)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(WorkerDict pid=2136256)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(WorkerDict pid=2136256)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(WorkerDict pid=2136256)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
[36m(WorkerDict pid=2136256)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=2136256)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=2136256)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
[36m(WorkerDict pid=2136256)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
[36m(WorkerDict pid=2136256)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(WorkerDict pid=2136256)[0m INFO 12-14 01:30:59 [utils.py:233] non-default args: {'load_format': 'dummy', 'dtype': 'bfloat16', 'seed': 0, 'max_model_len': 36864, 'distributed_executor_backend': 'external_launcher', 'tensor_parallel_size': 8, 'enable_prefix_caching': False, 'gpu_memory_utilization': 0.85, 'max_num_batched_tokens': 36864, 'max_num_seqs': 128, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'speculative_config': {'method': 'sam', 'num_speculative_tokens': 3}, 'additional_config': {'torchair_graph_config': {'enabled': False}, 'ascend_scheduler_config': {'enabled': True, 'enable_chunked_prefill': False}, 'refresh': True, 'dynamic_eplb': False, 'num_iterations_eplb_update': 400, 'gate_eplb': True, 'num_wait_worker_iterations': 30}, 'model': '/home/data/Qwen3-32B'}
[36m(WorkerDict pid=2136256)[0m INFO 12-14 01:30:59 [model.py:547] Resolved architecture: Qwen3ForCausalLM
[36m(WorkerDict pid=2136256)[0m INFO 12-14 01:30:59 [model.py:1510] Using max model len 36864
[36m(WorkerDict pid=2136256)[0m INFO 12-14 01:30:59 [arg_utils.py:1215] Using ray runtime env: {'env_vars': {'MASTER_ADDR': '192.168.0.163', 'MASTER_PORT': '56741', 'NCCL_CUMEM_ENABLE': '0', 'NCCL_DEBUG': 'WARN', 'RANK': '7', 'RAY_LOCAL_WORLD_SIZE': '16', 'TOKENIZERS_PARALLELISM': 'true', 'VLLM_ALLOW_RUNTIME_LORA_UPDATING': 'true', 'WG_BACKEND': 'ray', 'WG_PREFIX': 'jWrpjg', 'WORLD_SIZE': '16'}}
[36m(WorkerDict pid=2136256)[0m INFO 12-14 01:30:59 [parallel.py:380] Using external launcher for distributed inference.
[36m(WorkerDict pid=2136256)[0m INFO 12-14 01:30:59 [parallel.py:420] Disabling V1 multiprocessing for external launcher.
[36m(WorkerDict pid=2136256)[0m INFO 12-14 01:30:59 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=36864.
[36m(WorkerDict pid=2136256)[0m INFO 12-14 01:30:59 [__init__.py:381] Cudagraph is disabled under eager mode
[36m(WorkerDict pid=2136256)[0m INFO 12-14 01:30:59 [platform.py:141] Non-MLA LLMs forcibly disable the chunked prefill feature,as the performance of operators supporting this feature functionality is currently suboptimal.
[36m(WorkerDict pid=2136256)[0m INFO 12-14 01:30:59 [platform.py:179] Compilation disabled, using eager mode by default
[36m(WorkerDict pid=2136255)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(WorkerDict pid=2136255)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(WorkerDict pid=2136255)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(WorkerDict pid=2136255)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(WorkerDict pid=2136255)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
[36m(WorkerDict pid=2136255)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=2136255)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=2136255)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
[36m(WorkerDict pid=2136255)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
[36m(WorkerDict pid=2136255)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(WorkerDict pid=2136255)[0m INFO 12-14 01:30:59 [utils.py:233] non-default args: {'load_format': 'dummy', 'dtype': 'bfloat16', 'seed': 0, 'max_model_len': 36864, 'distributed_executor_backend': 'external_launcher', 'tensor_parallel_size': 8, 'enable_prefix_caching': False, 'gpu_memory_utilization': 0.85, 'max_num_batched_tokens': 36864, 'max_num_seqs': 128, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'speculative_config': {'method': 'sam', 'num_speculative_tokens': 3}, 'additional_config': {'torchair_graph_config': {'enabled': False}, 'ascend_scheduler_config': {'enabled': True, 'enable_chunked_prefill': False}, 'refresh': True, 'dynamic_eplb': False, 'num_iterations_eplb_update': 400, 'gate_eplb': True, 'num_wait_worker_iterations': 30}, 'model': '/home/data/Qwen3-32B'}
[36m(WorkerDict pid=2136255)[0m INFO 12-14 01:30:59 [model.py:547] Resolved architecture: Qwen3ForCausalLM
[36m(WorkerDict pid=2136255)[0m INFO 12-14 01:30:59 [model.py:1510] Using max model len 36864
[36m(WorkerDict pid=2136255)[0m INFO 12-14 01:30:59 [arg_utils.py:1215] Using ray runtime env: {'env_vars': {'MASTER_ADDR': '192.168.0.163', 'MASTER_PORT': '56741', 'NCCL_CUMEM_ENABLE': '0', 'NCCL_DEBUG': 'WARN', 'RANK': '6', 'RAY_LOCAL_WORLD_SIZE': '16', 'TOKENIZERS_PARALLELISM': 'true', 'VLLM_ALLOW_RUNTIME_LORA_UPDATING': 'true', 'WG_BACKEND': 'ray', 'WG_PREFIX': 'jWrpjg', 'WORLD_SIZE': '16'}}
[36m(WorkerDict pid=2136255)[0m INFO 12-14 01:30:59 [parallel.py:380] Using external launcher for distributed inference.
[36m(WorkerDict pid=2136255)[0m INFO 12-14 01:30:59 [parallel.py:420] Disabling V1 multiprocessing for external launcher.
[36m(WorkerDict pid=2136255)[0m INFO 12-14 01:30:59 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=36864.
[36m(WorkerDict pid=2136255)[0m INFO 12-14 01:30:59 [__init__.py:381] Cudagraph is disabled under eager mode
[36m(WorkerDict pid=2136255)[0m INFO 12-14 01:30:59 [platform.py:141] Non-MLA LLMs forcibly disable the chunked prefill feature,as the performance of operators supporting this feature functionality is currently suboptimal.
[36m(WorkerDict pid=2136255)[0m INFO 12-14 01:30:59 [platform.py:179] Compilation disabled, using eager mode by default
[36m(WorkerDict pid=2136257)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(WorkerDict pid=2136257)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(WorkerDict pid=2136257)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(WorkerDict pid=2136257)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(WorkerDict pid=2136257)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
[36m(WorkerDict pid=2136257)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=2136257)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=2136257)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
[36m(WorkerDict pid=2136257)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
[36m(WorkerDict pid=2136257)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(WorkerDict pid=2136257)[0m INFO 12-14 01:30:59 [utils.py:233] non-default args: {'load_format': 'dummy', 'dtype': 'bfloat16', 'seed': 0, 'max_model_len': 36864, 'distributed_executor_backend': 'external_launcher', 'tensor_parallel_size': 8, 'enable_prefix_caching': False, 'gpu_memory_utilization': 0.85, 'max_num_batched_tokens': 36864, 'max_num_seqs': 128, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'speculative_config': {'method': 'sam', 'num_speculative_tokens': 3}, 'additional_config': {'torchair_graph_config': {'enabled': False}, 'ascend_scheduler_config': {'enabled': True, 'enable_chunked_prefill': False}, 'refresh': True, 'dynamic_eplb': False, 'num_iterations_eplb_update': 400, 'gate_eplb': True, 'num_wait_worker_iterations': 30}, 'model': '/home/data/Qwen3-32B'}
[36m(WorkerDict pid=2136257)[0m INFO 12-14 01:30:59 [model.py:547] Resolved architecture: Qwen3ForCausalLM
[36m(WorkerDict pid=2136257)[0m INFO 12-14 01:30:59 [model.py:1510] Using max model len 36864
[36m(WorkerDict pid=2136257)[0m INFO 12-14 01:30:59 [arg_utils.py:1215] Using ray runtime env: {'env_vars': {'MASTER_ADDR': '192.168.0.163', 'MASTER_PORT': '56741', 'NCCL_CUMEM_ENABLE': '0', 'NCCL_DEBUG': 'WARN', 'RANK': '8', 'RAY_LOCAL_WORLD_SIZE': '16', 'TOKENIZERS_PARALLELISM': 'true', 'VLLM_ALLOW_RUNTIME_LORA_UPDATING': 'true', 'WG_BACKEND': 'ray', 'WG_PREFIX': 'jWrpjg', 'WORLD_SIZE': '16'}}
[36m(WorkerDict pid=2136257)[0m INFO 12-14 01:30:59 [parallel.py:380] Using external launcher for distributed inference.
[36m(WorkerDict pid=2136257)[0m INFO 12-14 01:30:59 [parallel.py:420] Disabling V1 multiprocessing for external launcher.
[36m(WorkerDict pid=2136257)[0m INFO 12-14 01:30:59 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=36864.
[36m(WorkerDict pid=2136257)[0m INFO 12-14 01:30:59 [__init__.py:381] Cudagraph is disabled under eager mode
[36m(WorkerDict pid=2136257)[0m INFO 12-14 01:30:59 [platform.py:141] Non-MLA LLMs forcibly disable the chunked prefill feature,as the performance of operators supporting this feature functionality is currently suboptimal.
[36m(WorkerDict pid=2136257)[0m INFO 12-14 01:30:59 [platform.py:179] Compilation disabled, using eager mode by default
[36m(WorkerDict pid=2136258)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(WorkerDict pid=2136258)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(WorkerDict pid=2136258)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(WorkerDict pid=2136258)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(WorkerDict pid=2136258)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
[36m(WorkerDict pid=2136258)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=2136258)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=2136258)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
[36m(WorkerDict pid=2136258)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
[36m(WorkerDict pid=2136258)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(WorkerDict pid=2136258)[0m INFO 12-14 01:30:59 [utils.py:233] non-default args: {'load_format': 'dummy', 'dtype': 'bfloat16', 'seed': 0, 'max_model_len': 36864, 'distributed_executor_backend': 'external_launcher', 'tensor_parallel_size': 8, 'enable_prefix_caching': False, 'gpu_memory_utilization': 0.85, 'max_num_batched_tokens': 36864, 'max_num_seqs': 128, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'speculative_config': {'method': 'sam', 'num_speculative_tokens': 3}, 'additional_config': {'torchair_graph_config': {'enabled': False}, 'ascend_scheduler_config': {'enabled': True, 'enable_chunked_prefill': False}, 'refresh': True, 'dynamic_eplb': False, 'num_iterations_eplb_update': 400, 'gate_eplb': True, 'num_wait_worker_iterations': 30}, 'model': '/home/data/Qwen3-32B'}
[36m(WorkerDict pid=2136258)[0m INFO 12-14 01:30:59 [model.py:547] Resolved architecture: Qwen3ForCausalLM
[36m(WorkerDict pid=2136258)[0m INFO 12-14 01:30:59 [model.py:1510] Using max model len 36864
[36m(WorkerDict pid=2136258)[0m INFO 12-14 01:30:59 [arg_utils.py:1215] Using ray runtime env: {'env_vars': {'MASTER_ADDR': '192.168.0.163', 'MASTER_PORT': '56741', 'NCCL_CUMEM_ENABLE': '0', 'NCCL_DEBUG': 'WARN', 'RANK': '9', 'RAY_LOCAL_WORLD_SIZE': '16', 'TOKENIZERS_PARALLELISM': 'true', 'VLLM_ALLOW_RUNTIME_LORA_UPDATING': 'true', 'WG_BACKEND': 'ray', 'WG_PREFIX': 'jWrpjg', 'WORLD_SIZE': '16'}}
[36m(WorkerDict pid=2136258)[0m INFO 12-14 01:30:59 [parallel.py:380] Using external launcher for distributed inference.
[36m(WorkerDict pid=2136258)[0m INFO 12-14 01:30:59 [parallel.py:420] Disabling V1 multiprocessing for external launcher.
[36m(WorkerDict pid=2136258)[0m INFO 12-14 01:30:59 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=36864.
[36m(WorkerDict pid=2136258)[0m INFO 12-14 01:30:59 [__init__.py:381] Cudagraph is disabled under eager mode
[36m(WorkerDict pid=2136258)[0m INFO 12-14 01:30:59 [platform.py:141] Non-MLA LLMs forcibly disable the chunked prefill feature,as the performance of operators supporting this feature functionality is currently suboptimal.
[36m(WorkerDict pid=2136258)[0m INFO 12-14 01:30:59 [platform.py:179] Compilation disabled, using eager mode by default
[36m(WorkerDict pid=2136261)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(WorkerDict pid=2136261)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(WorkerDict pid=2136261)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(WorkerDict pid=2136261)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(WorkerDict pid=2136261)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
[36m(WorkerDict pid=2136261)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=2136261)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=2136261)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
[36m(WorkerDict pid=2136261)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
[36m(WorkerDict pid=2136261)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(WorkerDict pid=2136261)[0m INFO 12-14 01:30:59 [utils.py:233] non-default args: {'load_format': 'dummy', 'dtype': 'bfloat16', 'seed': 0, 'max_model_len': 36864, 'distributed_executor_backend': 'external_launcher', 'tensor_parallel_size': 8, 'enable_prefix_caching': False, 'gpu_memory_utilization': 0.85, 'max_num_batched_tokens': 36864, 'max_num_seqs': 128, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'speculative_config': {'method': 'sam', 'num_speculative_tokens': 3}, 'additional_config': {'torchair_graph_config': {'enabled': False}, 'ascend_scheduler_config': {'enabled': True, 'enable_chunked_prefill': False}, 'refresh': True, 'dynamic_eplb': False, 'num_iterations_eplb_update': 400, 'gate_eplb': True, 'num_wait_worker_iterations': 30}, 'model': '/home/data/Qwen3-32B'}
[36m(WorkerDict pid=2136261)[0m INFO 12-14 01:30:59 [model.py:547] Resolved architecture: Qwen3ForCausalLM
[36m(WorkerDict pid=2136261)[0m INFO 12-14 01:30:59 [model.py:1510] Using max model len 36864
[36m(WorkerDict pid=2136261)[0m INFO 12-14 01:30:59 [arg_utils.py:1215] Using ray runtime env: {'env_vars': {'MASTER_ADDR': '192.168.0.163', 'MASTER_PORT': '56741', 'NCCL_CUMEM_ENABLE': '0', 'NCCL_DEBUG': 'WARN', 'RANK': '13', 'RAY_LOCAL_WORLD_SIZE': '16', 'TOKENIZERS_PARALLELISM': 'true', 'VLLM_ALLOW_RUNTIME_LORA_UPDATING': 'true', 'WG_BACKEND': 'ray', 'WG_PREFIX': 'jWrpjg', 'WORLD_SIZE': '16'}}
[36m(WorkerDict pid=2136261)[0m INFO 12-14 01:30:59 [parallel.py:380] Using external launcher for distributed inference.
[36m(WorkerDict pid=2136261)[0m INFO 12-14 01:30:59 [parallel.py:420] Disabling V1 multiprocessing for external launcher.
[36m(WorkerDict pid=2136261)[0m INFO 12-14 01:30:59 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=36864.
[36m(WorkerDict pid=2136261)[0m INFO 12-14 01:30:59 [__init__.py:381] Cudagraph is disabled under eager mode
[36m(WorkerDict pid=2136261)[0m INFO 12-14 01:30:59 [platform.py:141] Non-MLA LLMs forcibly disable the chunked prefill feature,as the performance of operators supporting this feature functionality is currently suboptimal.
[36m(WorkerDict pid=2136261)[0m INFO 12-14 01:30:59 [platform.py:179] Compilation disabled, using eager mode by default
[36m(WorkerDict pid=2136260)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(WorkerDict pid=2136260)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(WorkerDict pid=2136260)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(WorkerDict pid=2136260)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(WorkerDict pid=2136260)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
[36m(WorkerDict pid=2136260)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=2136260)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=2136260)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
[36m(WorkerDict pid=2136260)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
[36m(WorkerDict pid=2136260)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(WorkerDict pid=2136260)[0m INFO 12-14 01:30:59 [utils.py:233] non-default args: {'load_format': 'dummy', 'dtype': 'bfloat16', 'seed': 0, 'max_model_len': 36864, 'distributed_executor_backend': 'external_launcher', 'tensor_parallel_size': 8, 'enable_prefix_caching': False, 'gpu_memory_utilization': 0.85, 'max_num_batched_tokens': 36864, 'max_num_seqs': 128, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'speculative_config': {'method': 'sam', 'num_speculative_tokens': 3}, 'additional_config': {'torchair_graph_config': {'enabled': False}, 'ascend_scheduler_config': {'enabled': True, 'enable_chunked_prefill': False}, 'refresh': True, 'dynamic_eplb': False, 'num_iterations_eplb_update': 400, 'gate_eplb': True, 'num_wait_worker_iterations': 30}, 'model': '/home/data/Qwen3-32B'}
[36m(WorkerDict pid=2136260)[0m INFO 12-14 01:30:59 [model.py:547] Resolved architecture: Qwen3ForCausalLM
[36m(WorkerDict pid=2136260)[0m INFO 12-14 01:30:59 [model.py:1510] Using max model len 36864
[36m(WorkerDict pid=2136260)[0m INFO 12-14 01:30:59 [arg_utils.py:1215] Using ray runtime env: {'env_vars': {'MASTER_ADDR': '192.168.0.163', 'MASTER_PORT': '56741', 'NCCL_CUMEM_ENABLE': '0', 'NCCL_DEBUG': 'WARN', 'RANK': '12', 'RAY_LOCAL_WORLD_SIZE': '16', 'TOKENIZERS_PARALLELISM': 'true', 'VLLM_ALLOW_RUNTIME_LORA_UPDATING': 'true', 'WG_BACKEND': 'ray', 'WG_PREFIX': 'jWrpjg', 'WORLD_SIZE': '16'}}
[36m(WorkerDict pid=2136260)[0m INFO 12-14 01:30:59 [parallel.py:380] Using external launcher for distributed inference.
[36m(WorkerDict pid=2136260)[0m INFO 12-14 01:30:59 [parallel.py:420] Disabling V1 multiprocessing for external launcher.
[36m(WorkerDict pid=2136260)[0m INFO 12-14 01:30:59 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=36864.
[36m(WorkerDict pid=2136260)[0m INFO 12-14 01:30:59 [__init__.py:381] Cudagraph is disabled under eager mode
[36m(WorkerDict pid=2136260)[0m INFO 12-14 01:30:59 [platform.py:141] Non-MLA LLMs forcibly disable the chunked prefill feature,as the performance of operators supporting this feature functionality is currently suboptimal.
[36m(WorkerDict pid=2136260)[0m INFO 12-14 01:30:59 [platform.py:179] Compilation disabled, using eager mode by default
[36m(WorkerDict pid=2136259)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(WorkerDict pid=2136259)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(WorkerDict pid=2136259)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(WorkerDict pid=2136259)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(WorkerDict pid=2136259)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
[36m(WorkerDict pid=2136259)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=2136259)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=2136259)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
[36m(WorkerDict pid=2136259)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
[36m(WorkerDict pid=2136259)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(WorkerDict pid=2136259)[0m INFO 12-14 01:30:59 [utils.py:233] non-default args: {'load_format': 'dummy', 'dtype': 'bfloat16', 'seed': 0, 'max_model_len': 36864, 'distributed_executor_backend': 'external_launcher', 'tensor_parallel_size': 8, 'enable_prefix_caching': False, 'gpu_memory_utilization': 0.85, 'max_num_batched_tokens': 36864, 'max_num_seqs': 128, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'speculative_config': {'method': 'sam', 'num_speculative_tokens': 3}, 'additional_config': {'torchair_graph_config': {'enabled': False}, 'ascend_scheduler_config': {'enabled': True, 'enable_chunked_prefill': False}, 'refresh': True, 'dynamic_eplb': False, 'num_iterations_eplb_update': 400, 'gate_eplb': True, 'num_wait_worker_iterations': 30}, 'model': '/home/data/Qwen3-32B'}
[36m(WorkerDict pid=2136259)[0m INFO 12-14 01:30:59 [model.py:547] Resolved architecture: Qwen3ForCausalLM
[36m(WorkerDict pid=2136259)[0m INFO 12-14 01:30:59 [model.py:1510] Using max model len 36864
[36m(WorkerDict pid=2136259)[0m INFO 12-14 01:30:59 [arg_utils.py:1215] Using ray runtime env: {'env_vars': {'MASTER_ADDR': '192.168.0.163', 'MASTER_PORT': '56741', 'NCCL_CUMEM_ENABLE': '0', 'NCCL_DEBUG': 'WARN', 'RANK': '10', 'RAY_LOCAL_WORLD_SIZE': '16', 'TOKENIZERS_PARALLELISM': 'true', 'VLLM_ALLOW_RUNTIME_LORA_UPDATING': 'true', 'WG_BACKEND': 'ray', 'WG_PREFIX': 'jWrpjg', 'WORLD_SIZE': '16'}}
[36m(WorkerDict pid=2136259)[0m INFO 12-14 01:30:59 [parallel.py:380] Using external launcher for distributed inference.
[36m(WorkerDict pid=2136259)[0m INFO 12-14 01:30:59 [parallel.py:420] Disabling V1 multiprocessing for external launcher.
[36m(WorkerDict pid=2136259)[0m INFO 12-14 01:30:59 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=36864.
[36m(WorkerDict pid=2136259)[0m INFO 12-14 01:30:59 [__init__.py:381] Cudagraph is disabled under eager mode
[36m(WorkerDict pid=2136259)[0m INFO 12-14 01:30:59 [platform.py:141] Non-MLA LLMs forcibly disable the chunked prefill feature,as the performance of operators supporting this feature functionality is currently suboptimal.
[36m(WorkerDict pid=2136259)[0m INFO 12-14 01:30:59 [platform.py:179] Compilation disabled, using eager mode by default
[36m(WorkerDict pid=2136265)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(WorkerDict pid=2136265)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(WorkerDict pid=2136265)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(WorkerDict pid=2136265)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(WorkerDict pid=2136265)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
[36m(WorkerDict pid=2136265)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=2136265)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=2136265)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
[36m(WorkerDict pid=2136265)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
[36m(WorkerDict pid=2136265)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(WorkerDict pid=2136265)[0m INFO 12-14 01:30:59 [utils.py:233] non-default args: {'load_format': 'dummy', 'dtype': 'bfloat16', 'seed': 0, 'max_model_len': 36864, 'distributed_executor_backend': 'external_launcher', 'tensor_parallel_size': 8, 'enable_prefix_caching': False, 'gpu_memory_utilization': 0.85, 'max_num_batched_tokens': 36864, 'max_num_seqs': 128, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'speculative_config': {'method': 'sam', 'num_speculative_tokens': 3}, 'additional_config': {'torchair_graph_config': {'enabled': False}, 'ascend_scheduler_config': {'enabled': True, 'enable_chunked_prefill': False}, 'refresh': True, 'dynamic_eplb': False, 'num_iterations_eplb_update': 400, 'gate_eplb': True, 'num_wait_worker_iterations': 30}, 'model': '/home/data/Qwen3-32B'}
[36m(WorkerDict pid=2136265)[0m INFO 12-14 01:30:59 [model.py:547] Resolved architecture: Qwen3ForCausalLM
[36m(WorkerDict pid=2136265)[0m INFO 12-14 01:30:59 [model.py:1510] Using max model len 36864
[36m(WorkerDict pid=2136265)[0m INFO 12-14 01:30:59 [arg_utils.py:1215] Using ray runtime env: {'env_vars': {'MASTER_ADDR': '192.168.0.163', 'MASTER_PORT': '56741', 'NCCL_CUMEM_ENABLE': '0', 'NCCL_DEBUG': 'WARN', 'RANK': '15', 'RAY_LOCAL_WORLD_SIZE': '16', 'TOKENIZERS_PARALLELISM': 'true', 'VLLM_ALLOW_RUNTIME_LORA_UPDATING': 'true', 'WG_BACKEND': 'ray', 'WG_PREFIX': 'jWrpjg', 'WORLD_SIZE': '16'}}
[36m(WorkerDict pid=2136265)[0m INFO 12-14 01:30:59 [parallel.py:380] Using external launcher for distributed inference.
[36m(WorkerDict pid=2136265)[0m INFO 12-14 01:30:59 [parallel.py:420] Disabling V1 multiprocessing for external launcher.
[36m(WorkerDict pid=2136265)[0m INFO 12-14 01:30:59 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=36864.
[36m(WorkerDict pid=2136265)[0m INFO 12-14 01:30:59 [__init__.py:381] Cudagraph is disabled under eager mode
[36m(WorkerDict pid=2136265)[0m INFO 12-14 01:30:59 [platform.py:141] Non-MLA LLMs forcibly disable the chunked prefill feature,as the performance of operators supporting this feature functionality is currently suboptimal.
[36m(WorkerDict pid=2136265)[0m INFO 12-14 01:30:59 [platform.py:179] Compilation disabled, using eager mode by default
[36m(WorkerDict pid=2136269)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(WorkerDict pid=2136269)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(WorkerDict pid=2136269)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(WorkerDict pid=2136269)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(WorkerDict pid=2136269)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
[36m(WorkerDict pid=2136269)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=2136269)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=2136269)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
[36m(WorkerDict pid=2136269)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
[36m(WorkerDict pid=2136269)[0m WARNING 12-14 01:30:59 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(WorkerDict pid=2136269)[0m INFO 12-14 01:30:59 [utils.py:233] non-default args: {'load_format': 'dummy', 'dtype': 'bfloat16', 'seed': 0, 'max_model_len': 36864, 'distributed_executor_backend': 'external_launcher', 'tensor_parallel_size': 8, 'enable_prefix_caching': False, 'gpu_memory_utilization': 0.85, 'max_num_batched_tokens': 36864, 'max_num_seqs': 128, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'speculative_config': {'method': 'sam', 'num_speculative_tokens': 3}, 'additional_config': {'torchair_graph_config': {'enabled': False}, 'ascend_scheduler_config': {'enabled': True, 'enable_chunked_prefill': False}, 'refresh': True, 'dynamic_eplb': False, 'num_iterations_eplb_update': 400, 'gate_eplb': True, 'num_wait_worker_iterations': 30}, 'model': '/home/data/Qwen3-32B'}
[36m(WorkerDict pid=2136269)[0m INFO 12-14 01:30:59 [model.py:547] Resolved architecture: Qwen3ForCausalLM
[36m(WorkerDict pid=2136269)[0m INFO 12-14 01:30:59 [model.py:1510] Using max model len 36864
[36m(WorkerDict pid=2136269)[0m INFO 12-14 01:30:59 [arg_utils.py:1215] Using ray runtime env: {'env_vars': {'MASTER_ADDR': '192.168.0.163', 'MASTER_PORT': '56741', 'NCCL_CUMEM_ENABLE': '0', 'NCCL_DEBUG': 'WARN', 'RANK': '14', 'RAY_LOCAL_WORLD_SIZE': '16', 'TOKENIZERS_PARALLELISM': 'true', 'VLLM_ALLOW_RUNTIME_LORA_UPDATING': 'true', 'WG_BACKEND': 'ray', 'WG_PREFIX': 'jWrpjg', 'WORLD_SIZE': '16'}}
[36m(WorkerDict pid=2136269)[0m INFO 12-14 01:30:59 [parallel.py:380] Using external launcher for distributed inference.
[36m(WorkerDict pid=2136269)[0m INFO 12-14 01:30:59 [parallel.py:420] Disabling V1 multiprocessing for external launcher.
[36m(WorkerDict pid=2136269)[0m INFO 12-14 01:30:59 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=36864.
[36m(WorkerDict pid=2136269)[0m INFO 12-14 01:30:59 [__init__.py:381] Cudagraph is disabled under eager mode
[36m(WorkerDict pid=2136269)[0m INFO 12-14 01:30:59 [platform.py:141] Non-MLA LLMs forcibly disable the chunked prefill feature,as the performance of operators supporting this feature functionality is currently suboptimal.
[36m(WorkerDict pid=2136269)[0m INFO 12-14 01:30:59 [platform.py:179] Compilation disabled, using eager mode by default
[36m(WorkerDict pid=2136248)[0m INFO 12-14 01:30:59 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='/home/data/Qwen3-32B', speculative_config=SpeculativeConfig(method='sam', model='/home/data/Qwen3-32B', num_spec_tokens=3), tokenizer='/home/data/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=36864, download_dir=None, load_format=dummy, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=npu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/data/Qwen3-32B, enable_prefix_caching=False, chunked_prefill_enabled=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[36m(WorkerDict pid=2136257)[0m INFO 12-14 01:30:59 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='/home/data/Qwen3-32B', speculative_config=SpeculativeConfig(method='sam', model='/home/data/Qwen3-32B', num_spec_tokens=3), tokenizer='/home/data/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=36864, download_dir=None, load_format=dummy, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=npu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/data/Qwen3-32B, enable_prefix_caching=False, chunked_prefill_enabled=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[36m(WorkerDict pid=2136265)[0m INFO 12-14 01:30:59 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='/home/data/Qwen3-32B', speculative_config=SpeculativeConfig(method='sam', model='/home/data/Qwen3-32B', num_spec_tokens=3), tokenizer='/home/data/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=36864, download_dir=None, load_format=dummy, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=npu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/data/Qwen3-32B, enable_prefix_caching=False, chunked_prefill_enabled=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[36m(WorkerDict pid=2136258)[0m INFO 12-14 01:30:59 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='/home/data/Qwen3-32B', speculative_config=SpeculativeConfig(method='sam', model='/home/data/Qwen3-32B', num_spec_tokens=3), tokenizer='/home/data/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=36864, download_dir=None, load_format=dummy, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=npu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/data/Qwen3-32B, enable_prefix_caching=False, chunked_prefill_enabled=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[36m(WorkerDict pid=2136269)[0m INFO 12-14 01:30:59 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='/home/data/Qwen3-32B', speculative_config=SpeculativeConfig(method='sam', model='/home/data/Qwen3-32B', num_spec_tokens=3), tokenizer='/home/data/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=36864, download_dir=None, load_format=dummy, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=npu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/data/Qwen3-32B, enable_prefix_caching=False, chunked_prefill_enabled=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[36m(WorkerDict pid=2136241)[0m INFO 12-14 01:31:00 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='/home/data/Qwen3-32B', speculative_config=SpeculativeConfig(method='sam', model='/home/data/Qwen3-32B', num_spec_tokens=3), tokenizer='/home/data/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=36864, download_dir=None, load_format=dummy, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=npu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/data/Qwen3-32B, enable_prefix_caching=False, chunked_prefill_enabled=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[36m(WorkerDict pid=2136244)[0m INFO 12-14 01:30:59 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='/home/data/Qwen3-32B', speculative_config=SpeculativeConfig(method='sam', model='/home/data/Qwen3-32B', num_spec_tokens=3), tokenizer='/home/data/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=36864, download_dir=None, load_format=dummy, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=npu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/data/Qwen3-32B, enable_prefix_caching=False, chunked_prefill_enabled=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[36m(WorkerDict pid=2136247)[0m INFO 12-14 01:31:00 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='/home/data/Qwen3-32B', speculative_config=SpeculativeConfig(method='sam', model='/home/data/Qwen3-32B', num_spec_tokens=3), tokenizer='/home/data/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=36864, download_dir=None, load_format=dummy, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=npu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/data/Qwen3-32B, enable_prefix_caching=False, chunked_prefill_enabled=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[36m(WorkerDict pid=2136253)[0m INFO 12-14 01:30:59 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='/home/data/Qwen3-32B', speculative_config=SpeculativeConfig(method='sam', model='/home/data/Qwen3-32B', num_spec_tokens=3), tokenizer='/home/data/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=36864, download_dir=None, load_format=dummy, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=npu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/data/Qwen3-32B, enable_prefix_caching=False, chunked_prefill_enabled=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[36m(WorkerDict pid=2136249)[0m INFO 12-14 01:31:00 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='/home/data/Qwen3-32B', speculative_config=SpeculativeConfig(method='sam', model='/home/data/Qwen3-32B', num_spec_tokens=3), tokenizer='/home/data/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=36864, download_dir=None, load_format=dummy, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=npu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/data/Qwen3-32B, enable_prefix_caching=False, chunked_prefill_enabled=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[36m(WorkerDict pid=2136254)[0m INFO 12-14 01:30:59 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='/home/data/Qwen3-32B', speculative_config=SpeculativeConfig(method='sam', model='/home/data/Qwen3-32B', num_spec_tokens=3), tokenizer='/home/data/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=36864, download_dir=None, load_format=dummy, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=npu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/data/Qwen3-32B, enable_prefix_caching=False, chunked_prefill_enabled=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[36m(WorkerDict pid=2136256)[0m INFO 12-14 01:30:59 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='/home/data/Qwen3-32B', speculative_config=SpeculativeConfig(method='sam', model='/home/data/Qwen3-32B', num_spec_tokens=3), tokenizer='/home/data/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=36864, download_dir=None, load_format=dummy, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=npu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/data/Qwen3-32B, enable_prefix_caching=False, chunked_prefill_enabled=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[36m(WorkerDict pid=2136255)[0m INFO 12-14 01:31:00 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='/home/data/Qwen3-32B', speculative_config=SpeculativeConfig(method='sam', model='/home/data/Qwen3-32B', num_spec_tokens=3), tokenizer='/home/data/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=36864, download_dir=None, load_format=dummy, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=npu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/data/Qwen3-32B, enable_prefix_caching=False, chunked_prefill_enabled=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[36m(WorkerDict pid=2136261)[0m INFO 12-14 01:30:59 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='/home/data/Qwen3-32B', speculative_config=SpeculativeConfig(method='sam', model='/home/data/Qwen3-32B', num_spec_tokens=3), tokenizer='/home/data/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=36864, download_dir=None, load_format=dummy, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=npu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/data/Qwen3-32B, enable_prefix_caching=False, chunked_prefill_enabled=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[36m(WorkerDict pid=2136260)[0m INFO 12-14 01:30:59 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='/home/data/Qwen3-32B', speculative_config=SpeculativeConfig(method='sam', model='/home/data/Qwen3-32B', num_spec_tokens=3), tokenizer='/home/data/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=36864, download_dir=None, load_format=dummy, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=npu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/data/Qwen3-32B, enable_prefix_caching=False, chunked_prefill_enabled=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[36m(WorkerDict pid=2136259)[0m INFO 12-14 01:31:00 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='/home/data/Qwen3-32B', speculative_config=SpeculativeConfig(method='sam', model='/home/data/Qwen3-32B', num_spec_tokens=3), tokenizer='/home/data/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=36864, download_dir=None, load_format=dummy, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=npu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/data/Qwen3-32B, enable_prefix_caching=False, chunked_prefill_enabled=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[36m(WorkerDict pid=2136257)[0m INFO 12-14 01:31:08 [model_runner_v1.py:2660] Starting to load model /home/data/Qwen3-32B...
[36m(WorkerDict pid=2136258)[0m INFO 12-14 01:31:08 [model_runner_v1.py:2660] Starting to load model /home/data/Qwen3-32B...
[36m(WorkerDict pid=2136248)[0m INFO 12-14 01:31:08 [model_runner_v1.py:2660] Starting to load model /home/data/Qwen3-32B...
[36m(WorkerDict pid=2136265)[0m INFO 12-14 01:31:08 [model_runner_v1.py:2660] Starting to load model /home/data/Qwen3-32B...
[36m(WorkerDict pid=2136257)[0m INFO 12-14 01:31:08 [model_runner_v1.py:2677] Loading drafter model...
[36m(WorkerDict pid=2136258)[0m INFO 12-14 01:31:08 [model_runner_v1.py:2677] Loading drafter model...
[36m(WorkerDict pid=2136253)[0m INFO 12-14 01:31:09 [model_runner_v1.py:2660] Starting to load model /home/data/Qwen3-32B...
[36m(WorkerDict pid=2136254)[0m INFO 12-14 01:31:09 [model_runner_v1.py:2660] Starting to load model /home/data/Qwen3-32B...
[36m(WorkerDict pid=2136269)[0m INFO 12-14 01:31:09 [model_runner_v1.py:2660] Starting to load model /home/data/Qwen3-32B...
[36m(WorkerDict pid=2136248)[0m INFO 12-14 01:31:09 [model_runner_v1.py:2677] Loading drafter model...
[36m(WorkerDict pid=2136265)[0m INFO 12-14 01:31:09 [model_runner_v1.py:2677] Loading drafter model...
[36m(WorkerDict pid=2136257)[0m INFO 12-14 01:31:09 [model_runner_v1.py:2694] Loading model weights took 7.6393 GB
[36m(WorkerDict pid=2136258)[0m INFO 12-14 01:31:09 [model_runner_v1.py:2694] Loading model weights took 7.6393 GB
[36m(WorkerDict pid=2136254)[0m INFO 12-14 01:31:09 [model_runner_v1.py:2677] Loading drafter model...
[36m(WorkerDict pid=2136255)[0m INFO 12-14 01:31:09 [model_runner_v1.py:2660] Starting to load model /home/data/Qwen3-32B...
[36m(WorkerDict pid=2136269)[0m INFO 12-14 01:31:09 [model_runner_v1.py:2677] Loading drafter model...
[36m(WorkerDict pid=2136248)[0m INFO 12-14 01:31:09 [model_runner_v1.py:2694] Loading model weights took 7.6393 GB
[36m(WorkerDict pid=2136265)[0m INFO 12-14 01:31:10 [model_runner_v1.py:2694] Loading model weights took 7.6393 GB
[36m(WorkerDict pid=2136253)[0m INFO 12-14 01:31:10 [model_runner_v1.py:2677] Loading drafter model...
[36m(WorkerDict pid=2136254)[0m INFO 12-14 01:31:10 [model_runner_v1.py:2694] Loading model weights took 7.6393 GB
[36m(WorkerDict pid=2136269)[0m INFO 12-14 01:31:10 [model_runner_v1.py:2694] Loading model weights took 7.6393 GB
[36m(WorkerDict pid=2136255)[0m INFO 12-14 01:31:10 [model_runner_v1.py:2677] Loading drafter model...
[36m(WorkerDict pid=2136253)[0m INFO 12-14 01:31:12 [model_runner_v1.py:2694] Loading model weights took 7.6393 GB
[36m(WorkerDict pid=2136260)[0m INFO 12-14 01:31:12 [model_runner_v1.py:2660] Starting to load model /home/data/Qwen3-32B...
[36m(WorkerDict pid=2136255)[0m INFO 12-14 01:31:12 [model_runner_v1.py:2694] Loading model weights took 7.6393 GB
[36m(WorkerDict pid=2136260)[0m INFO 12-14 01:31:13 [model_runner_v1.py:2677] Loading drafter model...
[36m(WorkerDict pid=2136260)[0m INFO 12-14 01:31:13 [model_runner_v1.py:2694] Loading model weights took 7.6393 GB
[36m(WorkerDict pid=2136259)[0m INFO 12-14 01:31:14 [model_runner_v1.py:2660] Starting to load model /home/data/Qwen3-32B...
[36m(WorkerDict pid=2136249)[0m INFO 12-14 01:31:15 [model_runner_v1.py:2660] Starting to load model /home/data/Qwen3-32B...
[36m(WorkerDict pid=2136244)[0m INFO 12-14 01:31:15 [model_runner_v1.py:2660] Starting to load model /home/data/Qwen3-32B...
[36m(WorkerDict pid=2136249)[0m INFO 12-14 01:31:15 [model_runner_v1.py:2677] Loading drafter model...
[36m(WorkerDict pid=2136256)[0m INFO 12-14 01:31:15 [model_runner_v1.py:2660] Starting to load model /home/data/Qwen3-32B...
[36m(WorkerDict pid=2136259)[0m INFO 12-14 01:31:15 [model_runner_v1.py:2677] Loading drafter model...
[36m(WorkerDict pid=2136244)[0m INFO 12-14 01:31:16 [model_runner_v1.py:2677] Loading drafter model...
[36m(WorkerDict pid=2136256)[0m INFO 12-14 01:31:16 [model_runner_v1.py:2677] Loading drafter model...
[36m(WorkerDict pid=2136249)[0m INFO 12-14 01:31:16 [model_runner_v1.py:2694] Loading model weights took 7.6393 GB
[36m(WorkerDict pid=2136244)[0m INFO 12-14 01:31:16 [model_runner_v1.py:2694] Loading model weights took 7.6393 GB
[36m(WorkerDict pid=2136256)[0m INFO 12-14 01:31:17 [model_runner_v1.py:2694] Loading model weights took 7.6393 GB
[36m(WorkerDict pid=2136259)[0m INFO 12-14 01:31:17 [model_runner_v1.py:2694] Loading model weights took 7.6393 GB
[36m(WorkerDict pid=2136241)[0m INFO 12-14 01:31:23 [model_runner_v1.py:2660] Starting to load model /home/data/Qwen3-32B...
[36m(WorkerDict pid=2136247)[0m INFO 12-14 01:31:23 [model_runner_v1.py:2660] Starting to load model /home/data/Qwen3-32B...
[36m(WorkerDict pid=2136261)[0m INFO 12-14 01:31:23 [model_runner_v1.py:2660] Starting to load model /home/data/Qwen3-32B...
[36m(WorkerDict pid=2136241)[0m INFO 12-14 01:31:24 [model_runner_v1.py:2677] Loading drafter model...
[36m(WorkerDict pid=2136247)[0m INFO 12-14 01:31:24 [model_runner_v1.py:2677] Loading drafter model...
[36m(WorkerDict pid=2136261)[0m INFO 12-14 01:31:24 [model_runner_v1.py:2677] Loading drafter model...
[36m(WorkerDict pid=2136241)[0m INFO 12-14 01:31:24 [model_runner_v1.py:2694] Loading model weights took 7.6393 GB
[36m(WorkerDict pid=2136247)[0m INFO 12-14 01:31:25 [model_runner_v1.py:2694] Loading model weights took 7.6393 GB
[36m(WorkerDict pid=2136261)[0m INFO 12-14 01:31:25 [model_runner_v1.py:2694] Loading model weights took 7.6393 GB
[36m(WorkerDict pid=2136244)[0m INFO 12-14 01:31:32 [worker_v1.py:234] Available memory: 42062740172, total memory: 65796046848
[36m(WorkerDict pid=2136248)[0m INFO 12-14 01:31:32 [worker_v1.py:234] Available memory: 42056596172, total memory: 65796046848
[36m(WorkerDict pid=2136249)[0m INFO 12-14 01:31:32 [worker_v1.py:234] Available memory: 42059569868, total memory: 65796046848
[36m(WorkerDict pid=2136256)[0m INFO 12-14 01:31:32 [worker_v1.py:234] Available memory: 42063538892, total memory: 65796046848
[36m(WorkerDict pid=2136254)[0m INFO 12-14 01:31:32 [worker_v1.py:234] Available memory: 42056420044, total memory: 65796046848
[36m(WorkerDict pid=2136258)[0m INFO 12-14 01:31:32 [worker_v1.py:234] Available memory: 42057407180, total memory: 65796046848
[36m(WorkerDict pid=2136269)[0m INFO 12-14 01:31:32 [worker_v1.py:234] Available memory: 41806712320, total memory: 65787658240
[36m(WorkerDict pid=2136261)[0m INFO 12-14 01:31:32 [worker_v1.py:234] Available memory: 42068740812, total memory: 65796046848
[36m(WorkerDict pid=2136257)[0m INFO 12-14 01:31:32 [worker_v1.py:234] Available memory: 41812270592, total memory: 65787658240
[36m(WorkerDict pid=2136265)[0m INFO 12-14 01:31:32 [worker_v1.py:234] Available memory: 42065971916, total memory: 65796046848
[36m(WorkerDict pid=2136260)[0m INFO 12-14 01:31:32 [worker_v1.py:234] Available memory: 41801301504, total memory: 65787658240
[36m(WorkerDict pid=2136247)[0m INFO 12-14 01:31:32 [worker_v1.py:234] Available memory: 41805811200, total memory: 65787658240
[36m(WorkerDict pid=2136259)[0m INFO 12-14 01:31:33 [worker_v1.py:234] Available memory: 41812286976, total memory: 65787658240
[36m(WorkerDict pid=2136255)[0m INFO 12-14 01:31:33 [worker_v1.py:234] Available memory: 41800707584, total memory: 65787658240
[36m(WorkerDict pid=2136241)[0m INFO 12-14 01:31:33 [worker_v1.py:234] Available memory: 41803918848, total memory: 65787658240
[36m(WorkerDict pid=2136241)[0m INFO 12-14 01:31:33 [kv_cache_utils.py:1087] GPU KV cache size: 1,275,648 tokens
[36m(WorkerDict pid=2136241)[0m INFO 12-14 01:31:33 [kv_cache_utils.py:1091] Maximum concurrency for 36,864 tokens per request: 34.60x
[36m(WorkerDict pid=2136244)[0m INFO 12-14 01:31:33 [kv_cache_utils.py:1087] GPU KV cache size: 1,275,648 tokens
[36m(WorkerDict pid=2136244)[0m INFO 12-14 01:31:33 [kv_cache_utils.py:1091] Maximum concurrency for 36,864 tokens per request: 34.60x
[36m(WorkerDict pid=2136247)[0m INFO 12-14 01:31:33 [kv_cache_utils.py:1087] GPU KV cache size: 1,275,648 tokens
[36m(WorkerDict pid=2136247)[0m INFO 12-14 01:31:33 [kv_cache_utils.py:1091] Maximum concurrency for 36,864 tokens per request: 34.60x
[36m(WorkerDict pid=2136248)[0m INFO 12-14 01:31:33 [kv_cache_utils.py:1087] GPU KV cache size: 1,275,648 tokens
[36m(WorkerDict pid=2136248)[0m INFO 12-14 01:31:33 [kv_cache_utils.py:1091] Maximum concurrency for 36,864 tokens per request: 34.60x
[36m(WorkerDict pid=2136253)[0m INFO 12-14 01:31:33 [worker_v1.py:234] Available memory: 41806552576, total memory: 65787658240
[36m(WorkerDict pid=2136253)[0m INFO 12-14 01:31:33 [kv_cache_utils.py:1087] GPU KV cache size: 1,275,648 tokens
[36m(WorkerDict pid=2136253)[0m INFO 12-14 01:31:33 [kv_cache_utils.py:1091] Maximum concurrency for 36,864 tokens per request: 34.60x
[36m(WorkerDict pid=2136249)[0m INFO 12-14 01:31:33 [kv_cache_utils.py:1087] GPU KV cache size: 1,275,648 tokens
[36m(WorkerDict pid=2136249)[0m INFO 12-14 01:31:33 [kv_cache_utils.py:1091] Maximum concurrency for 36,864 tokens per request: 34.60x
[36m(WorkerDict pid=2136254)[0m INFO 12-14 01:31:33 [kv_cache_utils.py:1087] GPU KV cache size: 1,275,648 tokens
[36m(WorkerDict pid=2136254)[0m INFO 12-14 01:31:33 [kv_cache_utils.py:1091] Maximum concurrency for 36,864 tokens per request: 34.60x
[36m(WorkerDict pid=2136256)[0m INFO 12-14 01:31:33 [kv_cache_utils.py:1087] GPU KV cache size: 1,275,648 tokens
[36m(WorkerDict pid=2136256)[0m INFO 12-14 01:31:33 [kv_cache_utils.py:1091] Maximum concurrency for 36,864 tokens per request: 34.60x
[36m(WorkerDict pid=2136255)[0m INFO 12-14 01:31:33 [kv_cache_utils.py:1087] GPU KV cache size: 1,275,648 tokens
[36m(WorkerDict pid=2136255)[0m INFO 12-14 01:31:33 [kv_cache_utils.py:1091] Maximum concurrency for 36,864 tokens per request: 34.60x
[36m(WorkerDict pid=2136257)[0m INFO 12-14 01:31:33 [kv_cache_utils.py:1087] GPU KV cache size: 1,275,648 tokens
[36m(WorkerDict pid=2136257)[0m INFO 12-14 01:31:33 [kv_cache_utils.py:1091] Maximum concurrency for 36,864 tokens per request: 34.60x
[36m(WorkerDict pid=2136258)[0m INFO 12-14 01:31:33 [kv_cache_utils.py:1087] GPU KV cache size: 1,275,648 tokens
[36m(WorkerDict pid=2136258)[0m INFO 12-14 01:31:33 [kv_cache_utils.py:1091] Maximum concurrency for 36,864 tokens per request: 34.60x
[36m(WorkerDict pid=2136261)[0m INFO 12-14 01:31:33 [kv_cache_utils.py:1087] GPU KV cache size: 1,275,648 tokens
[36m(WorkerDict pid=2136261)[0m INFO 12-14 01:31:33 [kv_cache_utils.py:1091] Maximum concurrency for 36,864 tokens per request: 34.60x
[36m(WorkerDict pid=2136260)[0m INFO 12-14 01:31:33 [kv_cache_utils.py:1087] GPU KV cache size: 1,275,648 tokens
[36m(WorkerDict pid=2136260)[0m INFO 12-14 01:31:33 [kv_cache_utils.py:1091] Maximum concurrency for 36,864 tokens per request: 34.60x
[36m(WorkerDict pid=2136259)[0m INFO 12-14 01:31:33 [kv_cache_utils.py:1087] GPU KV cache size: 1,275,648 tokens
[36m(WorkerDict pid=2136259)[0m INFO 12-14 01:31:33 [kv_cache_utils.py:1091] Maximum concurrency for 36,864 tokens per request: 34.60x
[36m(WorkerDict pid=2136265)[0m INFO 12-14 01:31:33 [kv_cache_utils.py:1087] GPU KV cache size: 1,275,648 tokens
[36m(WorkerDict pid=2136265)[0m INFO 12-14 01:31:33 [kv_cache_utils.py:1091] Maximum concurrency for 36,864 tokens per request: 34.60x
[36m(WorkerDict pid=2136269)[0m INFO 12-14 01:31:33 [kv_cache_utils.py:1087] GPU KV cache size: 1,275,648 tokens
[36m(WorkerDict pid=2136269)[0m INFO 12-14 01:31:33 [kv_cache_utils.py:1091] Maximum concurrency for 36,864 tokens per request: 34.60x
[36m(WorkerDict pid=2136241)[0m INFO 12-14 01:31:33 [core.py:210] init engine (profile, create kv cache, warmup model) took 8.62 seconds
[36m(WorkerDict pid=2136244)[0m INFO 12-14 01:31:33 [core.py:210] init engine (profile, create kv cache, warmup model) took 16.76 seconds
[36m(WorkerDict pid=2136247)[0m INFO 12-14 01:31:33 [core.py:210] init engine (profile, create kv cache, warmup model) took 8.42 seconds
[36m(WorkerDict pid=2136248)[0m INFO 12-14 01:31:33 [core.py:210] init engine (profile, create kv cache, warmup model) took 23.64 seconds
[36m(WorkerDict pid=2136253)[0m INFO 12-14 01:31:33 [core.py:210] init engine (profile, create kv cache, warmup model) took 21.58 seconds
[36m(WorkerDict pid=2136249)[0m INFO 12-14 01:31:33 [core.py:210] init engine (profile, create kv cache, warmup model) took 17.19 seconds
[36m(WorkerDict pid=2136254)[0m INFO 12-14 01:31:33 [core.py:210] init engine (profile, create kv cache, warmup model) took 23.02 seconds
[36m(WorkerDict pid=2136256)[0m INFO 12-14 01:31:33 [core.py:210] init engine (profile, create kv cache, warmup model) took 16.58 seconds
[36m(WorkerDict pid=2136255)[0m INFO 12-14 01:31:33 [core.py:210] init engine (profile, create kv cache, warmup model) took 20.65 seconds
[36m(WorkerDict pid=2136257)[0m INFO 12-14 01:31:33 [core.py:210] init engine (profile, create kv cache, warmup model) took 24.08 seconds
[36m(WorkerDict pid=2136258)[0m INFO 12-14 01:31:33 [core.py:210] init engine (profile, create kv cache, warmup model) took 24.04 seconds
[36m(WorkerDict pid=2136261)[0m INFO 12-14 01:31:33 [core.py:210] init engine (profile, create kv cache, warmup model) took 8.41 seconds
[36m(WorkerDict pid=2136260)[0m INFO 12-14 01:31:33 [core.py:210] init engine (profile, create kv cache, warmup model) took 19.68 seconds
[36m(WorkerDict pid=2136259)[0m INFO 12-14 01:31:33 [core.py:210] init engine (profile, create kv cache, warmup model) took 16.44 seconds
[36m(WorkerDict pid=2136265)[0m INFO 12-14 01:31:33 [core.py:210] init engine (profile, create kv cache, warmup model) took 23.53 seconds
[36m(WorkerDict pid=2136269)[0m INFO 12-14 01:31:33 [core.py:210] init engine (profile, create kv cache, warmup model) took 23.09 seconds
[36m(WorkerDict pid=2136257)[0m WARNING 12-14 01:31:34 [core.py:112] Using configured V1 scheduler class vllm_ascend.core.scheduler.AscendScheduler. This scheduler interface is not public and compatibility may not be maintained.
[36m(WorkerDict pid=2136257)[0m INFO 12-14 01:31:34 [llm.py:307] Supported_tasks: ('generate',)
[36m(WorkerDict pid=2136248)[0m WARNING 12-14 01:31:34 [core.py:112] Using configured V1 scheduler class vllm_ascend.core.scheduler.AscendScheduler. This scheduler interface is not public and compatibility may not be maintained.
[36m(WorkerDict pid=2136248)[0m INFO 12-14 01:31:34 [llm.py:307] Supported_tasks: ('generate',)
[36m(WorkerDict pid=2136258)[0m WARNING 12-14 01:31:34 [core.py:112] Using configured V1 scheduler class vllm_ascend.core.scheduler.AscendScheduler. This scheduler interface is not public and compatibility may not be maintained.
[36m(WorkerDict pid=2136258)[0m INFO 12-14 01:31:34 [llm.py:307] Supported_tasks: ('generate',)
[36m(WorkerDict pid=2136241)[0m WARNING 12-14 01:31:34 [core.py:112] Using configured V1 scheduler class vllm_ascend.core.scheduler.AscendScheduler. This scheduler interface is not public and compatibility may not be maintained.
[36m(WorkerDict pid=2136241)[0m INFO 12-14 01:31:34 [llm.py:307] Supported_tasks: ('generate',)
[36m(WorkerDict pid=2136244)[0m WARNING 12-14 01:31:34 [core.py:112] Using configured V1 scheduler class vllm_ascend.core.scheduler.AscendScheduler. This scheduler interface is not public and compatibility may not be maintained.
[36m(WorkerDict pid=2136244)[0m INFO 12-14 01:31:34 [llm.py:307] Supported_tasks: ('generate',)
[36m(WorkerDict pid=2136247)[0m WARNING 12-14 01:31:34 [core.py:112] Using configured V1 scheduler class vllm_ascend.core.scheduler.AscendScheduler. This scheduler interface is not public and compatibility may not be maintained.
[36m(WorkerDict pid=2136247)[0m INFO 12-14 01:31:34 [llm.py:307] Supported_tasks: ('generate',)
[36m(WorkerDict pid=2136249)[0m WARNING 12-14 01:31:34 [core.py:112] Using configured V1 scheduler class vllm_ascend.core.scheduler.AscendScheduler. This scheduler interface is not public and compatibility may not be maintained.
[36m(WorkerDict pid=2136249)[0m INFO 12-14 01:31:34 [llm.py:307] Supported_tasks: ('generate',)
[36m(WorkerDict pid=2136254)[0m WARNING 12-14 01:31:34 [core.py:112] Using configured V1 scheduler class vllm_ascend.core.scheduler.AscendScheduler. This scheduler interface is not public and compatibility may not be maintained.
[36m(WorkerDict pid=2136254)[0m INFO 12-14 01:31:34 [llm.py:307] Supported_tasks: ('generate',)
[36m(WorkerDict pid=2136256)[0m WARNING 12-14 01:31:34 [core.py:112] Using configured V1 scheduler class vllm_ascend.core.scheduler.AscendScheduler. This scheduler interface is not public and compatibility may not be maintained.
[36m(WorkerDict pid=2136256)[0m INFO 12-14 01:31:34 [llm.py:307] Supported_tasks: ('generate',)
[36m(WorkerDict pid=2136261)[0m WARNING 12-14 01:31:34 [core.py:112] Using configured V1 scheduler class vllm_ascend.core.scheduler.AscendScheduler. This scheduler interface is not public and compatibility may not be maintained.
[36m(WorkerDict pid=2136261)[0m INFO 12-14 01:31:34 [llm.py:307] Supported_tasks: ('generate',)
[36m(WorkerDict pid=2136260)[0m WARNING 12-14 01:31:34 [core.py:112] Using configured V1 scheduler class vllm_ascend.core.scheduler.AscendScheduler. This scheduler interface is not public and compatibility may not be maintained.
[36m(WorkerDict pid=2136260)[0m INFO 12-14 01:31:34 [llm.py:307] Supported_tasks: ('generate',)
[36m(WorkerDict pid=2136259)[0m WARNING 12-14 01:31:34 [core.py:112] Using configured V1 scheduler class vllm_ascend.core.scheduler.AscendScheduler. This scheduler interface is not public and compatibility may not be maintained.
[36m(WorkerDict pid=2136259)[0m INFO 12-14 01:31:34 [llm.py:307] Supported_tasks: ('generate',)
[36m(WorkerDict pid=2136265)[0m WARNING 12-14 01:31:34 [core.py:112] Using configured V1 scheduler class vllm_ascend.core.scheduler.AscendScheduler. This scheduler interface is not public and compatibility may not be maintained.
[36m(WorkerDict pid=2136265)[0m INFO 12-14 01:31:34 [llm.py:307] Supported_tasks: ('generate',)
[36m(WorkerDict pid=2136269)[0m WARNING 12-14 01:31:34 [core.py:112] Using configured V1 scheduler class vllm_ascend.core.scheduler.AscendScheduler. This scheduler interface is not public and compatibility may not be maintained.
[36m(WorkerDict pid=2136269)[0m INFO 12-14 01:31:34 [llm.py:307] Supported_tasks: ('generate',)
[36m(WorkerDict pid=2136253)[0m WARNING 12-14 01:31:34 [core.py:112] Using configured V1 scheduler class vllm_ascend.core.scheduler.AscendScheduler. This scheduler interface is not public and compatibility may not be maintained.
[36m(WorkerDict pid=2136253)[0m INFO 12-14 01:31:34 [llm.py:307] Supported_tasks: ('generate',)
[36m(WorkerDict pid=2136255)[0m WARNING 12-14 01:31:34 [core.py:112] Using configured V1 scheduler class vllm_ascend.core.scheduler.AscendScheduler. This scheduler interface is not public and compatibility may not be maintained.
[36m(WorkerDict pid=2136255)[0m INFO 12-14 01:31:34 [llm.py:307] Supported_tasks: ('generate',)
[36m(WorkerDict pid=2136265)[0m [Rank 15 | Local Rank 0] 2025-12-14 01:31:43,277 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py:265] => The 'logprobs' parameter is incompatible with Speculative Decoding and has been disabled.
[36m(WorkerDict pid=2136265)[0m kwargs: {'n': 1, 'max_tokens': 34816, 'repetition_penalty': 1.0, 'detokenize': False, 'temperature': 0.9, 'top_k': -1, 'top_p': 1.0, 'ignore_eos': False}
[36m(WorkerDict pid=2136249)[0m [Rank 11 | Local Rank 0] 2025-12-14 01:31:43,448 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py:265] => The 'logprobs' parameter is incompatible with Speculative Decoding and has been disabled.
[36m(WorkerDict pid=2136249)[0m kwargs: {'n': 1, 'max_tokens': 34816, 'repetition_penalty': 1.0, 'detokenize': False, 'temperature': 0.9, 'top_k': -1, 'top_p': 1.0, 'ignore_eos': False}
[36m(WorkerDict pid=2136256)[0m [Rank 7 | Local Rank 0] 2025-12-14 01:31:43,393 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py:265] => The 'logprobs' parameter is incompatible with Speculative Decoding and has been disabled.
[36m(WorkerDict pid=2136256)[0m kwargs: {'n': 1, 'max_tokens': 34816, 'repetition_penalty': 1.0, 'detokenize': False, 'temperature': 0.9, 'top_k': -1, 'top_p': 1.0, 'ignore_eos': False}
[36m(WorkerDict pid=2136258)[0m [Rank 9 | Local Rank 0] 2025-12-14 01:31:43,396 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py:265] => The 'logprobs' parameter is incompatible with Speculative Decoding and has been disabled.
[36m(WorkerDict pid=2136258)[0m kwargs: {'n': 1, 'max_tokens': 34816, 'repetition_penalty': 1.0, 'detokenize': False, 'temperature': 0.9, 'top_k': -1, 'top_p': 1.0, 'ignore_eos': False}
[36m(WorkerDict pid=2136261)[0m [Rank 13 | Local Rank 0] 2025-12-14 01:31:43,379 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py:265] => The 'logprobs' parameter is incompatible with Speculative Decoding and has been disabled.
[36m(WorkerDict pid=2136261)[0m kwargs: {'n': 1, 'max_tokens': 34816, 'repetition_penalty': 1.0, 'detokenize': False, 'temperature': 0.9, 'top_k': -1, 'top_p': 1.0, 'ignore_eos': False}
[36m(WorkerDict pid=2136248)[0m [Rank 3 | Local Rank 0] 2025-12-14 01:31:43,476 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py:265] => The 'logprobs' parameter is incompatible with Speculative Decoding and has been disabled.
[36m(WorkerDict pid=2136248)[0m kwargs: {'n': 1, 'max_tokens': 34816, 'repetition_penalty': 1.0, 'detokenize': False, 'temperature': 0.9, 'top_k': -1, 'top_p': 1.0, 'ignore_eos': False}
[36m(WorkerDict pid=2136244)[0m [Rank 1 | Local Rank 0] 2025-12-14 01:31:43,591 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py:265] => The 'logprobs' parameter is incompatible with Speculative Decoding and has been disabled.
[36m(WorkerDict pid=2136244)[0m kwargs: {'n': 1, 'max_tokens': 34816, 'repetition_penalty': 1.0, 'detokenize': False, 'temperature': 0.9, 'top_k': -1, 'top_p': 1.0, 'ignore_eos': False}
[36m(WorkerDict pid=2136254)[0m [Rank 5 | Local Rank 0] 2025-12-14 01:31:43,721 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py:265] => The 'logprobs' parameter is incompatible with Speculative Decoding and has been disabled.
[36m(WorkerDict pid=2136254)[0m kwargs: {'n': 1, 'max_tokens': 34816, 'repetition_penalty': 1.0, 'detokenize': False, 'temperature': 0.9, 'top_k': -1, 'top_p': 1.0, 'ignore_eos': False}
[36m(WorkerDict pid=2136257)[0m [Rank 8 | Local Rank 0] 2025-12-14 01:31:43,829 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py:265] => The 'logprobs' parameter is incompatible with Speculative Decoding and has been disabled.
[36m(WorkerDict pid=2136257)[0m kwargs: {'n': 1, 'max_tokens': 34816, 'repetition_penalty': 1.0, 'detokenize': False, 'temperature': 0.9, 'top_k': -1, 'top_p': 1.0, 'ignore_eos': False}
[36m(WorkerDict pid=2136241)[0m [Rank 0 | Local Rank 0] 2025-12-14 01:31:44,155 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py:265] => The 'logprobs' parameter is incompatible with Speculative Decoding and has been disabled.
[36m(WorkerDict pid=2136241)[0m kwargs: {'n': 1, 'max_tokens': 34816, 'repetition_penalty': 1.0, 'detokenize': False, 'temperature': 0.9, 'top_k': -1, 'top_p': 1.0, 'ignore_eos': False}
[36m(WorkerDict pid=2136241)[0m Overridden TransformerConfig init config: {'num_layers': 64, 'hidden_size': 5120, 'num_attention_heads': 64, 'num_query_groups': 8, 'ffn_hidden_size': 25600, 'attention_dropout': 0.0, 'hidden_dropout': 0.0, 'kv_channels': 128, 'layernorm_epsilon': 1e-06, 'add_bias_linear': False, 'activation_func': <function silu at 0xffff04265ee0>, 'normalization': 'RMSNorm', 'gated_linear_unit': True, 'pipeline_dtype': torch.bfloat16, 'params_dtype': torch.bfloat16, 'bf16': True, 'tensor_model_parallel_size': 8, 'pipeline_model_parallel_size': 2, 'expert_model_parallel_size': 1, 'expert_tensor_parallel_size': 1, 'virtual_pipeline_model_parallel_size': None, 'context_parallel_size': 1, 'overlap_p2p_comm': False, 'batch_p2p_comm': False, 'sequence_parallel': True, 'variable_seq_lengths': True, 'masked_softmax_fusion': True, 'moe_token_dispatcher_type': 'alltoall', 'use_cpu_initialization': False, 'add_qkv_bias': False, 'qk_layernorm': True}
[36m(WorkerDict pid=2136247)[0m [Rank 2 | Local Rank 0] 2025-12-14 01:31:44,058 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py:265] => The 'logprobs' parameter is incompatible with Speculative Decoding and has been disabled.
[36m(WorkerDict pid=2136247)[0m kwargs: {'n': 1, 'max_tokens': 34816, 'repetition_penalty': 1.0, 'detokenize': False, 'temperature': 0.9, 'top_k': -1, 'top_p': 1.0, 'ignore_eos': False}
[36m(WorkerDict pid=2136260)[0m [Rank 12 | Local Rank 0] 2025-12-14 01:31:44,063 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py:265] => The 'logprobs' parameter is incompatible with Speculative Decoding and has been disabled.
[36m(WorkerDict pid=2136260)[0m kwargs: {'n': 1, 'max_tokens': 34816, 'repetition_penalty': 1.0, 'detokenize': False, 'temperature': 0.9, 'top_k': -1, 'top_p': 1.0, 'ignore_eos': False}
[36m(WorkerDict pid=2136269)[0m [Rank 14 | Local Rank 0] 2025-12-14 01:31:44,254 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py:265] => The 'logprobs' parameter is incompatible with Speculative Decoding and has been disabled.
[36m(WorkerDict pid=2136269)[0m kwargs: {'n': 1, 'max_tokens': 34816, 'repetition_penalty': 1.0, 'detokenize': False, 'temperature': 0.9, 'top_k': -1, 'top_p': 1.0, 'ignore_eos': False}
[36m(WorkerDict pid=2136253)[0m [Rank 4 | Local Rank 0] 2025-12-14 01:31:44,355 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py:265] => The 'logprobs' parameter is incompatible with Speculative Decoding and has been disabled.
[36m(WorkerDict pid=2136253)[0m kwargs: {'n': 1, 'max_tokens': 34816, 'repetition_penalty': 1.0, 'detokenize': False, 'temperature': 0.9, 'top_k': -1, 'top_p': 1.0, 'ignore_eos': False}
[36m(WorkerDict pid=2136259)[0m [Rank 10 | Local Rank 0] 2025-12-14 01:31:44,390 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py:265] => The 'logprobs' parameter is incompatible with Speculative Decoding and has been disabled.
[36m(WorkerDict pid=2136259)[0m kwargs: {'n': 1, 'max_tokens': 34816, 'repetition_penalty': 1.0, 'detokenize': False, 'temperature': 0.9, 'top_k': -1, 'top_p': 1.0, 'ignore_eos': False}
[36m(WorkerDict pid=2136255)[0m [Rank 6 | Local Rank 0] 2025-12-14 01:31:44,442 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py:265] => The 'logprobs' parameter is incompatible with Speculative Decoding and has been disabled.
[36m(WorkerDict pid=2136255)[0m kwargs: {'n': 1, 'max_tokens': 34816, 'repetition_penalty': 1.0, 'detokenize': False, 'temperature': 0.9, 'top_k': -1, 'top_p': 1.0, 'ignore_eos': False}
[36m(TaskRunner pid=2108541)[0m [TaskRunner] Before trainer.fit()
[36m(TaskRunner pid=2108541)[0m Saving tensorboard log to tensorboard_log/verl_grpo_example_deepscaler/qwen3_32b_verl_true_weights.
[36m(TaskRunner pid=2108541)[0m Checkpoint tracker file does not exist: /workspace/cann-recipes-train/llm_rl/qwen3/checkpoints/verl_grpo_example_deepscaler/qwen3_32b_verl_true_weights/latest_checkpointed_iteration.txt
[36m(TaskRunner pid=2108541)[0m Training from scratch
[36m(WorkerDict pid=2136258)[0m INFO 12-14 01:31:54 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136248)[0m INFO 12-14 01:31:54 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136254)[0m INFO 12-14 01:31:54 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136257)[0m INFO 12-14 01:31:54 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136265)[0m INFO 12-14 01:31:54 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136269)[0m INFO 12-14 01:31:54 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136255)[0m INFO 12-14 01:31:54 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136255)[0m WARNING 12-14 01:31:54 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[36m(WorkerDict pid=2136269)[0m WARNING 12-14 01:31:54 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[36m(WorkerDict pid=2136253)[0m INFO 12-14 01:31:54 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136241)[0m INFO 12-14 01:31:56 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136241)[0m WARNING 12-14 01:31:56 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[36m(WorkerDict pid=2136257)[0m WARNING 12-14 01:31:56 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[36m(WorkerDict pid=2136256)[0m INFO 12-14 01:31:56 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136256)[0m WARNING 12-14 01:31:56 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[36m(WorkerDict pid=2136265)[0m WARNING 12-14 01:31:56 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[36m(WorkerDict pid=2136244)[0m INFO 12-14 01:31:56 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136244)[0m WARNING 12-14 01:31:56 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[36m(WorkerDict pid=2136258)[0m WARNING 12-14 01:31:56 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[36m(WorkerDict pid=2136249)[0m INFO 12-14 01:31:56 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136248)[0m WARNING 12-14 01:31:57 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[36m(WorkerDict pid=2136249)[0m WARNING 12-14 01:31:57 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[36m(WorkerDict pid=2136247)[0m INFO 12-14 01:31:57 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136254)[0m WARNING 12-14 01:31:57 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[36m(WorkerDict pid=2136261)[0m INFO 12-14 01:31:57 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136261)[0m WARNING 12-14 01:31:57 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[36m(WorkerDict pid=2136260)[0m INFO 12-14 01:31:57 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136253)[0m WARNING 12-14 01:31:57 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[36m(WorkerDict pid=2136260)[0m WARNING 12-14 01:31:57 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[36m(WorkerDict pid=2136247)[0m WARNING 12-14 01:31:57 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[36m(WorkerDict pid=2136259)[0m INFO 12-14 01:31:57 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136259)[0m WARNING 12-14 01:31:57 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[36m(WorkerDict pid=2136248)[0m ninja: no work to do.
[36m(WorkerDict pid=2136257)[0m ninja: no work to do.
[36m(WorkerDict pid=2136249)[0m [Rank 11 | Local Rank 0] 2025-12-14 02:14:23,530 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/actor/megatron_actor.py:328] => For memory-efficient computation, enable fused kernels via `actor_rollout_ref.model.use_fused_kernels=True`. The current `clone()` operation ensures correctness but increases memory usage.
[36m(WorkerDict pid=2136257)[0m [Rank 8 | Local Rank 0] 2025-12-14 02:14:23,528 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/actor/megatron_actor.py:328] => For memory-efficient computation, enable fused kernels via `actor_rollout_ref.model.use_fused_kernels=True`. The current `clone()` operation ensures correctness but increases memory usage.
[36m(WorkerDict pid=2136258)[0m [Rank 9 | Local Rank 0] 2025-12-14 02:14:23,529 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/actor/megatron_actor.py:328] => For memory-efficient computation, enable fused kernels via `actor_rollout_ref.model.use_fused_kernels=True`. The current `clone()` operation ensures correctness but increases memory usage.
[36m(WorkerDict pid=2136261)[0m [Rank 13 | Local Rank 0] 2025-12-14 02:14:23,530 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/actor/megatron_actor.py:328] => For memory-efficient computation, enable fused kernels via `actor_rollout_ref.model.use_fused_kernels=True`. The current `clone()` operation ensures correctness but increases memory usage.
[36m(WorkerDict pid=2136260)[0m [Rank 12 | Local Rank 0] 2025-12-14 02:14:23,532 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/actor/megatron_actor.py:328] => For memory-efficient computation, enable fused kernels via `actor_rollout_ref.model.use_fused_kernels=True`. The current `clone()` operation ensures correctness but increases memory usage.
[36m(WorkerDict pid=2136259)[0m [Rank 10 | Local Rank 0] 2025-12-14 02:14:23,531 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/actor/megatron_actor.py:328] => For memory-efficient computation, enable fused kernels via `actor_rollout_ref.model.use_fused_kernels=True`. The current `clone()` operation ensures correctness but increases memory usage.
[36m(WorkerDict pid=2136265)[0m [Rank 15 | Local Rank 0] 2025-12-14 02:14:23,542 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/actor/megatron_actor.py:328] => For memory-efficient computation, enable fused kernels via `actor_rollout_ref.model.use_fused_kernels=True`. The current `clone()` operation ensures correctness but increases memory usage.
[36m(WorkerDict pid=2136269)[0m [Rank 14 | Local Rank 0] 2025-12-14 02:14:23,529 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/actor/megatron_actor.py:328] => For memory-efficient computation, enable fused kernels via `actor_rollout_ref.model.use_fused_kernels=True`. The current `clone()` operation ensures correctness but increases memory usage.
[36m(WorkerDict pid=2136258)[0m ninja: no work to do.
[36m(TaskRunner pid=2108541)[0m Dumped generations to /workspace/data/dump/1.jsonl
[36m(TaskRunner pid=2108541)[0m step:1 - actor/entropy:0.2555282711982727 - actor/kl_loss:-2.1947922849071355e-10 - actor/kl_coef:0.0010000000000000002 - actor/pg_loss:0.0012157415834868827 - actor/pg_clipfrac:0.0 - actor/ppo_kl:-1.1076155256565502e-09 - actor/pg_clipfrac_lower:0.0 - actor/grad_norm:0.028906233447624313 - perf/mfu/actor:0.40432668148477113 - perf/max_memory_allocated_gb:47.246397972106934 - perf/max_memory_reserved_gb:47.375 - perf/cpu_memory_used_gb:995.6583976745605 - actor/lr:1e-06 - training/global_step:1 - training/epoch:0 - critic/score/mean:0.91015625 - critic/score/max:1.0 - critic/score/min:0.0 - critic/rewards/mean:0.91015625 - critic/rewards/max:1.0 - critic/rewards/min:0.0 - critic/advantages/mean:-0.0013948868727311492 - critic/advantages/max:0.8539109230041504 - critic/advantages/min:-2.561730146408081 - critic/returns/mean:-0.0013948868727311492 - critic/returns/max:0.8539109230041504 - critic/returns/min:-2.561730146408081 - response_length/mean:5804.841796875 - response_length/max:34816.0 - response_length/min:877.0 - response_length/clip_ratio:0.0078125 - response_length_non_aborted/mean:5804.841796875 - response_length_non_aborted/max:34816.0 - response_length_non_aborted/min:877.0 - response_length_non_aborted/clip_ratio:0.0078125 - response/aborted_ratio:0.0 - prompt_length/mean:78.15625 - prompt_length/max:191.0 - prompt_length/min:34.0 - prompt_length/clip_ratio:0.0 - timing_s/start_profile:0.0004230220802128315 - timing_s/generate_sequences:2520.799072265625 - timing_s/generation_timing/max:2522.459228515625 - timing_s/generation_timing/min:2519.0400390625 - timing_s/generation_timing/topk_ratio:0.125 - timing_s/gen:2549.2286252039485 - timing_s/reward:0.9651640290394425 - timing_s/old_log_prob:113.00333625683561 - timing_s/ref:105.07309376401827 - timing_s/adv:0.1568740839138627 - timing_s/update_actor:354.0888996287249 - timing_s/dump_rollout_generations:2.0813410780392587 - timing_s/step:3124.6056611537933 - timing_s/stop_profile:7.522013038396835e-05 - timing_per_token_ms/ref:0.034883725036567 - timing_per_token_ms/adv:5.208138651465598e-05 - timing_per_token_ms/update_actor:0.1175556878613473 - timing_per_token_ms/gen:0.8577257284224102 - perf/total_num_tokens:3012095 - perf/time_per_step:3124.6056611537933 - perf/throughput:60.249502790212745
[36m(WorkerDict pid=2136258)[0m INFO 12-14 02:23:59 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136248)[0m INFO 12-14 02:23:59 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136254)[0m INFO 12-14 02:23:59 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136257)[0m INFO 12-14 02:23:59 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136241)[0m INFO 12-14 02:23:59 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136253)[0m INFO 12-14 02:23:59 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136265)[0m INFO 12-14 02:23:59 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136269)[0m INFO 12-14 02:23:59 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136255)[0m INFO 12-14 02:23:59 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136261)[0m INFO 12-14 02:24:00 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136244)[0m INFO 12-14 02:24:00 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136256)[0m INFO 12-14 02:24:01 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136249)[0m INFO 12-14 02:24:01 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136260)[0m INFO 12-14 02:24:01 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136247)[0m INFO 12-14 02:24:01 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136259)[0m INFO 12-14 02:24:01 [block_pool.py:378] Successfully reset prefix cache
[36m(TaskRunner pid=2108541)[0m old timing: {'generate_sequences': 1833.7559814453125, 'generation_timing/max': 1834.6734619140625, 'generation_timing/min': 1832.61279296875, 'generation_timing/topk_ratio': 0.125}
[36m(TaskRunner pid=2108541)[0m new timing: {'generate_sequences': 1833.756103515625, 'generation_timing/max': 1834.6734619140625, 'generation_timing/min': 1832.61279296875, 'generation_timing/topk_ratio': 0.125}
[36m(TaskRunner pid=2108541)[0m Dumped generations to /workspace/data/dump/2.jsonl
[36m(TaskRunner pid=2108541)[0m step:2 - actor/entropy:0.2781406342983246 - actor/kl_loss:0.0009952645709581044 - actor/kl_coef:0.0010000000000000002 - actor/pg_loss:-0.00032403052459100643 - actor/pg_clipfrac:0.0 - actor/ppo_kl:-2.7824513911129437e-10 - actor/pg_clipfrac_lower:0.0 - actor/grad_norm:0.039475765731040086 - perf/mfu/actor:0.3961528930012543 - perf/max_memory_allocated_gb:47.85068607330322 - perf/max_memory_reserved_gb:48.01953125 - perf/cpu_memory_used_gb:995.485767364502 - actor/lr:1e-06 - training/global_step:2 - training/epoch:0 - critic/score/mean:0.681640625 - critic/score/max:1.0 - critic/score/min:0.0 - critic/rewards/mean:0.681640625 - critic/rewards/max:1.0 - critic/rewards/min:0.0 - critic/advantages/mean:0.0003428845957387239 - critic/advantages/max:3.7499847412109375 - critic/advantages/min:-3.7499847412109375 - critic/returns/mean:0.0003428845957387239 - critic/returns/max:3.7499847412109375 - critic/returns/min:-3.7499847412109375 - response_length/mean:5861.2109375 - response_length/max:24766.0 - response_length/min:959.0 - response_length/clip_ratio:0.0 - response_length_non_aborted/mean:5861.2109375 - response_length_non_aborted/max:24766.0 - response_length_non_aborted/min:959.0 - response_length_non_aborted/clip_ratio:0.0 - response/aborted_ratio:0.0 - prompt_length/mean:74.375 - prompt_length/max:176.0 - prompt_length/min:23.0 - prompt_length/clip_ratio:0.0 - timing_s/start_profile:6.745010614395142e-05 - timing_s/generate_sequences:1833.7559814453125 - timing_s/generation_timing/max:1834.6734619140625 - timing_s/generation_timing/min:1832.61279296875 - timing_s/generation_timing/topk_ratio:0.125 - timing_s/gen:1858.5783107928 - timing_s/reward:0.9599964758381248 - timing_s/old_log_prob:109.04293211502954 - timing_s/ref:103.6118728951551 - timing_s/adv:0.03630603197962046 - timing_s/update_actor:348.3869198928587 - timing_s/dump_rollout_generations:2.004035410936922 - timing_s/step:2422.6285864687525 - timing_s/stop_profile:8.341995999217033e-05 - timing_per_token_ms/ref:0.03409384370460053 - timing_per_token_ms/adv:1.1946624892110107e-05 - timing_per_token_ms/update_actor:0.11463791613508918 - timing_per_token_ms/gen:0.6193320462231168 - perf/total_num_tokens:3039020 - perf/time_per_step:2422.6285864687525 - perf/throughput:78.40192717153421
[36m(WorkerDict pid=2136258)[0m INFO 12-14 03:04:22 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136254)[0m INFO 12-14 03:04:22 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136265)[0m INFO 12-14 03:04:22 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136257)[0m INFO 12-14 03:04:22 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136269)[0m INFO 12-14 03:04:22 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136241)[0m INFO 12-14 03:04:22 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136253)[0m INFO 12-14 03:04:22 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136255)[0m INFO 12-14 03:04:22 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136256)[0m INFO 12-14 03:04:23 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136249)[0m INFO 12-14 03:04:23 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136261)[0m INFO 12-14 03:04:23 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136259)[0m INFO 12-14 03:04:23 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136244)[0m INFO 12-14 03:04:23 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136248)[0m INFO 12-14 03:04:23 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136260)[0m INFO 12-14 03:04:23 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136247)[0m INFO 12-14 03:04:25 [block_pool.py:378] Successfully reset prefix cache
[36m(TaskRunner pid=2108541)[0m Dumped generations to /workspace/data/dump/3.jsonl
[36m(TaskRunner pid=2108541)[0m step:3 - actor/entropy:0.25478729605674744 - actor/kl_loss:0.0009060278662662135 - actor/kl_coef:0.0010000000000000002 - actor/pg_loss:0.00042521501550587214 - actor/pg_clipfrac:0.0 - actor/ppo_kl:-9.794561227874433e-10 - actor/pg_clipfrac_lower:0.0 - actor/grad_norm:0.028513467556845706 - perf/mfu/actor:0.4095036498541251 - perf/max_memory_allocated_gb:47.892290115356445 - perf/max_memory_reserved_gb:48.0390625 - perf/cpu_memory_used_gb:1008.0940322875977 - actor/lr:1e-06 - training/global_step:3 - training/epoch:1 - critic/score/mean:0.90625 - critic/score/max:1.0 - critic/score/min:0.0 - critic/rewards/mean:0.90625 - critic/rewards/max:1.0 - critic/rewards/min:0.0 - critic/advantages/mean:-0.0005081532872281969 - critic/advantages/max:0.9682439565658569 - critic/advantages/min:-3.7499847412109375 - critic/returns/mean:-0.0005081532872281969 - critic/returns/max:0.9682439565658569 - critic/returns/min:-3.7499847412109375 - response_length/mean:5811.74609375 - response_length/max:34816.0 - response_length/min:959.0 - response_length/clip_ratio:0.009765625 - response_length_non_aborted/mean:5811.74609375 - response_length_non_aborted/max:34816.0 - response_length_non_aborted/min:959.0 - response_length_non_aborted/clip_ratio:0.009765625 - response/aborted_ratio:0.0 - prompt_length/mean:78.15625 - prompt_length/max:191.0 - prompt_length/min:34.0 - prompt_length/clip_ratio:0.0 - timing_s/start_profile:0.00040812138468027115 - timing_s/generate_sequences:2700.927001953125 - timing_s/generation_timing/max:2701.602783203125 - timing_s/generation_timing/min:2698.879638671875 - timing_s/generation_timing/topk_ratio:0.125 - timing_s/gen:2726.43145450484 - timing_s/reward:0.8860866539180279 - timing_s/old_log_prob:112.35853646183386 - timing_s/ref:106.64516251394525 - timing_s/adv:0.03719445876777172 - timing_s/update_actor:358.4740468952805 - timing_s/dump_rollout_generations:2.0426100962795317 - timing_s/step:3306.8829527590424 - timing_s/stop_profile:7.099984213709831e-05 - timing_per_token_ms/ref:0.035364140333510825 - timing_per_token_ms/adv:1.233389333829804e-05 - timing_per_token_ms/update_actor:0.11887202571113846 - timing_per_token_ms/gen:0.9162584443092552 - perf/total_num_tokens:3015630 - perf/time_per_step:3306.8829527590424 - perf/throughput:56.99532692644821
[36m(WorkerDict pid=2136258)[0m INFO 12-14 03:59:30 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136248)[0m INFO 12-14 03:59:30 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136265)[0m INFO 12-14 03:59:30 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136254)[0m INFO 12-14 03:59:30 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136257)[0m INFO 12-14 03:59:30 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136269)[0m INFO 12-14 03:59:30 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136241)[0m INFO 12-14 03:59:30 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136253)[0m INFO 12-14 03:59:30 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136255)[0m INFO 12-14 03:59:30 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136259)[0m INFO 12-14 03:59:31 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136249)[0m INFO 12-14 03:59:31 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136256)[0m INFO 12-14 03:59:31 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136260)[0m INFO 12-14 03:59:31 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136261)[0m INFO 12-14 03:59:31 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136244)[0m INFO 12-14 03:59:32 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136247)[0m INFO 12-14 03:59:33 [block_pool.py:378] Successfully reset prefix cache
[36m(TaskRunner pid=2108541)[0m Dumped generations to /workspace/data/dump/4.jsonl
[36m(TaskRunner pid=2108541)[0m step:4 - actor/entropy:0.27807241678237915 - actor/kl_loss:0.0009893006247627299 - actor/kl_coef:0.0010000000000000002 - actor/pg_loss:-0.002467105965834202 - actor/pg_clipfrac:0.0 - actor/ppo_kl:-8.284337662429184e-10 - actor/pg_clipfrac_lower:0.0 - actor/grad_norm:0.04893369943728103 - perf/mfu/actor:0.39588256925797055 - perf/max_memory_allocated_gb:47.892290115356445 - perf/max_memory_reserved_gb:48.0390625 - perf/cpu_memory_used_gb:1004.6591377258301 - actor/lr:1e-06 - training/global_step:4 - training/epoch:1 - critic/score/mean:0.677734375 - critic/score/max:1.0 - critic/score/min:0.0 - critic/rewards/mean:0.677734375 - critic/rewards/max:1.0 - critic/rewards/min:0.0 - critic/advantages/mean:0.0026555098593235016 - critic/advantages/max:3.7499847412109375 - critic/advantages/min:-3.7499847412109375 - critic/returns/mean:0.0026555098593235016 - critic/returns/max:3.7499847412109375 - critic/returns/min:-3.7499847412109375 - response_length/mean:5807.796875 - response_length/max:29088.0 - response_length/min:1061.0 - response_length/clip_ratio:0.0 - response_length_non_aborted/mean:5807.796875 - response_length_non_aborted/max:29088.0 - response_length_non_aborted/min:1061.0 - response_length_non_aborted/clip_ratio:0.0 - response/aborted_ratio:0.0 - prompt_length/mean:74.375 - prompt_length/max:176.0 - prompt_length/min:23.0 - prompt_length/clip_ratio:0.0 - timing_s/start_profile:6.248988211154938e-05 - timing_s/generate_sequences:2057.7802734375 - timing_s/generation_timing/max:2058.506103515625 - timing_s/generation_timing/min:2055.871337890625 - timing_s/generation_timing/topk_ratio:0.125 - timing_s/gen:2086.1758644469082 - timing_s/reward:0.8814678639173508 - timing_s/old_log_prob:109.97057169815525 - timing_s/ref:104.20657931501046 - timing_s/adv:0.04126518918201327 - timing_s/update_actor:347.76895052334294 - timing_s/dump_rollout_generations:1.9794727368280292 - timing_s/step:2651.0323416292667 - timing_s/stop_profile:8.377013728022575e-05 - timing_per_token_ms/ref:0.03460090584732018 - timing_per_token_ms/adv:1.3701754102708819e-05 - timing_per_token_ms/update_actor:0.11547371377870597 - timing_per_token_ms/gen:0.7015676207250047 - perf/total_num_tokens:3011672 - perf/time_per_step:2651.0323416292667 - perf/throughput:71.0023401239678
[36m(WorkerDict pid=2136258)[0m INFO 12-14 04:43:43 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136257)[0m INFO 12-14 04:43:43 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136241)[0m INFO 12-14 04:43:43 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136254)[0m INFO 12-14 04:43:43 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136265)[0m INFO 12-14 04:43:43 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136269)[0m INFO 12-14 04:43:43 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136253)[0m INFO 12-14 04:43:43 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136255)[0m INFO 12-14 04:43:43 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136248)[0m INFO 12-14 04:43:44 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136249)[0m INFO 12-14 04:43:44 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136247)[0m INFO 12-14 04:43:44 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136244)[0m INFO 12-14 04:43:44 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136256)[0m INFO 12-14 04:43:44 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136261)[0m INFO 12-14 04:43:44 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136260)[0m INFO 12-14 04:43:44 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136259)[0m INFO 12-14 04:43:44 [block_pool.py:378] Successfully reset prefix cache
[36m(TaskRunner pid=2108541)[0m Dumped generations to /workspace/data/dump/5.jsonl
[36m(TaskRunner pid=2108541)[0m step:5 - actor/entropy:0.25356152653694153 - actor/kl_loss:0.0009269602546456879 - actor/kl_coef:0.0010000000000000002 - actor/pg_loss:-0.0025668393613563645 - actor/pg_clipfrac:0.0 - actor/ppo_kl:-1.3664093868727258e-09 - actor/pg_clipfrac_lower:0.0 - actor/grad_norm:0.030142128258643413 - perf/mfu/actor:0.40390306852922575 - perf/max_memory_allocated_gb:47.892290115356445 - perf/max_memory_reserved_gb:48.0390625 - perf/cpu_memory_used_gb:1011.5143966674805 - actor/lr:1e-06 - training/global_step:5 - training/epoch:2 - critic/score/mean:0.904296875 - critic/score/max:1.0 - critic/score/min:0.0 - critic/rewards/mean:0.904296875 - critic/rewards/max:1.0 - critic/rewards/min:0.0 - critic/advantages/mean:0.0025767118204385042 - critic/advantages/max:0.7499985098838806 - critic/advantages/min:-3.7499847412109375 - critic/returns/mean:0.0025767118204385042 - critic/returns/max:0.7499985098838806 - critic/returns/min:-3.7499847412109375 - response_length/mean:5748.3984375 - response_length/max:34816.0 - response_length/min:856.0 - response_length/clip_ratio:0.00390625 - response_length_non_aborted/mean:5748.3984375 - response_length_non_aborted/max:34816.0 - response_length_non_aborted/min:856.0 - response_length_non_aborted/clip_ratio:0.00390625 - response/aborted_ratio:0.0 - prompt_length/mean:78.15625 - prompt_length/max:191.0 - prompt_length/min:34.0 - prompt_length/clip_ratio:0.0 - timing_s/start_profile:0.0004056519828736782 - timing_s/generate_sequences:2507.23486328125 - timing_s/generation_timing/max:2508.044677734375 - timing_s/generation_timing/min:2506.412109375 - timing_s/generation_timing/topk_ratio:0.125 - timing_s/gen:2537.4543183152564 - timing_s/reward:0.8806808888912201 - timing_s/old_log_prob:112.9399866652675 - timing_s/ref:107.43994249636307 - timing_s/adv:0.03813699493184686 - timing_s/update_actor:353.24276616983116 - timing_s/dump_rollout_generations:1.9812156637199223 - timing_s/step:3113.9847494270653 - timing_s/stop_profile:7.21607357263565e-05 - timing_per_token_ms/ref:0.03601504644561171 - timing_per_token_ms/adv:1.2783938746179218e-05 - timing_per_token_ms/update_actor:0.11841084734956442 - timing_per_token_ms/gen:0.8621471735725496 - perf/total_num_tokens:2983196 - perf/time_per_step:3113.9847494270653 - perf/throughput:59.87497210264259
[36m(WorkerDict pid=2136258)[0m INFO 12-14 05:35:38 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136257)[0m INFO 12-14 05:35:39 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136254)[0m INFO 12-14 05:35:39 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136269)[0m INFO 12-14 05:35:39 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136241)[0m INFO 12-14 05:35:39 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136255)[0m INFO 12-14 05:35:39 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136253)[0m INFO 12-14 05:35:39 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136248)[0m INFO 12-14 05:35:40 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136244)[0m INFO 12-14 05:35:40 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136265)[0m INFO 12-14 05:35:40 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136249)[0m INFO 12-14 05:35:40 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136256)[0m INFO 12-14 05:35:40 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136261)[0m INFO 12-14 05:35:40 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136259)[0m INFO 12-14 05:35:40 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136247)[0m INFO 12-14 05:35:40 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136260)[0m INFO 12-14 05:35:40 [block_pool.py:378] Successfully reset prefix cache
[36m(TaskRunner pid=2108541)[0m Dumped generations to /workspace/data/dump/6.jsonl
[36m(TaskRunner pid=2108541)[0m step:6 - actor/entropy:0.2781839370727539 - actor/kl_loss:0.001005900159858667 - actor/kl_coef:0.0010000000000000002 - actor/pg_loss:0.004364978306268047 - actor/pg_clipfrac:0.0 - actor/ppo_kl:-8.659924217823118e-10 - actor/pg_clipfrac_lower:0.0 - actor/grad_norm:0.042072982187580966 - perf/mfu/actor:0.39585054145321513 - perf/max_memory_allocated_gb:47.892290115356445 - perf/max_memory_reserved_gb:48.0390625 - perf/cpu_memory_used_gb:1011.2413444519043 - actor/lr:1e-06 - training/global_step:6 - training/epoch:2 - critic/score/mean:0.697265625 - critic/score/max:1.0 - critic/score/min:0.0 - critic/rewards/mean:0.697265625 - critic/rewards/max:1.0 - critic/rewards/min:0.0 - critic/advantages/mean:-0.0051915086805820465 - critic/advantages/max:1.2499974966049194 - critic/advantages/min:-3.7499847412109375 - critic/returns/mean:-0.0051915086805820465 - critic/returns/max:1.2499974966049194 - critic/returns/min:-3.7499847412109375 - response_length/mean:5792.162109375 - response_length/max:25228.0 - response_length/min:899.0 - response_length/clip_ratio:0.0 - response_length_non_aborted/mean:5792.162109375 - response_length_non_aborted/max:25228.0 - response_length_non_aborted/min:899.0 - response_length_non_aborted/clip_ratio:0.0 - response/aborted_ratio:0.0 - prompt_length/mean:74.375 - prompt_length/max:176.0 - prompt_length/min:23.0 - prompt_length/clip_ratio:0.0 - timing_s/start_profile:8.729100227355957e-05 - timing_s/generate_sequences:2122.55859375 - timing_s/generation_timing/max:2123.64599609375 - timing_s/generation_timing/min:2121.649169921875 - timing_s/generation_timing/topk_ratio:0.125 - timing_s/gen:2156.063805301208 - timing_s/reward:0.9675890672951937 - timing_s/old_log_prob:111.16717911325395 - timing_s/ref:104.94796313298866 - timing_s/adv:0.04660640098154545 - timing_s/update_actor:345.86931539000943 - timing_s/dump_rollout_generations:1.9701224309392273 - timing_s/step:2721.040465008933 - timing_s/stop_profile:8.765095844864845e-05 - timing_per_token_ms/ref:0.03493994611685938 - timing_per_token_ms/adv:1.551650065787767e-05 - timing_per_token_ms/update_actor:0.11514902130962235 - timing_per_token_ms/gen:0.7270276694972051 - perf/total_num_tokens:3003667 - perf/time_per_step:2721.040465008933 - perf/throughput:68.99169266833512
[36m(WorkerDict pid=2136248)[0m INFO 12-14 06:21:01 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136257)[0m INFO 12-14 06:21:01 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136258)[0m INFO 12-14 06:21:01 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136265)[0m INFO 12-14 06:21:01 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136241)[0m INFO 12-14 06:21:01 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136255)[0m INFO 12-14 06:21:01 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136269)[0m INFO 12-14 06:21:01 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136254)[0m INFO 12-14 06:21:01 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136253)[0m INFO 12-14 06:21:01 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136259)[0m INFO 12-14 06:21:01 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136249)[0m INFO 12-14 06:21:02 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136260)[0m INFO 12-14 06:21:02 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136247)[0m INFO 12-14 06:21:02 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136256)[0m INFO 12-14 06:21:02 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136244)[0m INFO 12-14 06:21:02 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136261)[0m INFO 12-14 06:21:03 [block_pool.py:378] Successfully reset prefix cache
[36m(TaskRunner pid=2108541)[0m old timing: {'generate_sequences': 2644.17236328125, 'generation_timing/max': 2645.279296875, 'generation_timing/min': 2642.486572265625, 'generation_timing/topk_ratio': 0.125}
[36m(TaskRunner pid=2108541)[0m new timing: {'generate_sequences': 2644.172607421875, 'generation_timing/max': 2645.279296875, 'generation_timing/min': 2642.486572265625, 'generation_timing/topk_ratio': 0.125}
[36m(TaskRunner pid=2108541)[0m Dumped generations to /workspace/data/dump/7.jsonl
[36m(TaskRunner pid=2108541)[0m step:7 - actor/entropy:0.2543092370033264 - actor/kl_loss:0.0009208911179890295 - actor/kl_coef:0.0010000000000000002 - actor/pg_loss:0.0003513953569962318 - actor/pg_clipfrac:0.0 - actor/ppo_kl:-2.0841402833792117e-09 - actor/pg_clipfrac_lower:0.0 - actor/grad_norm:0.026324134354074696 - perf/mfu/actor:0.4080916578236862 - perf/max_memory_allocated_gb:47.892290115356445 - perf/max_memory_reserved_gb:48.0390625 - perf/cpu_memory_used_gb:1017.4002914428711 - actor/lr:1e-06 - training/global_step:7 - training/epoch:3 - critic/score/mean:0.90625 - critic/score/max:1.0 - critic/score/min:0.0 - critic/rewards/mean:0.90625 - critic/rewards/max:1.0 - critic/rewards/min:0.0 - critic/advantages/mean:-0.0002084879670292139 - critic/advantages/max:0.8539109230041504 - critic/advantages/min:-3.7499847412109375 - critic/returns/mean:-0.0002084879670292139 - critic/returns/max:0.8539109230041504 - critic/returns/min:-3.7499847412109375 - response_length/mean:5896.34375 - response_length/max:34816.0 - response_length/min:1126.0 - response_length/clip_ratio:0.001953125 - response_length_non_aborted/mean:5896.34375 - response_length_non_aborted/max:34816.0 - response_length_non_aborted/min:1126.0 - response_length_non_aborted/clip_ratio:0.001953125 - response/aborted_ratio:0.0 - prompt_length/mean:78.15625 - prompt_length/max:191.0 - prompt_length/min:34.0 - prompt_length/clip_ratio:0.0 - timing_s/start_profile:0.0004105018451809883 - timing_s/generate_sequences:2644.17236328125 - timing_s/generation_timing/max:2645.279296875 - timing_s/generation_timing/min:2642.486572265625 - timing_s/generation_timing/topk_ratio:0.125 - timing_s/gen:2677.2587078609504 - timing_s/reward:0.8901625298894942 - timing_s/old_log_prob:116.08481106813997 - timing_s/ref:110.59665400907397 - timing_s/adv:0.03714068094268441 - timing_s/update_actor:364.46880395989865 - timing_s/dump_rollout_generations:1.9975240267813206 - timing_s/step:3271.3413754892536 - timing_s/stop_profile:7.051974534988403e-05 - timing_per_token_ms/ref:0.036155174468402815 - timing_per_token_ms/adv:1.214166749789614e-05 - timing_per_token_ms/update_actor:0.11914857021243234 - timing_per_token_ms/gen:0.8868242991753862 - perf/total_num_tokens:3058944 - perf/time_per_step:3271.3413754892536 - perf/throughput:58.442081719889906
[36m(WorkerDict pid=2136248)[0m INFO 12-14 07:15:33 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136257)[0m INFO 12-14 07:15:33 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136258)[0m INFO 12-14 07:15:33 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136241)[0m INFO 12-14 07:15:33 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136254)[0m INFO 12-14 07:15:33 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136253)[0m INFO 12-14 07:15:33 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136255)[0m INFO 12-14 07:15:33 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136269)[0m INFO 12-14 07:15:33 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136256)[0m INFO 12-14 07:15:34 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136265)[0m INFO 12-14 07:15:34 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136261)[0m INFO 12-14 07:15:34 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136249)[0m INFO 12-14 07:15:34 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136247)[0m INFO 12-14 07:15:34 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136259)[0m INFO 12-14 07:15:35 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136260)[0m INFO 12-14 07:15:35 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136244)[0m INFO 12-14 07:15:35 [block_pool.py:378] Successfully reset prefix cache
[36m(TaskRunner pid=2108541)[0m Dumped generations to /workspace/data/dump/8.jsonl
[36m(TaskRunner pid=2108541)[0m step:8 - actor/entropy:0.2781178951263428 - actor/kl_loss:0.0010163659545859055 - actor/kl_coef:0.0010000000000000002 - actor/pg_loss:-0.0018924152190309194 - actor/pg_clipfrac:0.0 - actor/ppo_kl:-4.716878829076158e-10 - actor/pg_clipfrac_lower:0.0 - actor/grad_norm:0.039749778112843345 - perf/mfu/actor:0.39744260317322655 - perf/max_memory_allocated_gb:47.892290115356445 - perf/max_memory_reserved_gb:48.0390625 - perf/cpu_memory_used_gb:1021.6685676574707 - actor/lr:1e-06 - training/global_step:8 - training/epoch:3 - critic/score/mean:0.6875 - critic/score/max:1.0 - critic/score/min:0.0 - critic/rewards/mean:0.6875 - critic/rewards/max:1.0 - critic/rewards/min:0.0 - critic/advantages/mean:0.0021445518359541893 - critic/advantages/max:3.7499847412109375 - critic/advantages/min:-2.561730146408081 - critic/returns/mean:0.0021445518359541893 - critic/returns/max:3.7499847412109375 - critic/returns/min:-2.561730146408081 - response_length/mean:5973.54296875 - response_length/max:28394.0 - response_length/min:984.0 - response_length/clip_ratio:0.0 - response_length_non_aborted/mean:5973.54296875 - response_length_non_aborted/max:28394.0 - response_length_non_aborted/min:984.0 - response_length_non_aborted/clip_ratio:0.0 - response/aborted_ratio:0.0 - prompt_length/mean:74.375 - prompt_length/max:176.0 - prompt_length/min:23.0 - prompt_length/clip_ratio:0.0 - timing_s/start_profile:6.34598545730114e-05 - timing_s/generate_sequences:2144.79296875 - timing_s/generation_timing/max:2145.6689453125 - timing_s/generation_timing/min:2143.621826171875 - timing_s/generation_timing/topk_ratio:0.125 - timing_s/gen:2178.6144221760333 - timing_s/reward:0.9424701589159667 - timing_s/old_log_prob:115.12782154371962 - timing_s/ref:107.82545731402934 - timing_s/adv:0.06535036629065871 - timing_s/update_actor:359.8420945168473 - timing_s/dump_rollout_generations:2.0489128376357257 - timing_s/step:2764.4743963903747 - timing_s/stop_profile:7.607974112033844e-05 - timing_per_token_ms/ref:0.03482133808769073 - timing_per_token_ms/adv:2.1104359354897672e-05 - timing_per_token_ms/update_actor:0.1162080230725215 - timing_per_token_ms/gen:0.7123253847126795 - perf/total_num_tokens:3096534 - perf/time_per_step:2764.4743963903747 - perf/throughput:70.00729514901643
[36m(WorkerDict pid=2136257)[0m INFO 12-14 08:01:39 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136258)[0m INFO 12-14 08:01:39 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136241)[0m INFO 12-14 08:01:39 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136253)[0m INFO 12-14 08:01:39 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136265)[0m INFO 12-14 08:01:39 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136269)[0m INFO 12-14 08:01:39 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136254)[0m INFO 12-14 08:01:39 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136255)[0m INFO 12-14 08:01:39 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136248)[0m INFO 12-14 08:01:39 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136247)[0m INFO 12-14 08:01:40 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136249)[0m INFO 12-14 08:01:40 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136244)[0m INFO 12-14 08:01:40 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136256)[0m INFO 12-14 08:01:40 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136259)[0m INFO 12-14 08:01:40 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136260)[0m INFO 12-14 08:01:41 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136261)[0m INFO 12-14 08:01:41 [block_pool.py:378] Successfully reset prefix cache
[36m(TaskRunner pid=2108541)[0m Dumped generations to /workspace/data/dump/9.jsonl
[36m(TaskRunner pid=2108541)[0m step:9 - actor/entropy:0.25510019063949585 - actor/kl_loss:0.0009489572581110729 - actor/kl_coef:0.0010000000000000002 - actor/pg_loss:-0.00011356232434879115 - actor/pg_clipfrac:0.0 - actor/ppo_kl:-6.24771526780558e-10 - actor/pg_clipfrac_lower:0.0 - actor/grad_norm:0.01808474148360134 - perf/mfu/actor:0.4046998147882122 - perf/max_memory_allocated_gb:47.89410972595215 - perf/max_memory_reserved_gb:48.05859375 - perf/cpu_memory_used_gb:1019.3204078674316 - actor/lr:1e-06 - training/global_step:9 - training/epoch:4 - critic/score/mean:0.904296875 - critic/score/max:1.0 - critic/score/min:0.0 - critic/rewards/mean:0.904296875 - critic/rewards/max:1.0 - critic/rewards/min:0.0 - critic/advantages/mean:0.00010663633293006569 - critic/advantages/max:2.561730146408081 - critic/advantages/min:-2.015559434890747 - critic/returns/mean:0.00010663633293006569 - critic/returns/max:2.561730146408081 - critic/returns/min:-2.015559434890747 - response_length/mean:5718.65234375 - response_length/max:34816.0 - response_length/min:996.0 - response_length/clip_ratio:0.00390625 - response_length_non_aborted/mean:5718.65234375 - response_length_non_aborted/max:34816.0 - response_length_non_aborted/min:996.0 - response_length_non_aborted/clip_ratio:0.00390625 - response/aborted_ratio:0.0 - prompt_length/mean:78.15625 - prompt_length/max:191.0 - prompt_length/min:34.0 - prompt_length/clip_ratio:0.0 - timing_s/start_profile:0.00040253205224871635 - timing_s/generate_sequences:2600.060546875 - timing_s/generation_timing/max:2601.451904296875 - timing_s/generation_timing/min:2598.61181640625 - timing_s/generation_timing/topk_ratio:0.125 - timing_s/gen:2635.637943083886 - timing_s/reward:0.8850590381771326 - timing_s/old_log_prob:112.9427116829902 - timing_s/ref:107.05421425728127 - timing_s/adv:0.04238671297207475 - timing_s/update_actor:350.87202314380556 - timing_s/dump_rollout_generations:2.0056040878407657 - timing_s/step:3209.4478084119037 - timing_s/stop_profile:8.005974814295769e-05 - timing_per_token_ms/ref:0.03606989239677317 - timing_per_token_ms/adv:1.4281401125240231e-05 - timing_per_token_ms/update_actor:0.11821969090744489 - timing_per_token_ms/gen:0.9001649423944692 - perf/total_num_tokens:2967966 - perf/time_per_step:3209.4478084119037 - perf/throughput:57.797442449075966
[36m(WorkerDict pid=2136258)[0m INFO 12-14 08:55:08 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136248)[0m INFO 12-14 08:55:08 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136257)[0m INFO 12-14 08:55:08 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136254)[0m INFO 12-14 08:55:08 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136269)[0m INFO 12-14 08:55:09 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136253)[0m INFO 12-14 08:55:09 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136255)[0m INFO 12-14 08:55:09 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136241)[0m INFO 12-14 08:55:09 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136256)[0m INFO 12-14 08:55:09 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136265)[0m INFO 12-14 08:55:09 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136244)[0m INFO 12-14 08:55:09 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136249)[0m INFO 12-14 08:55:09 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136247)[0m INFO 12-14 08:55:10 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136260)[0m INFO 12-14 08:55:10 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136259)[0m INFO 12-14 08:55:10 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136261)[0m INFO 12-14 08:55:10 [block_pool.py:378] Successfully reset prefix cache
[36m(TaskRunner pid=2108541)[0m Dumped generations to /workspace/data/dump/10.jsonl
[36m(TaskRunner pid=2108541)[0m step:10 - actor/entropy:0.27806296944618225 - actor/kl_loss:0.0010364206831493393 - actor/kl_coef:0.0010000000000000002 - actor/pg_loss:-0.0065547775210267525 - actor/pg_clipfrac:0.0 - actor/ppo_kl:-3.3975900023625185e-10 - actor/pg_clipfrac_lower:0.0 - actor/grad_norm:0.042300711330000594 - perf/mfu/actor:0.3960381935187848 - perf/max_memory_allocated_gb:47.89410972595215 - perf/max_memory_reserved_gb:48.05859375 - perf/cpu_memory_used_gb:1021.6851234436035 - actor/lr:1e-06 - training/global_step:10 - training/epoch:4 - critic/score/mean:0.701171875 - critic/score/max:1.0 - critic/score/min:0.0 - critic/rewards/mean:0.701171875 - critic/rewards/max:1.0 - critic/rewards/min:0.0 - critic/advantages/mean:0.006744118873029947 - critic/advantages/max:2.561730146408081 - critic/advantages/min:-3.7499847412109375 - critic/returns/mean:0.006744118873029947 - critic/returns/max:2.561730146408081 - critic/returns/min:-3.7499847412109375 - response_length/mean:5829.064453125 - response_length/max:26768.0 - response_length/min:939.0 - response_length/clip_ratio:0.0 - response_length_non_aborted/mean:5829.064453125 - response_length_non_aborted/max:26768.0 - response_length_non_aborted/min:939.0 - response_length_non_aborted/clip_ratio:0.0 - response/aborted_ratio:0.0 - prompt_length/mean:74.375 - prompt_length/max:176.0 - prompt_length/min:23.0 - prompt_length/clip_ratio:0.0 - timing_s/start_profile:9.877979755401611e-05 - timing_s/generate_sequences:2302.688232421875 - timing_s/generation_timing/max:2303.92626953125 - timing_s/generation_timing/min:2301.554931640625 - timing_s/generation_timing/topk_ratio:0.125 - timing_s/gen:2338.559044233989 - timing_s/reward:0.9074823320843279 - timing_s/old_log_prob:113.41221518162638 - timing_s/ref:107.5536854560487 - timing_s/adv:0.0686406809836626 - timing_s/update_actor:347.93873264081776 - timing_s/dump_rollout_generations:1.9895358900539577 - timing_s/step:2910.437571571674 - timing_s/stop_profile:8.291983976960182e-05 - timing_per_token_ms/ref:0.03558362774350913 - timing_per_token_ms/adv:2.270944440283012e-05 - timing_per_token_ms/update_actor:0.11511388277716074 - timing_per_token_ms/gen:0.7835731050839289 - perf/total_num_tokens:3022561 - perf/time_per_step:2910.437571571674 - perf/throughput:64.90778718128838
[36m(WorkerDict pid=2136257)[0m INFO 12-14 09:43:39 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136248)[0m INFO 12-14 09:43:39 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136241)[0m INFO 12-14 09:43:39 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136254)[0m INFO 12-14 09:43:39 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136269)[0m INFO 12-14 09:43:39 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136253)[0m INFO 12-14 09:43:39 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136255)[0m INFO 12-14 09:43:39 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136258)[0m INFO 12-14 09:43:39 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136265)[0m INFO 12-14 09:43:39 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136244)[0m INFO 12-14 09:43:40 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136256)[0m INFO 12-14 09:43:40 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136249)[0m INFO 12-14 09:43:40 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136247)[0m INFO 12-14 09:43:41 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136260)[0m INFO 12-14 09:43:41 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136259)[0m INFO 12-14 09:43:41 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136261)[0m INFO 12-14 09:43:42 [block_pool.py:378] Successfully reset prefix cache
[36m(TaskRunner pid=2108541)[0m Dumped generations to /workspace/data/dump/11.jsonl
[36m(TaskRunner pid=2108541)[0m step:11 - actor/entropy:0.2565997540950775 - actor/kl_loss:0.0009571807203450944 - actor/kl_coef:0.0010000000000000002 - actor/pg_loss:0.0008067362373921929 - actor/pg_clipfrac:0.0 - actor/ppo_kl:-1.4646109027839112e-10 - actor/pg_clipfrac_lower:0.0 - actor/grad_norm:0.01721119625542709 - perf/mfu/actor:0.4054689510664545 - perf/max_memory_allocated_gb:47.89426231384277 - perf/max_memory_reserved_gb:48.05859375 - perf/cpu_memory_used_gb:1027.6041450500488 - actor/lr:1e-06 - training/global_step:11 - training/epoch:5 - critic/score/mean:0.919921875 - critic/score/max:1.0 - critic/score/min:0.0 - critic/rewards/mean:0.919921875 - critic/rewards/max:1.0 - critic/rewards/min:0.0 - critic/advantages/mean:-0.0008208349463529885 - critic/advantages/max:0.8539109230041504 - critic/advantages/min:-2.561730146408081 - critic/returns/mean:-0.0008208349463529885 - critic/returns/max:0.8539109230041504 - critic/returns/min:-2.561730146408081 - response_length/mean:5782.544921875 - response_length/max:34816.0 - response_length/min:755.0 - response_length/clip_ratio:0.00390625 - response_length_non_aborted/mean:5782.544921875 - response_length_non_aborted/max:34816.0 - response_length_non_aborted/min:755.0 - response_length_non_aborted/clip_ratio:0.00390625 - response/aborted_ratio:0.0 - prompt_length/mean:78.15625 - prompt_length/max:191.0 - prompt_length/min:34.0 - prompt_length/clip_ratio:0.0 - timing_s/start_profile:0.000401162076741457 - timing_s/generate_sequences:2669.557373046875 - timing_s/generation_timing/max:2670.50390625 - timing_s/generation_timing/min:2667.97607421875 - timing_s/generation_timing/topk_ratio:0.125 - timing_s/gen:2704.463754595723 - timing_s/reward:0.9004714437760413 - timing_s/old_log_prob:119.74769347114488 - timing_s/ref:109.52935016714036 - timing_s/adv:0.04813721589744091 - timing_s/update_actor:358.37245898414403 - timing_s/dump_rollout_generations:1.9981782040558755 - timing_s/step:3295.067872794345 - timing_s/stop_profile:8.528027683496475e-05 - timing_per_token_ms/ref:0.03650152187792842 - timing_per_token_ms/adv:1.6042107768755308e-05 - timing_per_token_ms/update_actor:0.11943045523501315 - timing_per_token_ms/gen:0.913465583416864 - perf/total_num_tokens:3000679 - perf/time_per_step:3295.067872794345 - perf/throughput:56.91610757048132
[36m(WorkerDict pid=2136258)[0m INFO 12-14 10:38:35 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136248)[0m INFO 12-14 10:38:35 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136257)[0m INFO 12-14 10:38:35 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136241)[0m INFO 12-14 10:38:35 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136255)[0m INFO 12-14 10:38:36 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136254)[0m INFO 12-14 10:38:36 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136265)[0m INFO 12-14 10:38:36 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136269)[0m INFO 12-14 10:38:36 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136253)[0m INFO 12-14 10:38:36 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136244)[0m INFO 12-14 10:38:37 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136247)[0m INFO 12-14 10:38:37 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136256)[0m INFO 12-14 10:38:37 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136260)[0m INFO 12-14 10:38:37 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136249)[0m INFO 12-14 10:38:37 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136259)[0m INFO 12-14 10:38:37 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136261)[0m INFO 12-14 10:38:37 [block_pool.py:378] Successfully reset prefix cache
[36m(TaskRunner pid=2108541)[0m Dumped generations to /workspace/data/dump/12.jsonl
[36m(TaskRunner pid=2108541)[0m step:12 - actor/entropy:0.2774141728878021 - actor/kl_loss:0.0010457753676569044 - actor/kl_coef:0.0010000000000000002 - actor/pg_loss:-0.0032584360550816467 - actor/pg_clipfrac:0.0 - actor/ppo_kl:-5.925578097304564e-10 - actor/pg_clipfrac_lower:0.0 - actor/grad_norm:0.04387303342276853 - perf/mfu/actor:0.3967071616452271 - perf/max_memory_allocated_gb:47.89426231384277 - perf/max_memory_reserved_gb:48.05859375 - perf/cpu_memory_used_gb:1028.4837608337402 - actor/lr:1e-06 - training/global_step:12 - training/epoch:5 - critic/score/mean:0.697265625 - critic/score/max:1.0 - critic/score/min:0.0 - critic/rewards/mean:0.697265625 - critic/rewards/max:1.0 - critic/rewards/min:0.0 - critic/advantages/mean:0.004025454167276621 - critic/advantages/max:3.7499847412109375 - critic/advantages/min:-3.7499847412109375 - critic/returns/mean:0.004025454167276621 - critic/returns/max:3.7499847412109375 - critic/returns/min:-3.7499847412109375 - response_length/mean:5820.841796875 - response_length/max:28429.0 - response_length/min:950.0 - response_length/clip_ratio:0.0 - response_length_non_aborted/mean:5820.841796875 - response_length_non_aborted/max:28429.0 - response_length_non_aborted/min:950.0 - response_length_non_aborted/clip_ratio:0.0 - response/aborted_ratio:0.0 - prompt_length/mean:74.375 - prompt_length/max:176.0 - prompt_length/min:23.0 - prompt_length/clip_ratio:0.0 - timing_s/start_profile:0.00010932097211480141 - timing_s/generate_sequences:2069.789794921875 - timing_s/generation_timing/max:2070.771728515625 - timing_s/generation_timing/min:2069.052734375 - timing_s/generation_timing/topk_ratio:0.125 - timing_s/gen:2110.205819163006 - timing_s/reward:0.8948059407994151 - timing_s/old_log_prob:116.45633679861203 - timing_s/ref:110.15350656816736 - timing_s/adv:0.03857121942564845 - timing_s/update_actor:351.3535585156642 - timing_s/dump_rollout_generations:1.9715045811608434 - timing_s/step:2691.0826068739407 - timing_s/stop_profile:8.54399986565113e-05 - timing_per_token_ms/ref:0.03649459806635059 - timing_per_token_ms/adv:1.2778904582551351e-05 - timing_per_token_ms/update_actor:0.11640579856870993 - timing_per_token_ms/gen:0.708058367565569 - perf/total_num_tokens:3018351 - perf/time_per_step:2691.0826068739407 - perf/throughput:70.10076057053452
[36m(WorkerDict pid=2136258)[0m INFO 12-14 11:23:29 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136257)[0m INFO 12-14 11:23:29 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136241)[0m INFO 12-14 11:23:29 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136248)[0m INFO 12-14 11:23:29 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136254)[0m INFO 12-14 11:23:29 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136265)[0m INFO 12-14 11:23:29 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136269)[0m INFO 12-14 11:23:29 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136253)[0m INFO 12-14 11:23:29 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136255)[0m INFO 12-14 11:23:29 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136247)[0m INFO 12-14 11:23:30 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136259)[0m INFO 12-14 11:23:31 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136261)[0m INFO 12-14 11:23:31 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136256)[0m INFO 12-14 11:23:31 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136244)[0m INFO 12-14 11:23:31 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136249)[0m INFO 12-14 11:23:31 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136260)[0m INFO 12-14 11:23:32 [block_pool.py:378] Successfully reset prefix cache
[36m(TaskRunner pid=2108541)[0m Dumped generations to /workspace/data/dump/13.jsonl
[36m(TaskRunner pid=2108541)[0m step:13 - actor/entropy:0.2554154396057129 - actor/kl_loss:0.0009805531050760027 - actor/kl_coef:0.0010000000000000002 - actor/pg_loss:-0.0014587930977208222 - actor/pg_clipfrac:0.0 - actor/ppo_kl:-5.280371407530112e-10 - actor/pg_clipfrac_lower:0.0 - actor/grad_norm:0.02582254773252674 - perf/mfu/actor:0.4067810819858659 - perf/max_memory_allocated_gb:47.89426231384277 - perf/max_memory_reserved_gb:48.05859375 - perf/cpu_memory_used_gb:1032.8742561340332 - actor/lr:1e-06 - training/global_step:13 - training/epoch:6 - critic/score/mean:0.9140625 - critic/score/max:1.0 - critic/score/min:0.0 - critic/rewards/mean:0.9140625 - critic/rewards/max:1.0 - critic/rewards/min:0.0 - critic/advantages/mean:0.0014472742332145572 - critic/advantages/max:0.7499985098838806 - critic/advantages/min:-3.7499847412109375 - critic/returns/mean:0.0014472742332145572 - critic/returns/max:0.7499985098838806 - critic/returns/min:-3.7499847412109375 - response_length/mean:5801.88671875 - response_length/max:34816.0 - response_length/min:939.0 - response_length/clip_ratio:0.00390625 - response_length_non_aborted/mean:5801.88671875 - response_length_non_aborted/max:34816.0 - response_length_non_aborted/min:939.0 - response_length_non_aborted/clip_ratio:0.00390625 - response/aborted_ratio:0.0 - prompt_length/mean:78.15625 - prompt_length/max:191.0 - prompt_length/min:34.0 - prompt_length/clip_ratio:0.0 - timing_s/start_profile:0.00040732091292738914 - timing_s/generate_sequences:2629.309814453125 - timing_s/generation_timing/max:2630.444091796875 - timing_s/generation_timing/min:2627.605712890625 - timing_s/generation_timing/topk_ratio:0.125 - timing_s/gen:2670.985949689988 - timing_s/reward:0.8914342639036477 - timing_s/old_log_prob:118.17907531699166 - timing_s/ref:111.04174014320597 - timing_s/adv:0.042556576896458864 - timing_s/update_actor:360.3901938321069 - timing_s/dump_rollout_generations:2.0020043798722327 - timing_s/step:3263.540741374716 - timing_s/stop_profile:7.886020466685295e-05 - timing_per_token_ms/ref:0.03688381188195703 - timing_per_token_ms/adv:1.4135664431813804e-05 - timing_per_token_ms/update_actor:0.11970781524373257 - timing_per_token_ms/gen:0.8991505153192988 - perf/total_num_tokens:3010582 - perf/time_per_step:3263.540741374716 - perf/throughput:57.655592471856174
[36m(WorkerDict pid=2136258)[0m INFO 12-14 12:17:54 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136257)[0m INFO 12-14 12:17:54 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136254)[0m INFO 12-14 12:17:54 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136255)[0m INFO 12-14 12:17:54 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136269)[0m INFO 12-14 12:17:54 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136241)[0m INFO 12-14 12:17:54 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136248)[0m INFO 12-14 12:17:54 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136253)[0m INFO 12-14 12:17:54 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136265)[0m INFO 12-14 12:17:55 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136249)[0m INFO 12-14 12:17:55 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136256)[0m INFO 12-14 12:17:55 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136261)[0m INFO 12-14 12:17:55 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136247)[0m INFO 12-14 12:17:55 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136259)[0m INFO 12-14 12:17:55 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136244)[0m INFO 12-14 12:17:56 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136260)[0m INFO 12-14 12:17:56 [block_pool.py:378] Successfully reset prefix cache
[36m(TaskRunner pid=2108541)[0m Dumped generations to /workspace/data/dump/14.jsonl
[36m(TaskRunner pid=2108541)[0m step:14 - actor/entropy:0.27975523471832275 - actor/kl_loss:0.0010901113827600627 - actor/kl_coef:0.0010000000000000002 - actor/pg_loss:0.001455492586884872 - actor/pg_clipfrac:0.0 - actor/ppo_kl:-2.0004689931013764e-10 - actor/pg_clipfrac_lower:0.0 - actor/grad_norm:0.038858809478710996 - perf/mfu/actor:0.39591615005500624 - perf/max_memory_allocated_gb:47.89426231384277 - perf/max_memory_reserved_gb:48.05859375 - perf/cpu_memory_used_gb:1034.1569213867188 - actor/lr:1e-06 - training/global_step:14 - training/epoch:6 - critic/score/mean:0.681640625 - critic/score/max:1.0 - critic/score/min:0.0 - critic/rewards/mean:0.681640625 - critic/rewards/max:1.0 - critic/rewards/min:0.0 - critic/advantages/mean:-0.0007757828570902348 - critic/advantages/max:3.7499847412109375 - critic/advantages/min:-1.6770472526550293 - critic/returns/mean:-0.0007757828570902348 - critic/returns/max:3.7499847412109375 - critic/returns/min:-1.6770472526550293 - response_length/mean:5889.984375 - response_length/max:34816.0 - response_length/min:958.0 - response_length/clip_ratio:0.001953125 - response_length_non_aborted/mean:5889.984375 - response_length_non_aborted/max:34816.0 - response_length_non_aborted/min:958.0 - response_length_non_aborted/clip_ratio:0.001953125 - response/aborted_ratio:0.0 - prompt_length/mean:74.375 - prompt_length/max:176.0 - prompt_length/min:23.0 - prompt_length/clip_ratio:0.0 - timing_s/start_profile:8.61799344420433e-05 - timing_s/generate_sequences:2601.001220703125 - timing_s/generation_timing/max:2602.32177734375 - timing_s/generation_timing/min:2599.96240234375 - timing_s/generation_timing/topk_ratio:0.125 - timing_s/gen:2646.506739712786 - timing_s/reward:0.912174048833549 - timing_s/old_log_prob:118.53132923413068 - timing_s/ref:111.78980660485104 - timing_s/adv:0.036792116705328226 - timing_s/update_actor:356.96647195192054 - timing_s/dump_rollout_generations:1.9677235740236938 - timing_s/step:3236.718132283073 - timing_s/stop_profile:7.696030661463737e-05 - timing_per_token_ms/ref:0.03660736255100317 - timing_per_token_ms/adv:1.2048167862134261e-05 - timing_per_token_ms/update_actor:0.11689438826463987 - timing_per_token_ms/gen:0.8775844122679077 - perf/total_num_tokens:3053752 - perf/time_per_step:3236.718132283073 - perf/throughput:58.96698204776147
[36m(WorkerDict pid=2136248)[0m INFO 12-14 13:11:52 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136257)[0m INFO 12-14 13:11:52 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136254)[0m INFO 12-14 13:11:52 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136255)[0m INFO 12-14 13:11:52 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136269)[0m INFO 12-14 13:11:52 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136253)[0m INFO 12-14 13:11:52 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136249)[0m INFO 12-14 13:11:53 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136256)[0m INFO 12-14 13:11:53 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136258)[0m INFO 12-14 13:11:53 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136241)[0m INFO 12-14 13:11:53 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136265)[0m INFO 12-14 13:11:53 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136247)[0m INFO 12-14 13:11:54 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136260)[0m INFO 12-14 13:11:54 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136259)[0m INFO 12-14 13:11:54 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136261)[0m INFO 12-14 13:11:54 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136244)[0m INFO 12-14 13:11:56 [block_pool.py:378] Successfully reset prefix cache
[36m(TaskRunner pid=2108541)[0m Dumped generations to /workspace/data/dump/15.jsonl
[36m(TaskRunner pid=2108541)[0m step:15 - actor/entropy:0.25595542788505554 - actor/kl_loss:0.0010162631437330076 - actor/kl_coef:0.0010000000000000002 - actor/pg_loss:2.865049215715106e-06 - actor/pg_clipfrac:0.0 - actor/ppo_kl:-1.6634439917139767e-10 - actor/pg_clipfrac_lower:0.0 - actor/grad_norm:0.021973921818843965 - perf/mfu/actor:0.4018282311706857 - perf/max_memory_allocated_gb:47.89426231384277 - perf/max_memory_reserved_gb:48.05859375 - perf/cpu_memory_used_gb:1036.0741920471191 - actor/lr:1e-06 - training/global_step:15 - training/epoch:7 - critic/score/mean:0.919921875 - critic/score/max:1.0 - critic/score/min:0.0 - critic/rewards/mean:0.919921875 - critic/rewards/max:1.0 - critic/rewards/min:0.0 - critic/advantages/mean:-8.305780647788197e-05 - critic/advantages/max:0.652789831161499 - critic/advantages/min:-3.7499847412109375 - critic/returns/mean:-8.305780647788197e-05 - critic/returns/max:0.652789831161499 - critic/returns/min:-3.7499847412109375 - response_length/mean:5754.185546875 - response_length/max:34816.0 - response_length/min:910.0 - response_length/clip_ratio:0.00390625 - response_length_non_aborted/mean:5754.185546875 - response_length_non_aborted/max:34816.0 - response_length_non_aborted/min:910.0 - response_length_non_aborted/clip_ratio:0.00390625 - response/aborted_ratio:0.0 - prompt_length/mean:78.15625 - prompt_length/max:191.0 - prompt_length/min:34.0 - prompt_length/clip_ratio:0.0 - timing_s/start_profile:0.00040073180571198463 - timing_s/generate_sequences:2643.981201171875 - timing_s/generation_timing/max:2645.15380859375 - timing_s/generation_timing/min:2640.626708984375 - timing_s/generation_timing/topk_ratio:0.125 - timing_s/gen:2687.9433568380773 - timing_s/reward:0.886625069193542 - timing_s/old_log_prob:117.00360371405259 - timing_s/ref:111.21060361620039 - timing_s/adv:0.04506293497979641 - timing_s/update_actor:357.93100003059953 - timing_s/dump_rollout_generations:1.9738102639093995 - timing_s/step:3277.00200701179 - timing_s/stop_profile:8.424092084169388e-05 - timing_per_token_ms/ref:0.03724202348776485 - timing_per_token_ms/adv:1.5090601330939313e-05 - timing_per_token_ms/update_actor:0.11986334285300934 - timing_per_token_ms/gen:0.9123601117929704 - perf/total_num_tokens:2986159 - perf/time_per_step:3277.00200701179 - perf/throughput:56.952951844599994
[36m(WorkerDict pid=2136258)[0m INFO 12-14 14:06:31 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136241)[0m INFO 12-14 14:06:32 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136257)[0m INFO 12-14 14:06:32 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136248)[0m INFO 12-14 14:06:32 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136254)[0m INFO 12-14 14:06:32 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136269)[0m INFO 12-14 14:06:32 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136265)[0m INFO 12-14 14:06:32 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136255)[0m INFO 12-14 14:06:32 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136253)[0m INFO 12-14 14:06:33 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136256)[0m INFO 12-14 14:06:33 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136249)[0m INFO 12-14 14:06:33 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136247)[0m INFO 12-14 14:06:33 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136260)[0m INFO 12-14 14:06:33 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136259)[0m INFO 12-14 14:06:34 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136261)[0m INFO 12-14 14:06:35 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136244)[0m INFO 12-14 14:06:36 [block_pool.py:378] Successfully reset prefix cache
[36m(TaskRunner pid=2108541)[0m Dumped generations to /workspace/data/dump/16.jsonl
[36m(TaskRunner pid=2108541)[0m step:16 - actor/entropy:0.27676960825920105 - actor/kl_loss:0.001102569750061326 - actor/kl_coef:0.0010000000000000002 - actor/pg_loss:-6.185698670795164e-05 - actor/pg_clipfrac:0.0 - actor/ppo_kl:-6.453132774627944e-10 - actor/pg_clipfrac_lower:0.0 - actor/grad_norm:0.03825768084588124 - perf/mfu/actor:0.3968559725303294 - perf/max_memory_allocated_gb:47.89426231384277 - perf/max_memory_reserved_gb:48.05859375 - perf/cpu_memory_used_gb:1037.2926979064941 - actor/lr:1e-06 - training/global_step:16 - training/epoch:7 - critic/score/mean:0.69921875 - critic/score/max:1.0 - critic/score/min:0.0 - critic/rewards/mean:0.69921875 - critic/rewards/max:1.0 - critic/rewards/min:0.0 - critic/advantages/mean:-0.0004469951963983476 - critic/advantages/max:3.7499847412109375 - critic/advantages/min:-1.0978854894638062 - critic/returns/mean:-0.0004469951963983476 - critic/returns/max:3.7499847412109375 - critic/returns/min:-1.0978854894638062 - response_length/mean:5895.716796875 - response_length/max:28895.0 - response_length/min:1029.0 - response_length/clip_ratio:0.0 - response_length_non_aborted/mean:5895.716796875 - response_length_non_aborted/max:28895.0 - response_length_non_aborted/min:1029.0 - response_length_non_aborted/clip_ratio:0.0 - response/aborted_ratio:0.0 - prompt_length/mean:74.375 - prompt_length/max:176.0 - prompt_length/min:23.0 - prompt_length/clip_ratio:0.0 - timing_s/start_profile:0.00010098004713654518 - timing_s/generate_sequences:2146.415771484375 - timing_s/generation_timing/max:2147.800048828125 - timing_s/generation_timing/min:2143.461181640625 - timing_s/generation_timing/topk_ratio:0.125 - timing_s/gen:2200.5507436441258 - timing_s/reward:0.931512557901442 - timing_s/old_log_prob:122.75280870171264 - timing_s/ref:115.20484905783087 - timing_s/adv:0.037053057923913 - timing_s/update_actor:363.32263367110863 - timing_s/dump_rollout_generations:1.9843456950038671 - timing_s/step:2804.79211336188 - timing_s/stop_profile:9.301025420427322e-05 - timing_per_token_ms/ref:0.03768944908583406 - timing_per_token_ms/adv:1.212196666649644e-05 - timing_per_token_ms/update_actor:0.11886157584047978 - timing_per_token_ms/gen:0.7289954418193975 - perf/total_num_tokens:3056687 - perf/time_per_step:2804.79211336188 - perf/throughput:68.11304716306125
[36m(WorkerDict pid=2136258)[0m INFO 12-14 14:53:14 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136257)[0m INFO 12-14 14:53:14 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136241)[0m INFO 12-14 14:53:14 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136253)[0m INFO 12-14 14:53:14 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136254)[0m INFO 12-14 14:53:14 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136255)[0m INFO 12-14 14:53:14 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136265)[0m INFO 12-14 14:53:14 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136269)[0m INFO 12-14 14:53:14 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136249)[0m INFO 12-14 14:53:15 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136247)[0m INFO 12-14 14:53:16 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136256)[0m INFO 12-14 14:53:16 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136260)[0m INFO 12-14 14:53:16 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136259)[0m INFO 12-14 14:53:16 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136244)[0m INFO 12-14 14:53:16 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136248)[0m INFO 12-14 14:53:17 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136261)[0m INFO 12-14 14:53:17 [block_pool.py:378] Successfully reset prefix cache
[36m(TaskRunner pid=2108541)[0m old timing: {'generate_sequences': 2620.092041015625, 'generation_timing/max': 2621.58544921875, 'generation_timing/min': 2618.1982421875, 'generation_timing/topk_ratio': 0.125}
[36m(TaskRunner pid=2108541)[0m new timing: {'generate_sequences': 2620.09228515625, 'generation_timing/max': 2621.58544921875, 'generation_timing/min': 2618.1982421875, 'generation_timing/topk_ratio': 0.125}
[36m(TaskRunner pid=2108541)[0m Dumped generations to /workspace/data/dump/17.jsonl
[36m(TaskRunner pid=2108541)[0m step:17 - actor/entropy:0.2539463937282562 - actor/kl_loss:0.0010301970511890676 - actor/kl_coef:0.0010000000000000002 - actor/pg_loss:-6.792753389695795e-05 - actor/pg_clipfrac:0.0 - actor/ppo_kl:-4.908603892591704e-10 - actor/pg_clipfrac_lower:0.0 - actor/grad_norm:0.02533253687065009 - perf/mfu/actor:0.4044237269118377 - perf/max_memory_allocated_gb:47.89426231384277 - perf/max_memory_reserved_gb:48.05859375 - perf/cpu_memory_used_gb:1042.0951881408691 - actor/lr:1e-06 - training/global_step:17 - training/epoch:8 - critic/score/mean:0.919921875 - critic/score/max:1.0 - critic/score/min:0.0 - critic/rewards/mean:0.919921875 - critic/rewards/max:1.0 - critic/rewards/min:0.0 - critic/advantages/mean:4.1063103708438575e-05 - critic/advantages/max:0.652789831161499 - critic/advantages/min:-3.7499847412109375 - critic/returns/mean:4.1063103708438575e-05 - critic/returns/max:0.652789831161499 - critic/returns/min:-3.7499847412109375 - response_length/mean:5793.431640625 - response_length/max:34816.0 - response_length/min:888.0 - response_length/clip_ratio:0.005859375 - response_length_non_aborted/mean:5793.431640625 - response_length_non_aborted/max:34816.0 - response_length_non_aborted/min:888.0 - response_length_non_aborted/clip_ratio:0.005859375 - response/aborted_ratio:0.0 - prompt_length/mean:78.15625 - prompt_length/max:191.0 - prompt_length/min:34.0 - prompt_length/clip_ratio:0.0 - timing_s/start_profile:0.00038854219019412994 - timing_s/generate_sequences:2620.092041015625 - timing_s/generation_timing/max:2621.58544921875 - timing_s/generation_timing/min:2618.1982421875 - timing_s/generation_timing/topk_ratio:0.125 - timing_s/gen:2671.0059438981116 - timing_s/reward:0.9045847211964428 - timing_s/old_log_prob:118.77138863084838 - timing_s/ref:118.36565449088812 - timing_s/adv:0.04458089591935277 - timing_s/update_actor:362.3183565577492 - timing_s/dump_rollout_generations:2.012486052233726 - timing_s/step:3273.431012901012 - timing_s/stop_profile:6.610015407204628e-05 - timing_per_token_ms/ref:0.03937315139174518 - timing_per_token_ms/adv:1.4829389249458635e-05 - timing_per_token_ms/update_actor:0.12052157837605458 - timing_per_token_ms/gen:0.9004694985256106 - perf/total_num_tokens:3006253 - perf/time_per_step:3273.431012901012 - perf/throughput:57.398739047653116
[36m(WorkerDict pid=2136258)[0m INFO 12-14 15:47:46 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136241)[0m INFO 12-14 15:47:46 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136257)[0m INFO 12-14 15:47:46 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136265)[0m INFO 12-14 15:47:46 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136248)[0m INFO 12-14 15:47:47 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136253)[0m INFO 12-14 15:47:47 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136255)[0m INFO 12-14 15:47:47 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136254)[0m INFO 12-14 15:47:47 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136269)[0m INFO 12-14 15:47:47 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136244)[0m INFO 12-14 15:47:48 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136247)[0m INFO 12-14 15:47:48 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136249)[0m INFO 12-14 15:47:48 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136256)[0m INFO 12-14 15:47:48 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136259)[0m INFO 12-14 15:47:48 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136260)[0m INFO 12-14 15:47:48 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136261)[0m INFO 12-14 15:47:51 [block_pool.py:378] Successfully reset prefix cache
[36m(TaskRunner pid=2108541)[0m Dumped generations to /workspace/data/dump/18.jsonl
[36m(TaskRunner pid=2108541)[0m step:18 - actor/entropy:0.27984970808029175 - actor/kl_loss:0.0011369963245843316 - actor/kl_coef:0.0010000000000000002 - actor/pg_loss:-0.0018839560936959017 - actor/pg_clipfrac:0.0 - actor/ppo_kl:-5.270290227091708e-10 - actor/pg_clipfrac_lower:0.0 - actor/grad_norm:0.042122408865897645 - perf/mfu/actor:0.39868862045764136 - perf/max_memory_allocated_gb:47.89426231384277 - perf/max_memory_reserved_gb:48.05859375 - perf/cpu_memory_used_gb:1044.220329284668 - actor/lr:1e-06 - training/global_step:18 - training/epoch:8 - critic/score/mean:0.689453125 - critic/score/max:1.0 - critic/score/min:0.0 - critic/rewards/mean:0.689453125 - critic/rewards/max:1.0 - critic/rewards/min:0.0 - critic/advantages/mean:0.0019134797621518373 - critic/advantages/max:1.6770472526550293 - critic/advantages/min:-2.561730146408081 - critic/returns/mean:0.0019134797621518373 - critic/returns/max:1.6770472526550293 - critic/returns/min:-2.561730146408081 - response_length/mean:5952.599609375 - response_length/max:29375.0 - response_length/min:964.0 - response_length/clip_ratio:0.0 - response_length_non_aborted/mean:5952.599609375 - response_length_non_aborted/max:29375.0 - response_length_non_aborted/min:964.0 - response_length_non_aborted/clip_ratio:0.0 - response/aborted_ratio:0.0 - prompt_length/mean:74.375 - prompt_length/max:176.0 - prompt_length/min:23.0 - prompt_length/clip_ratio:0.0 - timing_s/start_profile:8.693011477589607e-05 - timing_s/generate_sequences:2345.601318359375 - timing_s/generation_timing/max:2346.94970703125 - timing_s/generation_timing/min:2342.54443359375 - timing_s/generation_timing/topk_ratio:0.125 - timing_s/gen:2390.901511091739 - timing_s/reward:0.9090118650346994 - timing_s/old_log_prob:127.0643307287246 - timing_s/ref:114.82537526311353 - timing_s/adv:0.04164782725274563 - timing_s/update_actor:360.4355412488803 - timing_s/dump_rollout_generations:2.0580299561843276 - timing_s/step:2996.2430525850505 - timing_s/stop_profile:7.257005199790001e-05 - timing_per_token_ms/ref:0.03721076088688307 - timing_per_token_ms/adv:1.3496558037010572e-05 - timing_per_token_ms/update_actor:0.1168041533486271 - timing_per_token_ms/gen:0.7844857407335946 - perf/total_num_tokens:3085811 - perf/time_per_step:2996.2430525850505 - perf/throughput:64.36833865450421
[36m(WorkerDict pid=2136257)[0m INFO 12-14 16:37:47 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136258)[0m INFO 12-14 16:37:47 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136241)[0m INFO 12-14 16:37:47 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136265)[0m INFO 12-14 16:37:47 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136253)[0m INFO 12-14 16:37:47 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136255)[0m INFO 12-14 16:37:47 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136254)[0m INFO 12-14 16:37:47 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136269)[0m INFO 12-14 16:37:47 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136256)[0m INFO 12-14 16:37:47 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136244)[0m INFO 12-14 16:37:47 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136249)[0m INFO 12-14 16:37:48 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136247)[0m INFO 12-14 16:37:49 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136261)[0m INFO 12-14 16:37:49 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136260)[0m INFO 12-14 16:37:49 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136259)[0m INFO 12-14 16:37:49 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136248)[0m INFO 12-14 16:37:49 [block_pool.py:378] Successfully reset prefix cache
[36m(TaskRunner pid=2108541)[0m old timing: {'generate_sequences': 2755.396240234375, 'generation_timing/max': 2756.7353515625, 'generation_timing/min': 2754.033935546875, 'generation_timing/topk_ratio': 0.125}
[36m(TaskRunner pid=2108541)[0m new timing: {'generate_sequences': 2755.39599609375, 'generation_timing/max': 2756.7353515625, 'generation_timing/min': 2754.033935546875, 'generation_timing/topk_ratio': 0.125}
[36m(TaskRunner pid=2108541)[0m Dumped generations to /workspace/data/dump/19.jsonl
[36m(TaskRunner pid=2108541)[0m step:19 - actor/entropy:0.25445330142974854 - actor/kl_loss:0.001057313422739775 - actor/kl_coef:0.0010000000000000002 - actor/pg_loss:-0.0017717566610459823 - actor/pg_clipfrac:0.0 - actor/ppo_kl:-1.3807534795278896e-09 - actor/pg_clipfrac_lower:0.0 - actor/grad_norm:0.023245538191534028 - perf/mfu/actor:0.40592002243051645 - perf/max_memory_allocated_gb:47.897844314575195 - perf/max_memory_reserved_gb:48.05859375 - perf/cpu_memory_used_gb:1051.642017364502 - actor/lr:1e-06 - training/global_step:19 - training/epoch:9 - critic/score/mean:0.92578125 - critic/score/max:1.0 - critic/score/min:0.0 - critic/rewards/mean:0.92578125 - critic/rewards/max:1.0 - critic/rewards/min:0.0 - critic/advantages/mean:0.0017100016120821238 - critic/advantages/max:0.5590157508850098 - critic/advantages/min:-3.7499847412109375 - critic/returns/mean:0.0017100016120821238 - critic/returns/max:0.5590157508850098 - critic/returns/min:-3.7499847412109375 - response_length/mean:5875.4765625 - response_length/max:34816.0 - response_length/min:935.0 - response_length/clip_ratio:0.005859375 - response_length_non_aborted/mean:5875.4765625 - response_length_non_aborted/max:34816.0 - response_length_non_aborted/min:935.0 - response_length_non_aborted/clip_ratio:0.005859375 - response/aborted_ratio:0.0 - prompt_length/mean:78.15625 - prompt_length/max:191.0 - prompt_length/min:34.0 - prompt_length/clip_ratio:0.0 - timing_s/start_profile:0.000410531647503376 - timing_s/generate_sequences:2755.396240234375 - timing_s/generation_timing/max:2756.7353515625 - timing_s/generation_timing/min:2754.033935546875 - timing_s/generation_timing/topk_ratio:0.125 - timing_s/gen:2810.9000723781064 - timing_s/reward:0.9073705980554223 - timing_s/old_log_prob:123.36759494012222 - timing_s/ref:118.85195974679664 - timing_s/adv:0.0370329818688333 - timing_s/update_actor:367.38156404392794 - timing_s/dump_rollout_generations:1.9921888341195881 - timing_s/step:3423.4453064557165 - timing_s/stop_profile:6.718980148434639e-05 - timing_per_token_ms/ref:0.03899009918668245 - timing_per_token_ms/adv:1.2148892111838657e-05 - timing_per_token_ms/update_actor:0.12052172847589376 - timing_per_token_ms/gen:0.9343989624439063 - perf/total_num_tokens:3048260 - perf/time_per_step:3423.4453064557165 - perf/throughput:55.65044361618295
[36m(WorkerDict pid=2136257)[0m INFO 12-14 17:34:51 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136258)[0m INFO 12-14 17:34:51 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136241)[0m INFO 12-14 17:34:51 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136253)[0m INFO 12-14 17:34:52 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136269)[0m INFO 12-14 17:34:51 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136255)[0m INFO 12-14 17:34:52 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136265)[0m INFO 12-14 17:34:52 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136254)[0m INFO 12-14 17:34:52 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136261)[0m INFO 12-14 17:34:52 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136244)[0m INFO 12-14 17:34:52 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136249)[0m INFO 12-14 17:34:53 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136247)[0m INFO 12-14 17:34:53 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136259)[0m INFO 12-14 17:34:53 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136260)[0m INFO 12-14 17:34:53 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136248)[0m INFO 12-14 17:34:53 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=2136256)[0m INFO 12-14 17:34:54 [block_pool.py:378] Successfully reset prefix cache
[36m(TaskRunner pid=2108541)[0m old timing: {'generate_sequences': 2046.9998779296875, 'generation_timing/max': 2047.8828125, 'generation_timing/min': 2045.086669921875, 'generation_timing/topk_ratio': 0.125}
[36m(TaskRunner pid=2108541)[0m new timing: {'generate_sequences': 2046.999755859375, 'generation_timing/max': 2047.8828125, 'generation_timing/min': 2045.086669921875, 'generation_timing/topk_ratio': 0.125}
[36m(TaskRunner pid=2108541)[0m Dumped generations to /workspace/data/dump/20.jsonl
[36m(TaskRunner pid=2108541)[0m step:20 - actor/entropy:0.27803313732147217 - actor/kl_loss:0.001167018640054656 - actor/kl_coef:0.0010000000000000002 - actor/pg_loss:-0.0015301534481945314 - actor/pg_clipfrac:0.0 - actor/ppo_kl:-6.870957757161038e-10 - actor/pg_clipfrac_lower:0.0 - actor/grad_norm:0.024213045596311232 - perf/mfu/actor:0.3949741382777407 - perf/max_memory_allocated_gb:47.897844314575195 - perf/max_memory_reserved_gb:48.05859375 - perf/cpu_memory_used_gb:1053.8215637207031 - actor/lr:1e-06 - training/global_step:20 - training/epoch:9 - critic/score/mean:0.6953125 - critic/score/max:1.0 - critic/score/min:0.0 - critic/rewards/mean:0.6953125 - critic/rewards/max:1.0 - critic/rewards/min:0.0 - critic/advantages/mean:0.0016287736361846328 - critic/advantages/max:1.4361376762390137 - critic/advantages/min:-1.0978854894638062 - critic/returns/mean:0.0016287736361846328 - critic/returns/max:1.4361376762390137 - critic/returns/min:-1.0978854894638062 - response_length/mean:5905.0 - response_length/max:27372.0 - response_length/min:940.0 - response_length/clip_ratio:0.0 - response_length_non_aborted/mean:5905.0 - response_length_non_aborted/max:27372.0 - response_length_non_aborted/min:940.0 - response_length_non_aborted/clip_ratio:0.0 - response/aborted_ratio:0.0 - prompt_length/mean:74.375 - prompt_length/max:176.0 - prompt_length/min:23.0 - prompt_length/clip_ratio:0.0 - timing_s/start_profile:5.953013896942139e-05 - timing_s/generate_sequences:2046.9998779296875 - timing_s/generation_timing/max:2047.8828125 - timing_s/generation_timing/min:2045.086669921875 - timing_s/generation_timing/topk_ratio:0.125 - timing_s/gen:2096.3825647523627 - timing_s/reward:0.9062305320985615 - timing_s/old_log_prob:119.68267835024744 - timing_s/ref:116.62788673583418 - timing_s/adv:0.040770874824374914 - timing_s/update_actor:362.08199339779094 - timing_s/dump_rollout_generations:1.976495515089482 - timing_s/step:2697.706136880908 - timing_s/stop_profile:6.662076339125633e-05 - timing_per_token_ms/ref:0.03809576105879396 - timing_per_token_ms/adv:1.3317548220567744e-05 - timing_per_token_ms/update_actor:0.11827179150915614 - timing_per_token_ms/gen:0.6933949528843283 - perf/total_num_tokens:3061440 - perf/time_per_step:2697.706136880908 - perf/throughput:70.92692468766357
[36m(TaskRunner pid=2108541)[0m 'Final validation metrics: None'
[36m(TaskRunner pid=2108541)[0m [TaskRunner] After trainer.fit(), about to finish run()
[main_ppo] After ray.get, about to exit run_ppo()
[main] After run_ppo, program should exit soon
[ERROR] 2026-01-03-21:01:14 (PID:3844973, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
INFO 01-03 21:01:46 [__init__.py:36] Available plugins for group vllm.platform_plugins:
INFO 01-03 21:01:46 [__init__.py:38] - ascend -> vllm_ascend:register
INFO 01-03 21:01:46 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 01-03 21:01:46 [__init__.py:207] Platform plugin ascend is activated
[Rank 0 | Local Rank 0] 2026-01-03 21:01:47,719 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[ERROR] 2026-01-03-21:01:48 (PID:3845423, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
INFO 01-03 21:11:37 [__init__.py:36] Available plugins for group vllm.platform_plugins:
INFO 01-03 21:11:37 [__init__.py:38] - ascend -> vllm_ascend:register
INFO 01-03 21:11:37 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 01-03 21:11:37 [__init__.py:207] Platform plugin ascend is activated
[Rank 0 | Local Rank 0] 2026-01-03 21:11:38,313 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[main] Before run_ppo
ray init kwargs: {'num_cpus': None, 'runtime_env': {'env_vars': {'TOKENIZERS_PARALLELISM': 'true', 'NCCL_DEBUG': 'WARN', 'VLLM_LOGGING_LEVEL': 'WARN', 'VLLM_ALLOW_RUNTIME_LORA_UPDATING': 'true', 'CUDA_DEVICE_MAX_CONNECTIONS': '1', 'NCCL_CUMEM_ENABLE': '0'}, 'working_dir': None}}
[main_ppo] Before ray.get(runner.run.remote)
[36m(pid=3869676)[0m [Rank 0 | Local Rank 0] 2026-01-03 21:11:56,254 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(TaskRunner pid=3869676)[0m TaskRunner hostname: liteserver-hps-1f00-00001, PID: 3869676
[36m(TaskRunner pid=3869676)[0m {'actor_rollout_ref': {'actor': {'_target_': 'verl.workers.config.McoreActorConfig',
[36m(TaskRunner pid=3869676)[0m                                  'checkpoint': {'_target_': 'verl.trainer.config.CheckpointConfig',
[36m(TaskRunner pid=3869676)[0m                                                 'async_save': False,
[36m(TaskRunner pid=3869676)[0m                                                 'load_contents': ['model',
[36m(TaskRunner pid=3869676)[0m                                                                   'optimizer',
[36m(TaskRunner pid=3869676)[0m                                                                   'extra'],
[36m(TaskRunner pid=3869676)[0m                                                 'save_contents': ['model',
[36m(TaskRunner pid=3869676)[0m                                                                   'optimizer',
[36m(TaskRunner pid=3869676)[0m                                                                   'extra']},
[36m(TaskRunner pid=3869676)[0m                                  'clip_ratio': 0.2,
[36m(TaskRunner pid=3869676)[0m                                  'clip_ratio_c': 3.0,
[36m(TaskRunner pid=3869676)[0m                                  'clip_ratio_high': 0.2,
[36m(TaskRunner pid=3869676)[0m                                  'clip_ratio_low': 0.2,
[36m(TaskRunner pid=3869676)[0m                                  'data_loader_seed': None,
[36m(TaskRunner pid=3869676)[0m                                  'entropy_coeff': 0,
[36m(TaskRunner pid=3869676)[0m                                  'freeze_vision_tower': False,
[36m(TaskRunner pid=3869676)[0m                                  'kl_loss_coef': 0.001,
[36m(TaskRunner pid=3869676)[0m                                  'kl_loss_type': 'low_var_kl',
[36m(TaskRunner pid=3869676)[0m                                  'load_weight': True,
[36m(TaskRunner pid=3869676)[0m                                  'loss_agg_mode': 'token-mean',
[36m(TaskRunner pid=3869676)[0m                                  'megatron': {'_target_': 'verl.workers.config.McoreEngineConfig',
[36m(TaskRunner pid=3869676)[0m                                               'context_parallel_size': 1,
[36m(TaskRunner pid=3869676)[0m                                               'dist_checkpointing_path': None,
[36m(TaskRunner pid=3869676)[0m                                               'expert_model_parallel_size': 1,
[36m(TaskRunner pid=3869676)[0m                                               'expert_tensor_parallel_size': 1,
[36m(TaskRunner pid=3869676)[0m                                               'forward_only': False,
[36m(TaskRunner pid=3869676)[0m                                               'grad_offload': True,
[36m(TaskRunner pid=3869676)[0m                                               'optimizer_offload': False,
[36m(TaskRunner pid=3869676)[0m                                               'override_ddp_config': {},
[36m(TaskRunner pid=3869676)[0m                                               'override_mcore_model_config': {},
[36m(TaskRunner pid=3869676)[0m                                               'override_transformer_config': {'attention_backend': 'flash',
[36m(TaskRunner pid=3869676)[0m                                                                               'context_parallel_algo': 'megatron_cp_algo',
[36m(TaskRunner pid=3869676)[0m                                                                               'context_parallel_size': 1,
[36m(TaskRunner pid=3869676)[0m                                                                               'cp_window_size': 1,
[36m(TaskRunner pid=3869676)[0m                                                                               'recompute_granularity': 'full',
[36m(TaskRunner pid=3869676)[0m                                                                               'recompute_method': 'uniform',
[36m(TaskRunner pid=3869676)[0m                                                                               'recompute_modules': ['core_attn'],
[36m(TaskRunner pid=3869676)[0m                                                                               'recompute_num_layers': 1,
[36m(TaskRunner pid=3869676)[0m                                                                               'seq_length': 2048,
[36m(TaskRunner pid=3869676)[0m                                                                               'swap_optimizer': True,
[36m(TaskRunner pid=3869676)[0m                                                                               'use_cp_send_recv_overlap': True,
[36m(TaskRunner pid=3869676)[0m                                                                               'use_flash_attn': True,
[36m(TaskRunner pid=3869676)[0m                                                                               'use_fused_ring_attention_update': True,
[36m(TaskRunner pid=3869676)[0m                                                                               'use_fused_rotary_pos_emb': True,
[36m(TaskRunner pid=3869676)[0m                                                                               'use_fused_swiglu': True},
[36m(TaskRunner pid=3869676)[0m                                               'param_offload': True,
[36m(TaskRunner pid=3869676)[0m                                               'pipeline_model_parallel_size': 2,
[36m(TaskRunner pid=3869676)[0m                                               'seed': 42,
[36m(TaskRunner pid=3869676)[0m                                               'sequence_parallel': True,
[36m(TaskRunner pid=3869676)[0m                                               'tensor_model_parallel_size': 8,
[36m(TaskRunner pid=3869676)[0m                                               'use_dist_checkpointing': False,
[36m(TaskRunner pid=3869676)[0m                                               'use_distributed_optimizer': True,
[36m(TaskRunner pid=3869676)[0m                                               'use_mbridge': False,
[36m(TaskRunner pid=3869676)[0m                                               'virtual_pipeline_model_parallel_size': None},
[36m(TaskRunner pid=3869676)[0m                                  'optim': {'_target_': 'verl.workers.config.McoreOptimizerConfig',
[36m(TaskRunner pid=3869676)[0m                                            'betas': [0.9, 0.999],
[36m(TaskRunner pid=3869676)[0m                                            'clip_grad': 10000,
[36m(TaskRunner pid=3869676)[0m                                            'lr': 1e-06,
[36m(TaskRunner pid=3869676)[0m                                            'lr_decay_steps': None,
[36m(TaskRunner pid=3869676)[0m                                            'lr_decay_style': 'constant',
[36m(TaskRunner pid=3869676)[0m                                            'lr_warmup_init': 0.0,
[36m(TaskRunner pid=3869676)[0m                                            'lr_warmup_steps': -1,
[36m(TaskRunner pid=3869676)[0m                                            'lr_warmup_steps_ratio': 0.0,
[36m(TaskRunner pid=3869676)[0m                                            'lr_wsd_decay_steps': None,
[36m(TaskRunner pid=3869676)[0m                                            'lr_wsd_decay_style': 'exponential',
[36m(TaskRunner pid=3869676)[0m                                            'min_lr': 0.0,
[36m(TaskRunner pid=3869676)[0m                                            'optimizer': 'adam',
[36m(TaskRunner pid=3869676)[0m                                            'override_optimizer_config': {},
[36m(TaskRunner pid=3869676)[0m                                            'total_training_steps': -1,
[36m(TaskRunner pid=3869676)[0m                                            'use_checkpoint_opt_param_scheduler': False,
[36m(TaskRunner pid=3869676)[0m                                            'weight_decay': 0.01,
[36m(TaskRunner pid=3869676)[0m                                            'weight_decay_incr_style': 'constant'},
[36m(TaskRunner pid=3869676)[0m                                  'policy_loss': {'_target_': 'verl.workers.config.PolicyLossConfig',
[36m(TaskRunner pid=3869676)[0m                                                  'clip_cov_lb': 1.0,
[36m(TaskRunner pid=3869676)[0m                                                  'clip_cov_ratio': 0.0002,
[36m(TaskRunner pid=3869676)[0m                                                  'clip_cov_ub': 5.0,
[36m(TaskRunner pid=3869676)[0m                                                  'kl_cov_ratio': 0.0002,
[36m(TaskRunner pid=3869676)[0m                                                  'loss_mode': 'vanilla',
[36m(TaskRunner pid=3869676)[0m                                                  'ppo_kl_coef': 0.1},
[36m(TaskRunner pid=3869676)[0m                                  'ppo_epochs': 1,
[36m(TaskRunner pid=3869676)[0m                                  'ppo_max_token_len_per_gpu': 36864,
[36m(TaskRunner pid=3869676)[0m                                  'ppo_micro_batch_size': None,
[36m(TaskRunner pid=3869676)[0m                                  'ppo_micro_batch_size_per_gpu': 1,
[36m(TaskRunner pid=3869676)[0m                                  'ppo_mini_batch_size': 32,
[36m(TaskRunner pid=3869676)[0m                                  'profiler': {'_target_': 'verl.utils.profiler.ProfilerConfig',
[36m(TaskRunner pid=3869676)[0m                                               'all_ranks': False,
[36m(TaskRunner pid=3869676)[0m                                               'enable': False,
[36m(TaskRunner pid=3869676)[0m                                               'ranks': [],
[36m(TaskRunner pid=3869676)[0m                                               'save_path': 'outputs/profile',
[36m(TaskRunner pid=3869676)[0m                                               'tool': None,
[36m(TaskRunner pid=3869676)[0m                                               'tool_config': {'npu': {'_target_': 'verl.utils.profiler.config.NPUToolConfig',
[36m(TaskRunner pid=3869676)[0m                                                                       'analysis': True,
[36m(TaskRunner pid=3869676)[0m                                                                       'contents': [],
[36m(TaskRunner pid=3869676)[0m                                                                       'discrete': False,
[36m(TaskRunner pid=3869676)[0m                                                                       'level': 'level1'},
[36m(TaskRunner pid=3869676)[0m                                                               'nsys': {'_target_': 'verl.utils.profiler.config.NsightToolConfig',
[36m(TaskRunner pid=3869676)[0m                                                                        'discrete': False},
[36m(TaskRunner pid=3869676)[0m                                                               'torch': {'_target_': 'verl.utils.profiler.config.TorchProfilerToolConfig',
[36m(TaskRunner pid=3869676)[0m                                                                         'step_end': None,
[36m(TaskRunner pid=3869676)[0m                                                                         'step_start': 0},
[36m(TaskRunner pid=3869676)[0m                                                               'torch_memory': {'_target_': 'verl.utils.profiler.config.TorchMemoryToolConfig',
[36m(TaskRunner pid=3869676)[0m                                                                                'stack_depth': 32,
[36m(TaskRunner pid=3869676)[0m                                                                                'trace_alloc_max_entries': 100000}}},
[36m(TaskRunner pid=3869676)[0m                                  'recompute_old_log_prob': True,
[36m(TaskRunner pid=3869676)[0m                                  'shuffle': False,
[36m(TaskRunner pid=3869676)[0m                                  'strategy': 'megatron',
[36m(TaskRunner pid=3869676)[0m                                  'use_dynamic_bsz': True,
[36m(TaskRunner pid=3869676)[0m                                  'use_fused_kernels': False,
[36m(TaskRunner pid=3869676)[0m                                  'use_kl_loss': True,
[36m(TaskRunner pid=3869676)[0m                                  'use_torch_compile': True},
[36m(TaskRunner pid=3869676)[0m                        'hybrid_engine': True,
[36m(TaskRunner pid=3869676)[0m                        'model': {'custom_chat_template': None,
[36m(TaskRunner pid=3869676)[0m                                  'external_lib': None,
[36m(TaskRunner pid=3869676)[0m                                  'override_config': {'model_config': {},
[36m(TaskRunner pid=3869676)[0m                                                      'moe_config': {'freeze_moe_router': False}},
[36m(TaskRunner pid=3869676)[0m                                  'path': '/home/data/Qwen3-32B',
[36m(TaskRunner pid=3869676)[0m                                  'trust_remote_code': False,
[36m(TaskRunner pid=3869676)[0m                                  'use_fused_kernels': False,
[36m(TaskRunner pid=3869676)[0m                                  'use_remove_padding': False},
[36m(TaskRunner pid=3869676)[0m                        'nccl_timeout': 600,
[36m(TaskRunner pid=3869676)[0m                        'ref': {'load_weight': True,
[36m(TaskRunner pid=3869676)[0m                                'log_prob_max_token_len_per_gpu': 36864,
[36m(TaskRunner pid=3869676)[0m                                'log_prob_micro_batch_size': None,
[36m(TaskRunner pid=3869676)[0m                                'log_prob_micro_batch_size_per_gpu': 8,
[36m(TaskRunner pid=3869676)[0m                                'log_prob_use_dynamic_bsz': True,
[36m(TaskRunner pid=3869676)[0m                                'megatron': {'_target_': 'verl.workers.config.MegatronEngineConfig',
[36m(TaskRunner pid=3869676)[0m                                             'context_parallel_size': 1,
[36m(TaskRunner pid=3869676)[0m                                             'dist_checkpointing_path': None,
[36m(TaskRunner pid=3869676)[0m                                             'expert_model_parallel_size': 1,
[36m(TaskRunner pid=3869676)[0m                                             'expert_tensor_parallel_size': 1,
[36m(TaskRunner pid=3869676)[0m                                             'forward_only': False,
[36m(TaskRunner pid=3869676)[0m                                             'grad_offload': False,
[36m(TaskRunner pid=3869676)[0m                                             'optimizer_offload': False,
[36m(TaskRunner pid=3869676)[0m                                             'override_ddp_config': {},
[36m(TaskRunner pid=3869676)[0m                                             'override_mcore_model_config': {},
[36m(TaskRunner pid=3869676)[0m                                             'override_transformer_config': {'attention_backend': 'flash',
[36m(TaskRunner pid=3869676)[0m                                                                             'context_parallel_algo': 'megatron_cp_algo',
[36m(TaskRunner pid=3869676)[0m                                                                             'context_parallel_size': 1,
[36m(TaskRunner pid=3869676)[0m                                                                             'cp_window_size': 1,
[36m(TaskRunner pid=3869676)[0m                                                                             'recompute_granularity': 'full',
[36m(TaskRunner pid=3869676)[0m                                                                             'recompute_method': 'uniform',
[36m(TaskRunner pid=3869676)[0m                                                                             'recompute_modules': ['core_attn'],
[36m(TaskRunner pid=3869676)[0m                                                                             'recompute_num_layers': 1,
[36m(TaskRunner pid=3869676)[0m                                                                             'seq_length': 2048,
[36m(TaskRunner pid=3869676)[0m                                                                             'swap_optimizer': True,
[36m(TaskRunner pid=3869676)[0m                                                                             'use_cp_send_recv_overlap': True,
[36m(TaskRunner pid=3869676)[0m                                                                             'use_flash_attn': True,
[36m(TaskRunner pid=3869676)[0m                                                                             'use_fused_ring_attention_update': True,
[36m(TaskRunner pid=3869676)[0m                                                                             'use_fused_rotary_pos_emb': True,
[36m(TaskRunner pid=3869676)[0m                                                                             'use_fused_swiglu': True},
[36m(TaskRunner pid=3869676)[0m                                             'param_offload': True,
[36m(TaskRunner pid=3869676)[0m                                             'pipeline_model_parallel_size': 1,
[36m(TaskRunner pid=3869676)[0m                                             'seed': 42,
[36m(TaskRunner pid=3869676)[0m                                             'sequence_parallel': True,
[36m(TaskRunner pid=3869676)[0m                                             'tensor_model_parallel_size': 1,
[36m(TaskRunner pid=3869676)[0m                                             'use_dist_checkpointing': False,
[36m(TaskRunner pid=3869676)[0m                                             'use_distributed_optimizer': True,
[36m(TaskRunner pid=3869676)[0m                                             'use_mbridge': False,
[36m(TaskRunner pid=3869676)[0m                                             'virtual_pipeline_model_parallel_size': None},
[36m(TaskRunner pid=3869676)[0m                                'profiler': {'_target_': 'verl.utils.profiler.ProfilerConfig',
[36m(TaskRunner pid=3869676)[0m                                             'all_ranks': False,
[36m(TaskRunner pid=3869676)[0m                                             'enable': False,
[36m(TaskRunner pid=3869676)[0m                                             'ranks': [],
[36m(TaskRunner pid=3869676)[0m                                             'save_path': 'outputs/profile',
[36m(TaskRunner pid=3869676)[0m                                             'tool': None,
[36m(TaskRunner pid=3869676)[0m                                             'tool_config': {'npu': {'_target_': 'verl.utils.profiler.config.NPUToolConfig',
[36m(TaskRunner pid=3869676)[0m                                                                     'analysis': True,
[36m(TaskRunner pid=3869676)[0m                                                                     'contents': [],
[36m(TaskRunner pid=3869676)[0m                                                                     'discrete': False,
[36m(TaskRunner pid=3869676)[0m                                                                     'level': 'level1'},
[36m(TaskRunner pid=3869676)[0m                                                             'nsys': {'_target_': 'verl.utils.profiler.config.NsightToolConfig',
[36m(TaskRunner pid=3869676)[0m                                                                      'discrete': False},
[36m(TaskRunner pid=3869676)[0m                                                             'torch': {'_target_': 'verl.utils.profiler.config.TorchProfilerToolConfig',
[36m(TaskRunner pid=3869676)[0m                                                                       'step_end': None,
[36m(TaskRunner pid=3869676)[0m                                                                       'step_start': 0},
[36m(TaskRunner pid=3869676)[0m                                                             'torch_memory': {'_target_': 'verl.utils.profiler.config.TorchMemoryToolConfig',
[36m(TaskRunner pid=3869676)[0m                                                                              'stack_depth': 32,
[36m(TaskRunner pid=3869676)[0m                                                                              'trace_alloc_max_entries': 100000}}},
[36m(TaskRunner pid=3869676)[0m                                'strategy': 'megatron',
[36m(TaskRunner pid=3869676)[0m                                'use_torch_compile': True},
[36m(TaskRunner pid=3869676)[0m                        'rollout': {'_target_': 'verl.workers.config.RolloutConfig',
[36m(TaskRunner pid=3869676)[0m                                    'agent': {'_target_': 'verl.workers.config.AgentLoopConfig',
[36m(TaskRunner pid=3869676)[0m                                              'agent_loop_config_path': None,
[36m(TaskRunner pid=3869676)[0m                                              'custom_async_server': {'_target_': 'verl.workers.config.CustomAsyncServerConfig',
[36m(TaskRunner pid=3869676)[0m                                                                      'name': None,
[36m(TaskRunner pid=3869676)[0m                                                                      'path': None},
[36m(TaskRunner pid=3869676)[0m                                              'default_agent_loop': 'single_turn_agent',
[36m(TaskRunner pid=3869676)[0m                                              'num_workers': 8},
[36m(TaskRunner pid=3869676)[0m                                    'calculate_log_probs': False,
[36m(TaskRunner pid=3869676)[0m                                    'cudagraph_capture_sizes': None,
[36m(TaskRunner pid=3869676)[0m                                    'data_parallel_size': 1,
[36m(TaskRunner pid=3869676)[0m                                    'disable_log_stats': True,
[36m(TaskRunner pid=3869676)[0m                                    'do_sample': True,
[36m(TaskRunner pid=3869676)[0m                                    'dtype': 'bfloat16',
[36m(TaskRunner pid=3869676)[0m                                    'enable_chunked_prefill': True,
[36m(TaskRunner pid=3869676)[0m                                    'enable_prefix_caching': True,
[36m(TaskRunner pid=3869676)[0m                                    'enforce_eager': True,
[36m(TaskRunner pid=3869676)[0m                                    'engine_kwargs': {'sglang': {},
[36m(TaskRunner pid=3869676)[0m                                                      'vllm': {'speculative_config': {'method': 'sam',
[36m(TaskRunner pid=3869676)[0m                                                                                      'num_speculative_tokens': 3}}},
[36m(TaskRunner pid=3869676)[0m                                    'expert_parallel_size': 1,
[36m(TaskRunner pid=3869676)[0m                                    'free_cache_engine': True,
[36m(TaskRunner pid=3869676)[0m                                    'gpu_memory_utilization': 0.85,
[36m(TaskRunner pid=3869676)[0m                                    'ignore_eos': False,
[36m(TaskRunner pid=3869676)[0m                                    'layer_name_map': {'gate_proj_layer_name': 'gate_up',
[36m(TaskRunner pid=3869676)[0m                                                       'qkv_layer_name': 'qkv'},
[36m(TaskRunner pid=3869676)[0m                                    'load_format': 'dummy',
[36m(TaskRunner pid=3869676)[0m                                    'log_prob_max_token_len_per_gpu': 36864,
[36m(TaskRunner pid=3869676)[0m                                    'log_prob_micro_batch_size': None,
[36m(TaskRunner pid=3869676)[0m                                    'log_prob_micro_batch_size_per_gpu': None,
[36m(TaskRunner pid=3869676)[0m                                    'log_prob_use_dynamic_bsz': True,
[36m(TaskRunner pid=3869676)[0m                                    'max_model_len': None,
[36m(TaskRunner pid=3869676)[0m                                    'max_num_batched_tokens': 36864,
[36m(TaskRunner pid=3869676)[0m                                    'max_num_seqs': 128,
[36m(TaskRunner pid=3869676)[0m                                    'mode': 'sync',
[36m(TaskRunner pid=3869676)[0m                                    'multi_stage_wake_up': False,
[36m(TaskRunner pid=3869676)[0m                                    'multi_turn': {'_target_': 'verl.workers.config.MultiTurnConfig',
[36m(TaskRunner pid=3869676)[0m                                                   'enable': False,
[36m(TaskRunner pid=3869676)[0m                                                   'format': 'hermes',
[36m(TaskRunner pid=3869676)[0m                                                   'interaction_config_path': None,
[36m(TaskRunner pid=3869676)[0m                                                   'max_assistant_turns': None,
[36m(TaskRunner pid=3869676)[0m                                                   'max_parallel_calls': 1,
[36m(TaskRunner pid=3869676)[0m                                                   'max_tool_response_length': 256,
[36m(TaskRunner pid=3869676)[0m                                                   'max_user_turns': None,
[36m(TaskRunner pid=3869676)[0m                                                   'num_repeat_rollouts': None,
[36m(TaskRunner pid=3869676)[0m                                                   'tokenization_sanity_check_mode': 'strict',
[36m(TaskRunner pid=3869676)[0m                                                   'tool_config_path': None,
[36m(TaskRunner pid=3869676)[0m                                                   'tool_response_truncate_side': 'middle',
[36m(TaskRunner pid=3869676)[0m                                                   'use_inference_chat_template': False},
[36m(TaskRunner pid=3869676)[0m                                    'n': 16,
[36m(TaskRunner pid=3869676)[0m                                    'name': 'vllm',
[36m(TaskRunner pid=3869676)[0m                                    'over_sample_rate': 0,
[36m(TaskRunner pid=3869676)[0m                                    'pipeline_model_parallel_size': 1,
[36m(TaskRunner pid=3869676)[0m                                    'profiler': {'_target_': 'verl.utils.profiler.ProfilerConfig',
[36m(TaskRunner pid=3869676)[0m                                                 'all_ranks': False,
[36m(TaskRunner pid=3869676)[0m                                                 'enable': False,
[36m(TaskRunner pid=3869676)[0m                                                 'ranks': [],
[36m(TaskRunner pid=3869676)[0m                                                 'save_path': 'outputs/profile',
[36m(TaskRunner pid=3869676)[0m                                                 'tool': None,
[36m(TaskRunner pid=3869676)[0m                                                 'tool_config': {'npu': {'_target_': 'verl.utils.profiler.config.NPUToolConfig',
[36m(TaskRunner pid=3869676)[0m                                                                         'analysis': True,
[36m(TaskRunner pid=3869676)[0m                                                                         'contents': [],
[36m(TaskRunner pid=3869676)[0m                                                                         'discrete': False,
[36m(TaskRunner pid=3869676)[0m                                                                         'level': 'level1'},
[36m(TaskRunner pid=3869676)[0m                                                                 'nsys': {'_target_': 'verl.utils.profiler.config.NsightToolConfig',
[36m(TaskRunner pid=3869676)[0m                                                                          'discrete': False},
[36m(TaskRunner pid=3869676)[0m                                                                 'torch': {'_target_': 'verl.utils.profiler.config.TorchProfilerToolConfig',
[36m(TaskRunner pid=3869676)[0m                                                                           'step_end': None,
[36m(TaskRunner pid=3869676)[0m                                                                           'step_start': 0},
[36m(TaskRunner pid=3869676)[0m                                                                 'torch_memory': {'_target_': 'verl.utils.profiler.config.TorchMemoryToolConfig',
[36m(TaskRunner pid=3869676)[0m                                                                                  'stack_depth': 32,
[36m(TaskRunner pid=3869676)[0m                                                                                  'trace_alloc_max_entries': 100000}}},
[36m(TaskRunner pid=3869676)[0m                                    'prompt_length': 2048,
[36m(TaskRunner pid=3869676)[0m                                    'response_length': 34816,
[36m(TaskRunner pid=3869676)[0m                                    'skip_dump_dir': '/tmp/rollout_dump',
[36m(TaskRunner pid=3869676)[0m                                    'skip_rollout': False,
[36m(TaskRunner pid=3869676)[0m                                    'skip_tokenizer_init': True,
[36m(TaskRunner pid=3869676)[0m                                    'temperature': 0.9,
[36m(TaskRunner pid=3869676)[0m                                    'tensor_model_parallel_size': 8,
[36m(TaskRunner pid=3869676)[0m                                    'top_k': -1,
[36m(TaskRunner pid=3869676)[0m                                    'top_p': 1.0,
[36m(TaskRunner pid=3869676)[0m                                    'trace': {'_target_': 'verl.workers.config.TraceConfig',
[36m(TaskRunner pid=3869676)[0m                                              'backend': None,
[36m(TaskRunner pid=3869676)[0m                                              'token2text': False},
[36m(TaskRunner pid=3869676)[0m                                    'update_weights_bucket_megabytes': 512,
[36m(TaskRunner pid=3869676)[0m                                    'val_kwargs': {'_target_': 'verl.workers.config.SamplingConfig',
[36m(TaskRunner pid=3869676)[0m                                                   'do_sample': False,
[36m(TaskRunner pid=3869676)[0m                                                   'n': 1,
[36m(TaskRunner pid=3869676)[0m                                                   'temperature': 0,
[36m(TaskRunner pid=3869676)[0m                                                   'top_k': -1,
[36m(TaskRunner pid=3869676)[0m                                                   'top_p': 1.0}}},
[36m(TaskRunner pid=3869676)[0m  'algorithm': {'_target_': 'verl.trainer.config.AlgoConfig',
[36m(TaskRunner pid=3869676)[0m                'adv_estimator': 'grpo',
[36m(TaskRunner pid=3869676)[0m                'gamma': 1.0,
[36m(TaskRunner pid=3869676)[0m                'kl_ctrl': {'_target_': 'verl.trainer.config.KLControlConfig',
[36m(TaskRunner pid=3869676)[0m                            'horizon': 10000,
[36m(TaskRunner pid=3869676)[0m                            'kl_coef': 0.001,
[36m(TaskRunner pid=3869676)[0m                            'target_kl': 0.1,
[36m(TaskRunner pid=3869676)[0m                            'type': 'fixed'},
[36m(TaskRunner pid=3869676)[0m                'kl_penalty': 'kl',
[36m(TaskRunner pid=3869676)[0m                'lam': 1.0,
[36m(TaskRunner pid=3869676)[0m                'norm_adv_by_std_in_grpo': True,
[36m(TaskRunner pid=3869676)[0m                'pf_ppo': {'reweight_method': 'pow', 'weight_pow': 2.0},
[36m(TaskRunner pid=3869676)[0m                'rollout_is': False,
[36m(TaskRunner pid=3869676)[0m                'rollout_is_level': 'token',
[36m(TaskRunner pid=3869676)[0m                'rollout_is_mode': 'truncate',
[36m(TaskRunner pid=3869676)[0m                'rollout_is_threshold': None,
[36m(TaskRunner pid=3869676)[0m                'rollout_is_threshold_lower': None,
[36m(TaskRunner pid=3869676)[0m                'rollout_is_veto_threshold': 0.0001,
[36m(TaskRunner pid=3869676)[0m                'use_kl_in_reward': False,
[36m(TaskRunner pid=3869676)[0m                'use_pf_ppo': False},
[36m(TaskRunner pid=3869676)[0m  'critic': {'_target_': 'verl.workers.config.McoreCriticConfig',
[36m(TaskRunner pid=3869676)[0m             'checkpoint': {'_target_': 'verl.trainer.config.CheckpointConfig',
[36m(TaskRunner pid=3869676)[0m                            'async_save': False,
[36m(TaskRunner pid=3869676)[0m                            'load_contents': ['model', 'optimizer', 'extra'],
[36m(TaskRunner pid=3869676)[0m                            'save_contents': ['model', 'optimizer', 'extra']},
[36m(TaskRunner pid=3869676)[0m             'cliprange_value': 0.5,
[36m(TaskRunner pid=3869676)[0m             'data_loader_seed': None,
[36m(TaskRunner pid=3869676)[0m             'enable': None,
[36m(TaskRunner pid=3869676)[0m             'forward_max_token_len_per_gpu': 32768,
[36m(TaskRunner pid=3869676)[0m             'load_weight': True,
[36m(TaskRunner pid=3869676)[0m             'loss_agg_mode': 'token-mean',
[36m(TaskRunner pid=3869676)[0m             'megatron': {'_target_': 'verl.workers.config.McoreEngineConfig',
[36m(TaskRunner pid=3869676)[0m                          'context_parallel_size': 1,
[36m(TaskRunner pid=3869676)[0m                          'dist_checkpointing_path': None,
[36m(TaskRunner pid=3869676)[0m                          'expert_model_parallel_size': 1,
[36m(TaskRunner pid=3869676)[0m                          'expert_tensor_parallel_size': 1,
[36m(TaskRunner pid=3869676)[0m                          'forward_only': False,
[36m(TaskRunner pid=3869676)[0m                          'grad_offload': False,
[36m(TaskRunner pid=3869676)[0m                          'optimizer_offload': False,
[36m(TaskRunner pid=3869676)[0m                          'override_ddp_config': {},
[36m(TaskRunner pid=3869676)[0m                          'override_mcore_model_config': {},
[36m(TaskRunner pid=3869676)[0m                          'override_transformer_config': {'attention_backend': 'flash',
[36m(TaskRunner pid=3869676)[0m                                                          'recompute_granularity': None,
[36m(TaskRunner pid=3869676)[0m                                                          'recompute_method': None,
[36m(TaskRunner pid=3869676)[0m                                                          'recompute_modules': ['core_attn'],
[36m(TaskRunner pid=3869676)[0m                                                          'recompute_num_layers': None},
[36m(TaskRunner pid=3869676)[0m                          'param_offload': False,
[36m(TaskRunner pid=3869676)[0m                          'pipeline_model_parallel_size': 1,
[36m(TaskRunner pid=3869676)[0m                          'seed': 42,
[36m(TaskRunner pid=3869676)[0m                          'sequence_parallel': True,
[36m(TaskRunner pid=3869676)[0m                          'tensor_model_parallel_size': 1,
[36m(TaskRunner pid=3869676)[0m                          'use_dist_checkpointing': False,
[36m(TaskRunner pid=3869676)[0m                          'use_distributed_optimizer': True,
[36m(TaskRunner pid=3869676)[0m                          'use_mbridge': False,
[36m(TaskRunner pid=3869676)[0m                          'virtual_pipeline_model_parallel_size': None},
[36m(TaskRunner pid=3869676)[0m             'model': {'_target_': 'verl.trainer.config.BaseModelConfig',
[36m(TaskRunner pid=3869676)[0m                       'external_lib': None,
[36m(TaskRunner pid=3869676)[0m                       'override_config': {'model_config': {},
[36m(TaskRunner pid=3869676)[0m                                           'moe_config': {'freeze_moe_router': False}},
[36m(TaskRunner pid=3869676)[0m                       'path': '~/models/deepseek-llm-7b-chat',
[36m(TaskRunner pid=3869676)[0m                       'tokenizer_path': '/home/data/Qwen3-32B',
[36m(TaskRunner pid=3869676)[0m                       'trust_remote_code': False},
[36m(TaskRunner pid=3869676)[0m             'nccl_timeout': 600,
[36m(TaskRunner pid=3869676)[0m             'optim': {'_target_': 'verl.workers.config.McoreOptimizerConfig',
[36m(TaskRunner pid=3869676)[0m                       'betas': [0.9, 0.999],
[36m(TaskRunner pid=3869676)[0m                       'clip_grad': 1.0,
[36m(TaskRunner pid=3869676)[0m                       'lr': 1e-05,
[36m(TaskRunner pid=3869676)[0m                       'lr_decay_steps': None,
[36m(TaskRunner pid=3869676)[0m                       'lr_decay_style': 'constant',
[36m(TaskRunner pid=3869676)[0m                       'lr_warmup_init': 0.0,
[36m(TaskRunner pid=3869676)[0m                       'lr_warmup_steps': -1,
[36m(TaskRunner pid=3869676)[0m                       'lr_warmup_steps_ratio': 0.0,
[36m(TaskRunner pid=3869676)[0m                       'lr_wsd_decay_steps': None,
[36m(TaskRunner pid=3869676)[0m                       'lr_wsd_decay_style': 'exponential',
[36m(TaskRunner pid=3869676)[0m                       'min_lr': 0.0,
[36m(TaskRunner pid=3869676)[0m                       'optimizer': 'adam',
[36m(TaskRunner pid=3869676)[0m                       'override_optimizer_config': {},
[36m(TaskRunner pid=3869676)[0m                       'total_training_steps': -1,
[36m(TaskRunner pid=3869676)[0m                       'use_checkpoint_opt_param_scheduler': False,
[36m(TaskRunner pid=3869676)[0m                       'weight_decay': 0.01,
[36m(TaskRunner pid=3869676)[0m                       'weight_decay_incr_style': 'constant'},
[36m(TaskRunner pid=3869676)[0m             'ppo_epochs': 1,
[36m(TaskRunner pid=3869676)[0m             'ppo_max_token_len_per_gpu': 32768,
[36m(TaskRunner pid=3869676)[0m             'ppo_micro_batch_size': None,
[36m(TaskRunner pid=3869676)[0m             'ppo_micro_batch_size_per_gpu': None,
[36m(TaskRunner pid=3869676)[0m             'ppo_mini_batch_size': 32,
[36m(TaskRunner pid=3869676)[0m             'profiler': {'_target_': 'verl.utils.profiler.ProfilerConfig',
[36m(TaskRunner pid=3869676)[0m                          'all_ranks': False,
[36m(TaskRunner pid=3869676)[0m                          'enable': False,
[36m(TaskRunner pid=3869676)[0m                          'ranks': [],
[36m(TaskRunner pid=3869676)[0m                          'save_path': 'outputs/profile',
[36m(TaskRunner pid=3869676)[0m                          'tool': None,
[36m(TaskRunner pid=3869676)[0m                          'tool_config': {'npu': {'_target_': 'verl.utils.profiler.config.NPUToolConfig',
[36m(TaskRunner pid=3869676)[0m                                                  'analysis': True,
[36m(TaskRunner pid=3869676)[0m                                                  'contents': [],
[36m(TaskRunner pid=3869676)[0m                                                  'discrete': False,
[36m(TaskRunner pid=3869676)[0m                                                  'level': 'level1'},
[36m(TaskRunner pid=3869676)[0m                                          'nsys': {'_target_': 'verl.utils.profiler.config.NsightToolConfig',
[36m(TaskRunner pid=3869676)[0m                                                   'discrete': False},
[36m(TaskRunner pid=3869676)[0m                                          'torch': {'_target_': 'verl.utils.profiler.config.TorchProfilerToolConfig',
[36m(TaskRunner pid=3869676)[0m                                                    'step_end': None,
[36m(TaskRunner pid=3869676)[0m                                                    'step_start': 0},
[36m(TaskRunner pid=3869676)[0m                                          'torch_memory': {'_target_': 'verl.utils.profiler.config.TorchMemoryToolConfig',
[36m(TaskRunner pid=3869676)[0m                                                           'stack_depth': 32,
[36m(TaskRunner pid=3869676)[0m                                                           'trace_alloc_max_entries': 100000}}},
[36m(TaskRunner pid=3869676)[0m             'rollout_n': 16,
[36m(TaskRunner pid=3869676)[0m             'shuffle': False,
[36m(TaskRunner pid=3869676)[0m             'strategy': 'megatron',
[36m(TaskRunner pid=3869676)[0m             'use_dynamic_bsz': True},
[36m(TaskRunner pid=3869676)[0m  'custom_reward_function': {'name': 'compute_score', 'path': 'deepscaler.py'},
[36m(TaskRunner pid=3869676)[0m  'data': {'apply_chat_template_kwargs': {},
[36m(TaskRunner pid=3869676)[0m           'custom_cls': {'name': None, 'path': None},
[36m(TaskRunner pid=3869676)[0m           'datagen': {'name': None, 'path': None},
[36m(TaskRunner pid=3869676)[0m           'dataloader_num_workers': 8,
[36m(TaskRunner pid=3869676)[0m           'dataset_fraction': 0.002,
[36m(TaskRunner pid=3869676)[0m           'filter_overlong_prompts': True,
[36m(TaskRunner pid=3869676)[0m           'filter_overlong_prompts_workers': 1,
[36m(TaskRunner pid=3869676)[0m           'image_key': 'images',
[36m(TaskRunner pid=3869676)[0m           'max_prompt_length': 2048,
[36m(TaskRunner pid=3869676)[0m           'max_response_length': 34816,
[36m(TaskRunner pid=3869676)[0m           'prompt_key': 'prompt',
[36m(TaskRunner pid=3869676)[0m           'return_full_prompt': False,
[36m(TaskRunner pid=3869676)[0m           'return_multi_modal_inputs': True,
[36m(TaskRunner pid=3869676)[0m           'return_raw_chat': False,
[36m(TaskRunner pid=3869676)[0m           'return_raw_input_ids': False,
[36m(TaskRunner pid=3869676)[0m           'reward_fn_key': 'data_source',
[36m(TaskRunner pid=3869676)[0m           'sampler': {'class_name': None, 'class_path': None},
[36m(TaskRunner pid=3869676)[0m           'shuffle': False,
[36m(TaskRunner pid=3869676)[0m           'tokenizer': None,
[36m(TaskRunner pid=3869676)[0m           'train_batch_size': 32,
[36m(TaskRunner pid=3869676)[0m           'train_files': '/workspace/data/deepscaler/train.parquet',
[36m(TaskRunner pid=3869676)[0m           'truncation': 'error',
[36m(TaskRunner pid=3869676)[0m           'trust_remote_code': False,
[36m(TaskRunner pid=3869676)[0m           'use_shm': False,
[36m(TaskRunner pid=3869676)[0m           'val_batch_size': None,
[36m(TaskRunner pid=3869676)[0m           'val_files': '/workspace/data/deepscaler/test.parquet',
[36m(TaskRunner pid=3869676)[0m           'validation_shuffle': False,
[36m(TaskRunner pid=3869676)[0m           'video_key': 'videos'},
[36m(TaskRunner pid=3869676)[0m  'global_profiler': {'_target_': 'verl.utils.profiler.ProfilerConfig',
[36m(TaskRunner pid=3869676)[0m                      'global_tool_config': {'nsys': {'controller_nsight_options': {'cuda-graph-trace': 'graph',
[36m(TaskRunner pid=3869676)[0m                                                                                    'cuda-memory-usage': 'true',
[36m(TaskRunner pid=3869676)[0m                                                                                    'trace': 'cuda,nvtx,cublas,ucx'},
[36m(TaskRunner pid=3869676)[0m                                                      'discrete': False,
[36m(TaskRunner pid=3869676)[0m                                                      'worker_nsight_options': {'capture-range': 'cudaProfilerApi',
[36m(TaskRunner pid=3869676)[0m                                                                                'capture-range-end': None,
[36m(TaskRunner pid=3869676)[0m                                                                                'cuda-graph-trace': 'graph',
[36m(TaskRunner pid=3869676)[0m                                                                                'cuda-memory-usage': 'true',
[36m(TaskRunner pid=3869676)[0m                                                                                'kill': 'none',
[36m(TaskRunner pid=3869676)[0m                                                                                'trace': 'cuda,nvtx,cublas,ucx'}},
[36m(TaskRunner pid=3869676)[0m                                             'torch_memory': {'context': 'all',
[36m(TaskRunner pid=3869676)[0m                                                              'kw_args': {},
[36m(TaskRunner pid=3869676)[0m                                                              'stack_depth': 32,
[36m(TaskRunner pid=3869676)[0m                                                              'stacks': 'all',
[36m(TaskRunner pid=3869676)[0m                                                              'trace_alloc_max_entries': 100000}},
[36m(TaskRunner pid=3869676)[0m                      'profile_continuous_steps': False,
[36m(TaskRunner pid=3869676)[0m                      'save_path': 'outputs/profile',
[36m(TaskRunner pid=3869676)[0m                      'steps': None,
[36m(TaskRunner pid=3869676)[0m                      'tool': None},
[36m(TaskRunner pid=3869676)[0m  'ray_kwargs': {'ray_init': {'num_cpus': None}, 'timeline_json_file': None},
[36m(TaskRunner pid=3869676)[0m  'reward_model': {'enable': False,
[36m(TaskRunner pid=3869676)[0m                   'enable_resource_pool': False,
[36m(TaskRunner pid=3869676)[0m                   'forward_max_token_len_per_gpu': 32768,
[36m(TaskRunner pid=3869676)[0m                   'launch_reward_fn_async': False,
[36m(TaskRunner pid=3869676)[0m                   'load_weight': True,
[36m(TaskRunner pid=3869676)[0m                   'max_length': None,
[36m(TaskRunner pid=3869676)[0m                   'megatron': {'_target_': 'verl.workers.config.MegatronEngineConfig',
[36m(TaskRunner pid=3869676)[0m                                'context_parallel_size': 1,
[36m(TaskRunner pid=3869676)[0m                                'dist_checkpointing_path': None,
[36m(TaskRunner pid=3869676)[0m                                'expert_model_parallel_size': 1,
[36m(TaskRunner pid=3869676)[0m                                'expert_tensor_parallel_size': 1,
[36m(TaskRunner pid=3869676)[0m                                'override_transformer_config': {'attention_backend': 'flash',
[36m(TaskRunner pid=3869676)[0m                                                                'context_parallel_algo': 'megatron_cp_algo',
[36m(TaskRunner pid=3869676)[0m                                                                'context_parallel_size': 1,
[36m(TaskRunner pid=3869676)[0m                                                                'cp_window_size': 1,
[36m(TaskRunner pid=3869676)[0m                                                                'recompute_granularity': 'full',
[36m(TaskRunner pid=3869676)[0m                                                                'recompute_method': 'uniform',
[36m(TaskRunner pid=3869676)[0m                                                                'recompute_modules': ['core_attn'],
[36m(TaskRunner pid=3869676)[0m                                                                'recompute_num_layers': 1,
[36m(TaskRunner pid=3869676)[0m                                                                'seq_length': 2048,
[36m(TaskRunner pid=3869676)[0m                                                                'swap_optimizer': True,
[36m(TaskRunner pid=3869676)[0m                                                                'use_cp_send_recv_overlap': True,
[36m(TaskRunner pid=3869676)[0m                                                                'use_flash_attn': True,
[36m(TaskRunner pid=3869676)[0m                                                                'use_fused_ring_attention_update': True,
[36m(TaskRunner pid=3869676)[0m                                                                'use_fused_rotary_pos_emb': True,
[36m(TaskRunner pid=3869676)[0m                                                                'use_fused_swiglu': True},
[36m(TaskRunner pid=3869676)[0m                                'param_offload': False,
[36m(TaskRunner pid=3869676)[0m                                'pipeline_model_parallel_size': 1,
[36m(TaskRunner pid=3869676)[0m                                'seed': 42,
[36m(TaskRunner pid=3869676)[0m                                'sequence_parallel': True,
[36m(TaskRunner pid=3869676)[0m                                'tensor_model_parallel_size': 1,
[36m(TaskRunner pid=3869676)[0m                                'use_dist_checkpointing': False,
[36m(TaskRunner pid=3869676)[0m                                'use_distributed_optimizer': False,
[36m(TaskRunner pid=3869676)[0m                                'use_mbridge': False,
[36m(TaskRunner pid=3869676)[0m                                'virtual_pipeline_model_parallel_size': None},
[36m(TaskRunner pid=3869676)[0m                   'micro_batch_size': None,
[36m(TaskRunner pid=3869676)[0m                   'micro_batch_size_per_gpu': None,
[36m(TaskRunner pid=3869676)[0m                   'model': {'external_lib': None,
[36m(TaskRunner pid=3869676)[0m                             'input_tokenizer': '/home/data/Qwen3-32B',
[36m(TaskRunner pid=3869676)[0m                             'path': '~/models/FsfairX-LLaMA3-RM-v0.1',
[36m(TaskRunner pid=3869676)[0m                             'trust_remote_code': False},
[36m(TaskRunner pid=3869676)[0m                   'n_gpus_per_node': 0,
[36m(TaskRunner pid=3869676)[0m                   'nccl_timeout': 600,
[36m(TaskRunner pid=3869676)[0m                   'nnodes': 0,
[36m(TaskRunner pid=3869676)[0m                   'profiler': {'_target_': 'verl.utils.profiler.ProfilerConfig',
[36m(TaskRunner pid=3869676)[0m                                'all_ranks': False,
[36m(TaskRunner pid=3869676)[0m                                'enable': False,
[36m(TaskRunner pid=3869676)[0m                                'ranks': [],
[36m(TaskRunner pid=3869676)[0m                                'save_path': 'outputs/profile',
[36m(TaskRunner pid=3869676)[0m                                'tool': None,
[36m(TaskRunner pid=3869676)[0m                                'tool_config': {'npu': {'_target_': 'verl.utils.profiler.config.NPUToolConfig',
[36m(TaskRunner pid=3869676)[0m                                                        'analysis': True,
[36m(TaskRunner pid=3869676)[0m                                                        'contents': [],
[36m(TaskRunner pid=3869676)[0m                                                        'discrete': False,
[36m(TaskRunner pid=3869676)[0m                                                        'level': 'level1'},
[36m(TaskRunner pid=3869676)[0m                                                'nsys': {'_target_': 'verl.utils.profiler.config.NsightToolConfig',
[36m(TaskRunner pid=3869676)[0m                                                         'discrete': False},
[36m(TaskRunner pid=3869676)[0m                                                'torch': {'_target_': 'verl.utils.profiler.config.TorchProfilerToolConfig',
[36m(TaskRunner pid=3869676)[0m                                                          'step_end': None,
[36m(TaskRunner pid=3869676)[0m                                                          'step_start': 0},
[36m(TaskRunner pid=3869676)[0m                                                'torch_memory': {'_target_': 'verl.utils.profiler.config.TorchMemoryToolConfig',
[36m(TaskRunner pid=3869676)[0m                                                                 'stack_depth': 32,
[36m(TaskRunner pid=3869676)[0m                                                                 'trace_alloc_max_entries': 100000}}},
[36m(TaskRunner pid=3869676)[0m                   'reward_manager': 'naive',
[36m(TaskRunner pid=3869676)[0m                   'sandbox_fusion': {'max_concurrent': 64,
[36m(TaskRunner pid=3869676)[0m                                      'memory_limit_mb': 1024,
[36m(TaskRunner pid=3869676)[0m                                      'url': None},
[36m(TaskRunner pid=3869676)[0m                   'strategy': 'megatron',
[36m(TaskRunner pid=3869676)[0m                   'use_dynamic_bsz': True},
[36m(TaskRunner pid=3869676)[0m  'trainer': {'balance_batch': False,
[36m(TaskRunner pid=3869676)[0m              'critic_warmup': 0,
[36m(TaskRunner pid=3869676)[0m              'default_hdfs_dir': None,
[36m(TaskRunner pid=3869676)[0m              'default_local_dir': 'checkpoints/verl_grpo_example_deepscaler/qwen3_32b_verl_true_weights',
[36m(TaskRunner pid=3869676)[0m              'del_local_ckpt_after_load': False,
[36m(TaskRunner pid=3869676)[0m              'device': 'npu',
[36m(TaskRunner pid=3869676)[0m              'esi_redundant_time': 0,
[36m(TaskRunner pid=3869676)[0m              'experiment_name': 'qwen3_32b_verl_true_weights',
[36m(TaskRunner pid=3869676)[0m              'log_val_generations': 0,
[36m(TaskRunner pid=3869676)[0m              'logger': ['console', 'tensorboard'],
[36m(TaskRunner pid=3869676)[0m              'max_actor_ckpt_to_keep': None,
[36m(TaskRunner pid=3869676)[0m              'max_critic_ckpt_to_keep': None,
[36m(TaskRunner pid=3869676)[0m              'n_gpus_per_node': 16,
[36m(TaskRunner pid=3869676)[0m              'nnodes': 1,
[36m(TaskRunner pid=3869676)[0m              'project_name': 'verl_grpo_example_deepscaler',
[36m(TaskRunner pid=3869676)[0m              'ray_wait_register_center_timeout': 300,
[36m(TaskRunner pid=3869676)[0m              'resume_from_path': None,
[36m(TaskRunner pid=3869676)[0m              'resume_mode': 'auto',
[36m(TaskRunner pid=3869676)[0m              'rollout_data_dir': '/workspace/data/dump',
[36m(TaskRunner pid=3869676)[0m              'rollout_length_dir': '/workspace/data/dump',
[36m(TaskRunner pid=3869676)[0m              'save_freq': -1,
[36m(TaskRunner pid=3869676)[0m              'test_freq': -1,
[36m(TaskRunner pid=3869676)[0m              'total_epochs': 10,
[36m(TaskRunner pid=3869676)[0m              'total_training_steps': None,
[36m(TaskRunner pid=3869676)[0m              'val_before_train': False}}
[36m(TaskRunner pid=3869676)[0m MoEStats enabled: True
[36m(TaskRunner pid=3869676)[0m successfully reset MoEStats for new epoch.
[36m(TaskRunner pid=3869676)[0m WARNING 01-03 21:11:58 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(TaskRunner pid=3869676)[0m [validate_config] All configuration checks passed successfully!
[36m(TaskRunner pid=3869676)[0m using customized reward function 'compute_score' from '/workspace/cann-recipes-train/llm_rl/qwen3/deepscaler.py'
[36m(TaskRunner pid=3869676)[0m using customized reward function 'compute_score' from '/workspace/cann-recipes-train/llm_rl/qwen3/deepscaler.py'
[36m(TaskRunner pid=3869676)[0m Using dataset class: RLHFDataset
[36m(TaskRunner pid=3869676)[0m dataset len: 36283
[36m(TaskRunner pid=3869676)[0m Sampled dataset len: 72 (fraction: 0.002)
[36m(TaskRunner pid=3869676)[0m filter dataset len: 72
[36m(TaskRunner pid=3869676)[0m Using dataset class: RLHFDataset
[36m(TaskRunner pid=3869676)[0m dataset len: 4032
[36m(TaskRunner pid=3869676)[0m Sampled dataset len: 8 (fraction: 0.002)
[36m(TaskRunner pid=3869676)[0m filter dataset len: 8
[36m(TaskRunner pid=3869676)[0m Size of train dataloader: 2, Size of val dataloader: 1
[36m(TaskRunner pid=3869676)[0m Total training steps: 20
[36m(TaskRunner pid=3869676)[0m colocated worker base class <class 'verl.workers.megatron_workers.MegatronWorker'>
[36m(pid=3899502)[0m [Rank 0 | Local Rank 0] 2026-01-03 21:12:11,933 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(pid=3899502)[0m MoEStats enabled: True
[36m(pid=3899502)[0m successfully reset MoEStats for new epoch.
[36m(pid=3899502)[0m WARNING 01-03 21:12:14 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(WorkerDict pid=3899502)[0m [Rank 0 | Local Rank 0] 2026-01-03 21:12:15,806 WARNING [mindspeed.args_utils:70] => Failed from megatron.training import get_args, use mindspeed arguments.
[36m(pid=3899855)[0m [Rank 12 | Local Rank 0] 2026-01-03 21:12:13,766 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.[32m [repeated 15x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(WorkerDict pid=3899855)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xfffee4276020>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xfffee40ebf60>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xfffee40ebf60>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='38771', object_store_name='/tmp/ray/session_2026-01-03_21-11-39_636899_3849868/sockets/plasma_store', raylet_name='/tmp/ray/session_2026-01-03_21-11-39_636899_3849868/sockets/raylet', redis_address='None', metrics_agent_port='59567', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='62090', gcs_address='192.168.0.163:49291', session_name='session_2026-01-03_21-11-39_636899_3849868', temp_dir='/tmp/ray', webui='127.0.0.1:8265', cluster_id='2b452664433e6a17c35de63766d2931f489249196ab4a37525e12f94', startup_token='334', worker_launch_time_ms='1767445923300', node_id='94e5cac3e5140a5f7dbca7e95727ae38c71fecae0ad16289eb59cb13', runtime_env_hash='-955859689', seq_length=2048, use_flash_attn=True)
[36m(pid=3899855)[0m MoEStats enabled: True[32m [repeated 15x across cluster][0m
[36m(pid=3899855)[0m successfully reset MoEStats for new epoch.[32m [repeated 15x across cluster][0m
[36m(pid=3899855)[0m WARNING 01-03 21:12:16 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=3899855)[0m [Rank 12 | Local Rank 0] 2026-01-03 21:12:17,626 WARNING [mindspeed.args_utils:70] => Failed from megatron.training import get_args, use mindspeed arguments.[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=3899502)[0m Model config after override: Qwen3Config {
[36m(WorkerDict pid=3899502)[0m   "architectures": [
[36m(WorkerDict pid=3899502)[0m     "Qwen3ForCausalLM"
[36m(WorkerDict pid=3899502)[0m   ],
[36m(WorkerDict pid=3899502)[0m   "attention_bias": false,
[36m(WorkerDict pid=3899502)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=3899502)[0m   "eos_token_id": 151645,
[36m(WorkerDict pid=3899502)[0m   "head_dim": 128,
[36m(WorkerDict pid=3899502)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=3899502)[0m   "hidden_size": 5120,
[36m(WorkerDict pid=3899502)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=3899502)[0m   "intermediate_size": 25600,
[36m(WorkerDict pid=3899502)[0m   "layer_types": [
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention"
[36m(WorkerDict pid=3899502)[0m   ],
[36m(WorkerDict pid=3899502)[0m   "max_position_embeddings": 40960,
[36m(WorkerDict pid=3899502)[0m   "max_window_layers": 64,
[36m(WorkerDict pid=3899502)[0m   "model_type": "qwen3",
[36m(WorkerDict pid=3899502)[0m   "num_attention_heads": 64,
[36m(WorkerDict pid=3899502)[0m   "num_hidden_layers": 64,
[36m(WorkerDict pid=3899502)[0m   "num_key_value_heads": 8,
[36m(WorkerDict pid=3899502)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=3899502)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=3899502)[0m   "rope_scaling": null,
[36m(WorkerDict pid=3899502)[0m   "rope_theta": 1000000,
[36m(WorkerDict pid=3899502)[0m   "sliding_window": null,
[36m(WorkerDict pid=3899502)[0m   "tie_word_embeddings": false,
[36m(WorkerDict pid=3899502)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=3899502)[0m   "transformers_version": "4.55.0",
[36m(WorkerDict pid=3899502)[0m   "use_cache": true,
[36m(WorkerDict pid=3899502)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=3899502)[0m   "vocab_size": 151936
[36m(WorkerDict pid=3899502)[0m }
[36m(WorkerDict pid=3899502)[0m 
[36m(WorkerDict pid=3899502)[0m Overridden TransformerConfig init config: {'num_layers': 64, 'hidden_size': 5120, 'num_attention_heads': 64, 'num_query_groups': 8, 'ffn_hidden_size': 25600, 'attention_dropout': 0.0, 'hidden_dropout': 0.0, 'kv_channels': 128, 'layernorm_epsilon': 1e-06, 'add_bias_linear': False, 'activation_func': <function silu at 0xffff14636020>, 'normalization': 'RMSNorm', 'gated_linear_unit': True, 'pipeline_dtype': torch.bfloat16, 'params_dtype': torch.bfloat16, 'bf16': True, 'tensor_model_parallel_size': 8, 'pipeline_model_parallel_size': 2, 'expert_model_parallel_size': 1, 'expert_tensor_parallel_size': 1, 'virtual_pipeline_model_parallel_size': None, 'context_parallel_size': 1, 'overlap_p2p_comm': False, 'batch_p2p_comm': False, 'sequence_parallel': True, 'variable_seq_lengths': True, 'masked_softmax_fusion': True, 'moe_token_dispatcher_type': 'alltoall', 'use_cpu_initialization': False, 'add_qkv_bias': False, 'qk_layernorm': True, 'recompute_granularity': 'full', 'recompute_modules': ['core_attn'], 'recompute_method': 'uniform', 'recompute_num_layers': 1, 'attention_backend': <AttnBackend.flash: 1>}
[36m(WorkerDict pid=3899856)[0m  > number of parameters on (tensor, pipeline) model parallel rank (7, 1): 2047931392
[36m(WorkerDict pid=3899858)[0m load ref weight start
[36m(WorkerDict pid=3899858)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=3899502)[0m loading embeddings...
[36m(WorkerDict pid=3899502)[0m loading layer #0, with layer_name model.layers.0...
[36m(WorkerDict pid=3899502)[0m loading layer #1, with layer_name model.layers.1...
[36m(WorkerDict pid=3899502)[0m loading layer #2, with layer_name model.layers.2...
[36m(WorkerDict pid=3899502)[0m loading layer #3, with layer_name model.layers.3...
[36m(WorkerDict pid=3899699)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff20636020>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff204a3f60>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff204a3f60>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='38771', object_store_name='/tmp/ray/session_2026-01-03_21-11-39_636899_3849868/sockets/plasma_store', raylet_name='/tmp/ray/session_2026-01-03_21-11-39_636899_3849868/sockets/raylet', redis_address='None', metrics_agent_port='59567', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='62090', gcs_address='192.168.0.163:49291', session_name='session_2026-01-03_21-11-39_636899_3849868', temp_dir='/tmp/ray', webui='127.0.0.1:8265', cluster_id='2b452664433e6a17c35de63766d2931f489249196ab4a37525e12f94', startup_token='329', worker_launch_time_ms='1767445923190', node_id='94e5cac3e5140a5f7dbca7e95727ae38c71fecae0ad16289eb59cb13', runtime_env_hash='-662445072', seq_length=2048, use_flash_attn=True)[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=3899699)[0m  > number of parameters on (tensor, pipeline) model parallel rank (7, 0): 2047926272[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=3899699)[0m load ref weight start[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=3899699)[0m load from local dir /home/data/Qwen3-32B[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=3899502)[0m loading layer #4, with layer_name model.layers.4...
[36m(WorkerDict pid=3899502)[0m loading layer #5, with layer_name model.layers.5...
[36m(WorkerDict pid=3899502)[0m loading layer #6, with layer_name model.layers.6...
[36m(WorkerDict pid=3899502)[0m loading layer #7, with layer_name model.layers.7...
[36m(WorkerDict pid=3899502)[0m loading layer #8, with layer_name model.layers.8...
[36m(WorkerDict pid=3899502)[0m loading layer #9, with layer_name model.layers.9...
[36m(WorkerDict pid=3899502)[0m loading layer #10, with layer_name model.layers.10...
[36m(WorkerDict pid=3899502)[0m loading layer #11, with layer_name model.layers.11...
[36m(WorkerDict pid=3899502)[0m loading layer #12, with layer_name model.layers.12...
[36m(WorkerDict pid=3899502)[0m loading layer #13, with layer_name model.layers.13...
[36m(WorkerDict pid=3899502)[0m loading layer #14, with layer_name model.layers.14...
[36m(WorkerDict pid=3899502)[0m loading layer #15, with layer_name model.layers.15...
[36m(WorkerDict pid=3899502)[0m loading layer #16, with layer_name model.layers.16...
[36m(WorkerDict pid=3899502)[0m loading layer #17, with layer_name model.layers.17...
[36m(WorkerDict pid=3899502)[0m loading layer #18, with layer_name model.layers.18...
[36m(WorkerDict pid=3899502)[0m loading layer #19, with layer_name model.layers.19...
[36m(WorkerDict pid=3899502)[0m loading layer #20, with layer_name model.layers.20...
[36m(WorkerDict pid=3899502)[0m loading layer #21, with layer_name model.layers.21...
[36m(WorkerDict pid=3899502)[0m loading layer #22, with layer_name model.layers.22...
[36m(WorkerDict pid=3899502)[0m loading layer #23, with layer_name model.layers.23...
[36m(WorkerDict pid=3899502)[0m loading layer #24, with layer_name model.layers.24...
[36m(WorkerDict pid=3899502)[0m loading layer #25, with layer_name model.layers.25...
[36m(WorkerDict pid=3899502)[0m loading layer #26, with layer_name model.layers.26...
[36m(WorkerDict pid=3899502)[0m loading layer #27, with layer_name model.layers.27...
[36m(WorkerDict pid=3899502)[0m loading layer #28, with layer_name model.layers.28...
[36m(WorkerDict pid=3899502)[0m loading layer #29, with layer_name model.layers.29...
[36m(WorkerDict pid=3899502)[0m loading layer #30, with layer_name model.layers.30...
[36m(WorkerDict pid=3899502)[0m loading layer #31, with layer_name model.layers.31...
[36m(WorkerDict pid=3899502)[0m loading layer #32, with layer_name model.layers.32...
[36m(WorkerDict pid=3899502)[0m loading layer #33, with layer_name model.layers.33...
[36m(WorkerDict pid=3899502)[0m loading layer #34, with layer_name model.layers.34...
[36m(WorkerDict pid=3899502)[0m loading layer #35, with layer_name model.layers.35...
[36m(WorkerDict pid=3899502)[0m loading layer #36, with layer_name model.layers.36...
[36m(WorkerDict pid=3899502)[0m loading layer #37, with layer_name model.layers.37...
[36m(WorkerDict pid=3899502)[0m loading layer #38, with layer_name model.layers.38...
[36m(WorkerDict pid=3899502)[0m loading layer #39, with layer_name model.layers.39...
[36m(WorkerDict pid=3899502)[0m loading layer #40, with layer_name model.layers.40...
[36m(WorkerDict pid=3899502)[0m loading layer #41, with layer_name model.layers.41...
[36m(WorkerDict pid=3899502)[0m loading layer #42, with layer_name model.layers.42...
[36m(WorkerDict pid=3899502)[0m loading layer #43, with layer_name model.layers.43...
[36m(WorkerDict pid=3899502)[0m loading layer #44, with layer_name model.layers.44...
[36m(WorkerDict pid=3899502)[0m loading layer #45, with layer_name model.layers.45...
[36m(WorkerDict pid=3899502)[0m loading layer #46, with layer_name model.layers.46...
[36m(WorkerDict pid=3899502)[0m loading layer #47, with layer_name model.layers.47...
[36m(WorkerDict pid=3899502)[0m loading layer #48, with layer_name model.layers.48...
[36m(WorkerDict pid=3899502)[0m loading layer #49, with layer_name model.layers.49...
[36m(WorkerDict pid=3899502)[0m loading layer #50, with layer_name model.layers.50...
[36m(WorkerDict pid=3899502)[0m loading layer #51, with layer_name model.layers.51...
[36m(WorkerDict pid=3899502)[0m loading layer #52, with layer_name model.layers.52...
[36m(WorkerDict pid=3899502)[0m loading layer #53, with layer_name model.layers.53...
[36m(WorkerDict pid=3899502)[0m loading layer #54, with layer_name model.layers.54...
[36m(WorkerDict pid=3899502)[0m loading layer #55, with layer_name model.layers.55...
[36m(WorkerDict pid=3899502)[0m loading layer #56, with layer_name model.layers.56...
[36m(WorkerDict pid=3899502)[0m loading layer #57, with layer_name model.layers.57...
[36m(WorkerDict pid=3899502)[0m loading layer #58, with layer_name model.layers.58...
[36m(WorkerDict pid=3899502)[0m loading layer #59, with layer_name model.layers.59...
[36m(WorkerDict pid=3899502)[0m loading layer #60, with layer_name model.layers.60...
[36m(WorkerDict pid=3899502)[0m loading layer #61, with layer_name model.layers.61...
[36m(WorkerDict pid=3899502)[0m loading layer #62, with layer_name model.layers.62...
[36m(WorkerDict pid=3899502)[0m loading layer #63, with layer_name model.layers.63...
[36m(WorkerDict pid=3899502)[0m loading final layernorm...
[36m(WorkerDict pid=3899502)[0m loading lm_head...
[36m(WorkerDict pid=3899502)[0m loading megatron ckpt done, time elapsed 119.82274007797241s
[36m(WorkerDict pid=3899535)[0m [Warining] Because actor tp size == 1, set sp to False
[36m(WorkerDict pid=3899535)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff10172020>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xfffeec6bbf60>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xfffeec6bbf60>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='38771', object_store_name='/tmp/ray/session_2026-01-03_21-11-39_636899_3849868/sockets/plasma_store', raylet_name='/tmp/ray/session_2026-01-03_21-11-39_636899_3849868/sockets/raylet', redis_address='None', metrics_agent_port='59567', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='62090', gcs_address='192.168.0.163:49291', session_name='session_2026-01-03_21-11-39_636899_3849868', temp_dir='/tmp/ray', webui='127.0.0.1:8265', cluster_id='2b452664433e6a17c35de63766d2931f489249196ab4a37525e12f94', startup_token='323', worker_launch_time_ms='1767445923077', node_id='94e5cac3e5140a5f7dbca7e95727ae38c71fecae0ad16289eb59cb13', runtime_env_hash='-273617316', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=3899612)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff10166020>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xfffef06abf60>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xfffef06abf60>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='38771', object_store_name='/tmp/ray/session_2026-01-03_21-11-39_636899_3849868/sockets/plasma_store', raylet_name='/tmp/ray/session_2026-01-03_21-11-39_636899_3849868/sockets/raylet', redis_address='None', metrics_agent_port='59567', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='62090', gcs_address='192.168.0.163:49291', session_name='session_2026-01-03_21-11-39_636899_3849868', temp_dir='/tmp/ray', webui='127.0.0.1:8265', cluster_id='2b452664433e6a17c35de63766d2931f489249196ab4a37525e12f94', startup_token='326', worker_launch_time_ms='1767445923131', node_id='94e5cac3e5140a5f7dbca7e95727ae38c71fecae0ad16289eb59cb13', runtime_env_hash='1179919171', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=3899502)[0m Model config after override: Qwen3Config {
[36m(WorkerDict pid=3899502)[0m   "architectures": [
[36m(WorkerDict pid=3899502)[0m     "Qwen3ForCausalLM"
[36m(WorkerDict pid=3899502)[0m   ],
[36m(WorkerDict pid=3899502)[0m   "attention_bias": false,
[36m(WorkerDict pid=3899502)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=3899502)[0m   "eos_token_id": 151645,
[36m(WorkerDict pid=3899502)[0m   "head_dim": 128,
[36m(WorkerDict pid=3899502)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=3899502)[0m   "hidden_size": 5120,
[36m(WorkerDict pid=3899502)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=3899502)[0m   "intermediate_size": 25600,
[36m(WorkerDict pid=3899502)[0m   "layer_types": [
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention",
[36m(WorkerDict pid=3899502)[0m     "full_attention"
[36m(WorkerDict pid=3899502)[0m   ],
[36m(WorkerDict pid=3899502)[0m   "max_position_embeddings": 40960,
[36m(WorkerDict pid=3899502)[0m   "max_window_layers": 64,
[36m(WorkerDict pid=3899502)[0m   "model_type": "qwen3",
[36m(WorkerDict pid=3899502)[0m   "num_attention_heads": 64,
[36m(WorkerDict pid=3899502)[0m   "num_hidden_layers": 64,
[36m(WorkerDict pid=3899502)[0m   "num_key_value_heads": 8,
[36m(WorkerDict pid=3899502)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=3899502)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=3899502)[0m   "rope_scaling": null,
[36m(WorkerDict pid=3899502)[0m   "rope_theta": 1000000,
[36m(WorkerDict pid=3899502)[0m   "sliding_window": null,
[36m(WorkerDict pid=3899502)[0m   "tie_word_embeddings": false,
[36m(WorkerDict pid=3899502)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=3899502)[0m   "transformers_version": "4.55.0",
[36m(WorkerDict pid=3899502)[0m   "use_cache": true,
[36m(WorkerDict pid=3899502)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=3899502)[0m   "vocab_size": 151936
[36m(WorkerDict pid=3899502)[0m }
[36m(WorkerDict pid=3899502)[0m 
[36m(WorkerDict pid=3899502)[0m Overridden TransformerConfig init config: {'num_layers': 64, 'hidden_size': 5120, 'num_attention_heads': 64, 'num_query_groups': 8, 'ffn_hidden_size': 25600, 'attention_dropout': 0.0, 'hidden_dropout': 0.0, 'kv_channels': 128, 'layernorm_epsilon': 1e-06, 'add_bias_linear': False, 'activation_func': <function silu at 0xffff14636020>, 'normalization': 'RMSNorm', 'gated_linear_unit': True, 'pipeline_dtype': torch.bfloat16, 'params_dtype': torch.bfloat16, 'bf16': True, 'tensor_model_parallel_size': 8, 'pipeline_model_parallel_size': 2, 'expert_model_parallel_size': 1, 'expert_tensor_parallel_size': 1, 'virtual_pipeline_model_parallel_size': None, 'context_parallel_size': 1, 'overlap_p2p_comm': False, 'batch_p2p_comm': False, 'sequence_parallel': True, 'variable_seq_lengths': True, 'masked_softmax_fusion': True, 'moe_token_dispatcher_type': 'alltoall', 'use_cpu_initialization': False, 'add_qkv_bias': False, 'qk_layernorm': True, 'recompute_granularity': 'full', 'recompute_modules': ['core_attn'], 'recompute_method': 'uniform', 'recompute_num_layers': 1, 'attention_backend': <AttnBackend.flash: 1>}
[36m(WorkerDict pid=3899502)[0m  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 2047926272
[36m(WorkerDict pid=3899502)[0m [Rank 0 | Local Rank 0] 2026-01-03 21:14:39,017 INFO [megatron.core.distributed.distributed_data_parallel:532] => Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=True, overlap_grad_reduce=False, overlap_param_gather=False, align_param_gather=False, use_distributed_optimizer=True, num_distributed_optimizer_instances=1, check_for_nan_in_grad=False, check_for_large_grads=False, bucket_size=None, pad_buckets_for_high_nccl_busbw=False, average_in_collective=False, fp8_param_gather=False, use_custom_fsdp=False, data_parallel_sharding_strategy='no_shard', gradient_reduce_div_fusion=True, suggested_communication_unit_size=None, preserve_fp32_weights=True, keep_fp8_transpose_cache_when_using_custom_fsdp=False)
[36m(WorkerDict pid=3899502)[0m [Rank 0 | Local Rank 0] 2026-01-03 21:14:39,052 INFO [megatron.core.distributed.param_and_grad_buffer:553] => Number of buckets for gradient all-reduce / reduce-scatter: 1
[36m(WorkerDict pid=3899502)[0m Params for bucket 1 (2047926272 elements, 2047926272 padded size):
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.29.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.26.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.25.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.23.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.21.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.19.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.17.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.9.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.8.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.8.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.8.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.14.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.15.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.10.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.22.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.30.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.31.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.28.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.26.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.23.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.19.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.15.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.15.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.14.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.4.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.1.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.6.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.2.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.25.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.30.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.31.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.27.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.20.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.19.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.13.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.11.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.10.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.8.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.7.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.5.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.10.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.26.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.25.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.31.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.29.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.21.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.19.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.18.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.15.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.13.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.9.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.7.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.1.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.embedding.word_embeddings.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.29.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.28.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.27.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.25.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.22.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.20.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.17.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.4.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.1.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.1.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.4.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.10.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.0.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.3.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.15.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.6.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.26.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.28.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.23.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.24.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.17.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.16.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.13.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.12.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.11.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.8.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.3.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.11.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.1.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.9.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.7.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.24.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.29.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.31.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.27.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.21.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.20.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.20.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.17.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.16.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.14.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.7.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.0.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.8.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.3.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.26.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.25.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.22.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.30.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.29.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.28.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.20.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.18.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.17.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.15.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.5.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.2.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.9.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.14.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.31.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.28.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.23.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.24.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.22.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.20.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.19.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.18.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.13.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.6.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.2.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.2.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.4.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.0.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.9.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.11.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.13.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.5.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.4.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.24.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.29.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.28.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.23.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.25.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.22.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.20.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.15.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.12.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.10.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.6.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.18.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.5.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.0.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.11.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.5.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.27.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.26.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.22.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.30.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.29.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.29.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.28.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.26.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.24.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.21.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.14.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.7.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.3.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.9.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.18.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.4.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.3.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.27.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.25.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.22.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.30.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.31.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.21.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.19.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.18.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.17.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.12.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.0.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.12.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.23.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.19.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.18.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.16.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.14.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.13.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.10.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.7.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.9.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.3.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.22.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.30.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.28.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.27.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.21.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.17.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.16.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.13.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.12.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.11.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.11.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.0.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.14.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.8.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.2.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.16.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.23.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.26.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.30.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.31.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.23.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.27.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.21.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.18.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.16.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.16.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.12.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.1.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.6.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.2.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.12.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.6.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.24.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.27.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.24.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.25.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.30.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.31.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.21.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.20.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.19.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.16.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.10.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899502)[0m 	module.decoder.layers.5.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899502)[0m actor_module: 1
[36m(WorkerDict pid=3899502)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.4.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.10.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.30.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.31.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.29.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.25.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.21.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.18.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.17.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.13.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.11.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.4.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.0.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.14.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.2.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.10.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.23.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.30.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.31.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.27.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.19.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.15.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.12.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.9.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.7.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.3.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.2.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.11.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.7.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.4.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.26.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.23.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.30.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.28.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.27.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.24.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.22.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.20.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.19.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.18.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.16.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.4.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.14.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.25.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.23.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.27.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.31.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.28.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.24.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.21.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.20.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.19.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.17.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.1.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.7.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.9.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.8.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.3.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.8.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.0.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.30.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.31.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.26.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.22.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.18.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.14.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.13.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.8.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.6.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.5.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.9.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.4.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.16.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.14.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.31.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.29.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.28.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.24.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.20.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.16.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.16.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.13.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.11.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.10.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.6.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.1.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.14.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.16.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.5.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.25.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.23.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.27.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.28.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.24.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.21.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.20.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.19.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.17.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.16.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.17.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.12.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.5.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.10.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.7.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.2.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.25.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.22.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.26.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.24.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.29.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.28.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.21.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.20.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.18.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.17.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.3.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.4.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.1.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.3.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.6.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.11.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.2.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.29.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.31.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.27.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.23.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.19.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.12.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.10.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.9.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.8.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.0.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.15.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.13.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.6.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.2.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.12.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.0.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.30.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.25.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.21.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.17.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.14.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.10.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.9.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.8.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.7.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.15.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.1.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.22.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.26.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.31.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.29.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.28.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.25.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.21.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.20.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.18.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.1.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.0.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.7.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.3.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.6.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.17.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.25.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.26.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.23.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.29.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.29.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.27.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.22.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.21.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.19.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.18.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.10.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.5.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.5.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.6.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.8.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.30.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.28.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.24.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.20.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.15.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.15.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.13.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.12.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.11.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.11.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.8.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.3.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.26.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.30.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.31.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.22.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.18.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.14.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.13.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.9.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.16.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.15.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.1.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.15.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.12.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.0.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.26.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.23.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.final_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.29.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.27.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.25.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.22.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.21.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.19.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.18.self_attention.linear_qkv.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.11.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.16.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.12.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.5.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.2.mlp.linear_fc2.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.24.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.23.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.27.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.26.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.30.mlp.linear_fc1.weight
[36m(WorkerDict pid=3899728)[0m 	module.output_layer.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.28.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.22.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.20.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.19.self_attention.k_layernorm.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.9.self_attention.linear_proj.weight
[36m(WorkerDict pid=3899728)[0m 	module.decoder.layers.13.self_attention.q_layernorm.weight
[36m(WorkerDict pid=3899502)[0m loading embeddings...
[36m(WorkerDict pid=3899502)[0m loading layer #0, with layer_name model.layers.0...
[36m(WorkerDict pid=3899502)[0m loading layer #1, with layer_name model.layers.1...
[36m(WorkerDict pid=3899502)[0m loading layer #2, with layer_name model.layers.2...
[36m(WorkerDict pid=3899502)[0m [Warining] Because actor tp size == 1, set sp to False[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=3899502)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff14636020>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff144a3f60>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff144a3f60>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='38771', object_store_name='/tmp/ray/session_2026-01-03_21-11-39_636899_3849868/sockets/plasma_store', raylet_name='/tmp/ray/session_2026-01-03_21-11-39_636899_3849868/sockets/raylet', redis_address='None', metrics_agent_port='59567', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='62090', gcs_address='192.168.0.163:49291', session_name='session_2026-01-03_21-11-39_636899_3849868', temp_dir='/tmp/ray', webui='127.0.0.1:8265', cluster_id='2b452664433e6a17c35de63766d2931f489249196ab4a37525e12f94', startup_token='322', worker_launch_time_ms='1767445923051', node_id='94e5cac3e5140a5f7dbca7e95727ae38c71fecae0ad16289eb59cb13', runtime_env_hash='1256871479', seq_length=2048, use_flash_attn=True)[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=3899502)[0m loading layer #3, with layer_name model.layers.3...
[36m(WorkerDict pid=3899502)[0m loading layer #4, with layer_name model.layers.4...
[36m(WorkerDict pid=3899502)[0m loading layer #5, with layer_name model.layers.5...
[36m(WorkerDict pid=3899502)[0m loading layer #6, with layer_name model.layers.6...
[36m(WorkerDict pid=3899502)[0m loading layer #7, with layer_name model.layers.7...
[36m(WorkerDict pid=3899502)[0m loading layer #8, with layer_name model.layers.8...
[36m(WorkerDict pid=3899502)[0m loading layer #9, with layer_name model.layers.9...
[36m(WorkerDict pid=3899502)[0m loading layer #10, with layer_name model.layers.10...
[36m(WorkerDict pid=3899502)[0m loading layer #11, with layer_name model.layers.11...
[36m(WorkerDict pid=3899502)[0m loading layer #12, with layer_name model.layers.12...
[36m(WorkerDict pid=3899502)[0m loading layer #13, with layer_name model.layers.13...
[36m(WorkerDict pid=3899502)[0m loading layer #14, with layer_name model.layers.14...
[36m(WorkerDict pid=3899502)[0m loading layer #15, with layer_name model.layers.15...
[36m(WorkerDict pid=3899502)[0m loading layer #16, with layer_name model.layers.16...
[36m(WorkerDict pid=3899588)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff00742020>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff005b3f60>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff005b3f60>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='38771', object_store_name='/tmp/ray/session_2026-01-03_21-11-39_636899_3849868/sockets/plasma_store', raylet_name='/tmp/ray/session_2026-01-03_21-11-39_636899_3849868/sockets/raylet', redis_address='None', metrics_agent_port='59567', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='62090', gcs_address='192.168.0.163:49291', session_name='session_2026-01-03_21-11-39_636899_3849868', temp_dir='/tmp/ray', webui='127.0.0.1:8265', cluster_id='2b452664433e6a17c35de63766d2931f489249196ab4a37525e12f94', startup_token='325', worker_launch_time_ms='1767445923113', node_id='94e5cac3e5140a5f7dbca7e95727ae38c71fecae0ad16289eb59cb13', runtime_env_hash='580958233', seq_length=2048, use_flash_attn=True)[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=3899502)[0m loading layer #17, with layer_name model.layers.17...
[36m(WorkerDict pid=3899855)[0m  > number of parameters on (tensor, pipeline) model parallel rank (4, 1): 2047931392[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=3899728)[0m [Rank 8 | Local Rank 0] 2026-01-03 21:14:39,081 INFO [megatron.core.distributed.param_and_grad_buffer:553] => Number of buckets for gradient all-reduce / reduce-scatter: 1
[36m(WorkerDict pid=3899728)[0m Params for bucket 1 (2047931392 elements, 2047931392 padded size):
[36m(WorkerDict pid=3899502)[0m loading layer #18, with layer_name model.layers.18...
[36m(WorkerDict pid=3899855)[0m actor_module: 1[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=3899855)[0m load from local dir /home/data/Qwen3-32B[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=3899502)[0m loading layer #19, with layer_name model.layers.19...
[36m(WorkerDict pid=3899502)[0m loading layer #20, with layer_name model.layers.20...
[36m(WorkerDict pid=3899502)[0m loading layer #21, with layer_name model.layers.21...
[36m(WorkerDict pid=3899502)[0m loading layer #22, with layer_name model.layers.22...
[36m(WorkerDict pid=3899502)[0m loading layer #23, with layer_name model.layers.23...
[36m(WorkerDict pid=3899502)[0m loading layer #24, with layer_name model.layers.24...
[36m(WorkerDict pid=3899502)[0m loading layer #25, with layer_name model.layers.25...
[36m(WorkerDict pid=3899502)[0m loading layer #26, with layer_name model.layers.26...
[36m(WorkerDict pid=3899502)[0m loading layer #27, with layer_name model.layers.27...
[36m(WorkerDict pid=3899502)[0m loading layer #28, with layer_name model.layers.28...
[36m(WorkerDict pid=3899502)[0m loading layer #29, with layer_name model.layers.29...
[36m(WorkerDict pid=3899502)[0m loading layer #30, with layer_name model.layers.30...
[36m(WorkerDict pid=3899502)[0m loading layer #31, with layer_name model.layers.31...
[36m(WorkerDict pid=3899502)[0m loading layer #32, with layer_name model.layers.32...
[36m(WorkerDict pid=3899502)[0m loading layer #33, with layer_name model.layers.33...
[36m(WorkerDict pid=3899502)[0m loading layer #34, with layer_name model.layers.34...
[36m(WorkerDict pid=3899502)[0m loading layer #35, with layer_name model.layers.35...
[36m(WorkerDict pid=3899502)[0m loading layer #36, with layer_name model.layers.36...
[36m(WorkerDict pid=3899502)[0m loading layer #37, with layer_name model.layers.37...
[36m(WorkerDict pid=3899502)[0m loading layer #38, with layer_name model.layers.38...
[36m(WorkerDict pid=3899502)[0m loading layer #39, with layer_name model.layers.39...
[36m(WorkerDict pid=3899502)[0m loading layer #40, with layer_name model.layers.40...
[36m(WorkerDict pid=3899502)[0m loading layer #41, with layer_name model.layers.41...
[36m(WorkerDict pid=3899502)[0m loading layer #42, with layer_name model.layers.42...
[36m(WorkerDict pid=3899502)[0m loading layer #43, with layer_name model.layers.43...
[36m(WorkerDict pid=3899502)[0m loading layer #44, with layer_name model.layers.44...
[36m(WorkerDict pid=3899502)[0m loading layer #45, with layer_name model.layers.45...
[36m(WorkerDict pid=3899502)[0m loading layer #46, with layer_name model.layers.46...
[36m(WorkerDict pid=3899502)[0m loading layer #47, with layer_name model.layers.47...
[36m(WorkerDict pid=3899502)[0m loading layer #48, with layer_name model.layers.48...
[36m(WorkerDict pid=3899502)[0m loading layer #49, with layer_name model.layers.49...
[36m(WorkerDict pid=3899502)[0m loading layer #50, with layer_name model.layers.50...
[36m(WorkerDict pid=3899502)[0m loading layer #51, with layer_name model.layers.51...
[36m(WorkerDict pid=3899502)[0m loading layer #52, with layer_name model.layers.52...
[36m(WorkerDict pid=3899502)[0m loading layer #53, with layer_name model.layers.53...
[36m(WorkerDict pid=3899502)[0m loading layer #54, with layer_name model.layers.54...
[36m(WorkerDict pid=3899502)[0m loading layer #55, with layer_name model.layers.55...
[36m(WorkerDict pid=3899502)[0m loading layer #56, with layer_name model.layers.56...
[36m(WorkerDict pid=3899502)[0m loading layer #57, with layer_name model.layers.57...
[36m(WorkerDict pid=3899502)[0m loading layer #58, with layer_name model.layers.58...
[36m(WorkerDict pid=3899502)[0m loading layer #59, with layer_name model.layers.59...
[36m(WorkerDict pid=3899502)[0m loading layer #60, with layer_name model.layers.60...
[36m(WorkerDict pid=3899502)[0m loading layer #61, with layer_name model.layers.61...
[36m(WorkerDict pid=3899502)[0m loading layer #62, with layer_name model.layers.62...
[36m(WorkerDict pid=3899502)[0m loading layer #63, with layer_name model.layers.63...
[36m(WorkerDict pid=3899502)[0m loading final layernorm...
[36m(WorkerDict pid=3899502)[0m loading lm_head...
[36m(WorkerDict pid=3899502)[0m loading megatron ckpt done, time elapsed 16.201906442642212s
[36m(WorkerDict pid=3899535)[0m [Rank 0] swap optimizer: 2047926272 (23436.65625 MB)/2047926272
[36m(WorkerDict pid=3899668)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff08636020>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff084a7f60>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff084a7f60>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='38771', object_store_name='/tmp/ray/session_2026-01-03_21-11-39_636899_3849868/sockets/plasma_store', raylet_name='/tmp/ray/session_2026-01-03_21-11-39_636899_3849868/sockets/raylet', redis_address='None', metrics_agent_port='59567', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='62090', gcs_address='192.168.0.163:49291', session_name='session_2026-01-03_21-11-39_636899_3849868', temp_dir='/tmp/ray', webui='127.0.0.1:8265', cluster_id='2b452664433e6a17c35de63766d2931f489249196ab4a37525e12f94', startup_token='328', worker_launch_time_ms='1767445923170', node_id='94e5cac3e5140a5f7dbca7e95727ae38c71fecae0ad16289eb59cb13', runtime_env_hash='962638844', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=3899668)[0m WARNING 01-03 21:14:58 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(WorkerDict pid=3899668)[0m WARNING 01-03 21:14:58 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(WorkerDict pid=3899668)[0m WARNING 01-03 21:14:58 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(WorkerDict pid=3899668)[0m WARNING 01-03 21:14:58 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(WorkerDict pid=3899668)[0m WARNING 01-03 21:14:58 [registry.py:582] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
[36m(WorkerDict pid=3899668)[0m WARNING 01-03 21:14:58 [registry.py:582] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=3899668)[0m WARNING 01-03 21:14:58 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=3899668)[0m WARNING 01-03 21:14:58 [registry.py:582] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
[36m(WorkerDict pid=3899668)[0m WARNING 01-03 21:14:58 [registry.py:582] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
[36m(WorkerDict pid=3899668)[0m WARNING 01-03 21:14:58 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(WorkerDict pid=3899502)[0m DistributedDataParallel contains 2.05B parameters
[36m(WorkerDict pid=3899502)[0m optimizer config after override: {'optimizer': 'adam', 'lr': 1e-06, 'min_lr': 0.0, 'clip_grad': 10000, 'weight_decay': 0.01, 'bf16': True, 'params_dtype': torch.bfloat16, 'use_distributed_optimizer': True}
[36m(WorkerDict pid=3899502)[0m [Rank 0 | Local Rank 0] 2026-01-03 21:14:59,769 INFO [megatron.core.optimizer:532] => Setting up optimizer with config OptimizerConfig(optimizer='adam', lr=1e-06, min_lr=0.0, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=True, params_dtype=torch.bfloat16, use_precision_aware_optimizer=False, main_grads_dtype=torch.float32, main_params_dtype=torch.float32, exp_avg_dtype=torch.float32, exp_avg_sq_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=True, overlap_param_gather_with_optimizer_step=False, optimizer_cpu_offload=False, optimizer_offload_fraction=0.0, use_torch_optimizer_for_cpu_offload=False, overlap_cpu_optimizer_d2h_h2d=False, pin_cpu_grads=True, pin_cpu_params=True, clip_grad=10000, log_num_zeros_in_grad=False, barrier_with_L1_time=False, timers=None, config_logger_dir='')
[36m(WorkerDict pid=3899502)[0m [Rank 0 | Local Rank 0] 2026-01-03 21:15:00,247 INFO [megatron.core.optimizer_param_scheduler:532] => > learning rate decay style: constant
[36m(WorkerDict pid=3899502)[0m [Rank 0] swap optimizer: 2047926272 (23436.65625 MB)/2047926272[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=3899502)[0m WARNING 01-03 21:15:28 [core.py:112] Using configured V1 scheduler class vllm_ascend.core.scheduler.AscendScheduler. This scheduler interface is not public and compatibility may not be maintained.
[36m(WorkerDict pid=3899502)[0m max_num_running_reqs: 128 max_num_scheduled_tokens: 36864 max_model_len: 36864
[36m(WorkerDict pid=3899502)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff14636020>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff144a3f60>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff144a3f60>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='38771', object_store_name='/tmp/ray/session_2026-01-03_21-11-39_636899_3849868/sockets/plasma_store', raylet_name='/tmp/ray/session_2026-01-03_21-11-39_636899_3849868/sockets/raylet', redis_address='None', metrics_agent_port='59567', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='62090', gcs_address='192.168.0.163:49291', session_name='session_2026-01-03_21-11-39_636899_3849868', temp_dir='/tmp/ray', webui='127.0.0.1:8265', cluster_id='2b452664433e6a17c35de63766d2931f489249196ab4a37525e12f94', startup_token='322', worker_launch_time_ms='1767445923051', node_id='94e5cac3e5140a5f7dbca7e95727ae38c71fecae0ad16289eb59cb13', runtime_env_hash='1256871479', seq_length=2048, use_flash_attn=True)[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=3899502)[0m WARNING 01-03 21:15:02 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.[32m [repeated 135x across cluster][0m
[36m(WorkerDict pid=3899502)[0m WARNING 01-03 21:15:02 [registry.py:582] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=3899699)[0m [Rank 7 | Local Rank 0] 2026-01-03 21:15:29,232 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py:265] => The 'logprobs' parameter is incompatible with Speculative Decoding and has been disabled.
[36m(WorkerDict pid=3899699)[0m kwargs: {'n': 1, 'max_tokens': 34816, 'repetition_penalty': 1.0, 'detokenize': False, 'temperature': 0.9, 'top_k': -1, 'top_p': 1.0, 'ignore_eos': False}
[36m(WorkerDict pid=3899502)[0m Overridden TransformerConfig init config: {'num_layers': 64, 'hidden_size': 5120, 'num_attention_heads': 64, 'num_query_groups': 8, 'ffn_hidden_size': 25600, 'attention_dropout': 0.0, 'hidden_dropout': 0.0, 'kv_channels': 128, 'layernorm_epsilon': 1e-06, 'add_bias_linear': False, 'activation_func': <function silu at 0xffff14636020>, 'normalization': 'RMSNorm', 'gated_linear_unit': True, 'pipeline_dtype': torch.bfloat16, 'params_dtype': torch.bfloat16, 'bf16': True, 'tensor_model_parallel_size': 8, 'pipeline_model_parallel_size': 2, 'expert_model_parallel_size': 1, 'expert_tensor_parallel_size': 1, 'virtual_pipeline_model_parallel_size': None, 'context_parallel_size': 1, 'overlap_p2p_comm': False, 'batch_p2p_comm': False, 'sequence_parallel': True, 'variable_seq_lengths': True, 'masked_softmax_fusion': True, 'moe_token_dispatcher_type': 'alltoall', 'use_cpu_initialization': False, 'add_qkv_bias': False, 'qk_layernorm': True}
[36m(TaskRunner pid=3869676)[0m [TaskRunner] Before trainer.fit()
[36m(TaskRunner pid=3869676)[0m Saving tensorboard log to tensorboard_log/verl_grpo_example_deepscaler/qwen3_32b_verl_true_weights.
[36m(TaskRunner pid=3869676)[0m Checkpoint tracker file does not exist: /workspace/cann-recipes-train/llm_rl/qwen3/checkpoints/verl_grpo_example_deepscaler/qwen3_32b_verl_true_weights/latest_checkpointed_iteration.txt
[36m(TaskRunner pid=3869676)[0m Training from scratch
[36m(WorkerDict pid=3899699)[0m WARNING 01-03 21:15:37 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[36m(WorkerDict pid=3899855)[0m WARNING 01-03 21:15:28 [core.py:112] Using configured V1 scheduler class vllm_ascend.core.scheduler.AscendScheduler. This scheduler interface is not public and compatibility may not be maintained.[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=3899855)[0m max_num_running_reqs: 128 max_num_scheduled_tokens: 36864 max_model_len: 36864[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=3899668)[0m [Rank 6 | Local Rank 0] 2026-01-03 21:15:29,690 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py:265] => The 'logprobs' parameter is incompatible with Speculative Decoding and has been disabled.[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=3899668)[0m kwargs: {'n': 1, 'max_tokens': 34816, 'repetition_penalty': 1.0, 'detokenize': False, 'temperature': 0.9, 'top_k': -1, 'top_p': 1.0, 'ignore_eos': False}[32m [repeated 15x across cluster][0m
