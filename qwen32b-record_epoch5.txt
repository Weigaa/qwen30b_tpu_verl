INFO 12-13 15:21:25 [__init__.py:36] Available plugins for group vllm.platform_plugins:
INFO 12-13 15:21:25 [__init__.py:38] - ascend -> vllm_ascend:register
INFO 12-13 15:21:25 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 15:21:25 [__init__.py:207] Platform plugin ascend is activated
[Rank 0 | Local Rank 0] 2025-12-13 15:21:26,157 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
ray init kwargs: {'num_cpus': None, 'runtime_env': {'env_vars': {'TOKENIZERS_PARALLELISM': 'true', 'NCCL_DEBUG': 'WARN', 'VLLM_ALLOW_RUNTIME_LORA_UPDATING': 'true', 'NCCL_CUMEM_ENABLE': '0'}, 'working_dir': None}}
[36m(pid=348037)[0m INFO 12-13 15:21:42 [__init__.py:36] Available plugins for group vllm.platform_plugins:
[36m(pid=348037)[0m INFO 12-13 15:21:42 [__init__.py:38] - ascend -> vllm_ascend:register
[36m(pid=348037)[0m INFO 12-13 15:21:42 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[36m(pid=348037)[0m INFO 12-13 15:21:42 [__init__.py:207] Platform plugin ascend is activated
[36m(pid=348037)[0m [Rank 0 | Local Rank 0] 2025-12-13 15:21:42,927 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(TaskRunner pid=348037)[0m TaskRunner hostname: liteserver-hps-1f00-00001, PID: 348037
[36m(TaskRunner pid=348037)[0m {'actor_rollout_ref': {'actor': {'_target_': 'verl.workers.config.McoreActorConfig',
[36m(TaskRunner pid=348037)[0m                                  'checkpoint': {'_target_': 'verl.trainer.config.CheckpointConfig',
[36m(TaskRunner pid=348037)[0m                                                 'async_save': False,
[36m(TaskRunner pid=348037)[0m                                                 'load_contents': ['model',
[36m(TaskRunner pid=348037)[0m                                                                   'optimizer',
[36m(TaskRunner pid=348037)[0m                                                                   'extra'],
[36m(TaskRunner pid=348037)[0m                                                 'save_contents': ['model',
[36m(TaskRunner pid=348037)[0m                                                                   'optimizer',
[36m(TaskRunner pid=348037)[0m                                                                   'extra']},
[36m(TaskRunner pid=348037)[0m                                  'clip_ratio': 0.2,
[36m(TaskRunner pid=348037)[0m                                  'clip_ratio_c': 3.0,
[36m(TaskRunner pid=348037)[0m                                  'clip_ratio_high': 0.2,
[36m(TaskRunner pid=348037)[0m                                  'clip_ratio_low': 0.2,
[36m(TaskRunner pid=348037)[0m                                  'data_loader_seed': None,
[36m(TaskRunner pid=348037)[0m                                  'entropy_coeff': 0,
[36m(TaskRunner pid=348037)[0m                                  'freeze_vision_tower': False,
[36m(TaskRunner pid=348037)[0m                                  'kl_loss_coef': 0.001,
[36m(TaskRunner pid=348037)[0m                                  'kl_loss_type': 'low_var_kl',
[36m(TaskRunner pid=348037)[0m                                  'load_weight': True,
[36m(TaskRunner pid=348037)[0m                                  'loss_agg_mode': 'token-mean',
[36m(TaskRunner pid=348037)[0m                                  'megatron': {'_target_': 'verl.workers.config.McoreEngineConfig',
[36m(TaskRunner pid=348037)[0m                                               'context_parallel_size': 1,
[36m(TaskRunner pid=348037)[0m                                               'dist_checkpointing_path': None,
[36m(TaskRunner pid=348037)[0m                                               'expert_model_parallel_size': 1,
[36m(TaskRunner pid=348037)[0m                                               'expert_tensor_parallel_size': 1,
[36m(TaskRunner pid=348037)[0m                                               'forward_only': False,
[36m(TaskRunner pid=348037)[0m                                               'grad_offload': True,
[36m(TaskRunner pid=348037)[0m                                               'optimizer_offload': False,
[36m(TaskRunner pid=348037)[0m                                               'override_ddp_config': {},
[36m(TaskRunner pid=348037)[0m                                               'override_mcore_model_config': {},
[36m(TaskRunner pid=348037)[0m                                               'override_transformer_config': {'attention_backend': 'flash',
[36m(TaskRunner pid=348037)[0m                                                                               'context_parallel_algo': 'megatron_cp_algo',
[36m(TaskRunner pid=348037)[0m                                                                               'context_parallel_size': 1,
[36m(TaskRunner pid=348037)[0m                                                                               'cp_window_size': 1,
[36m(TaskRunner pid=348037)[0m                                                                               'recompute_granularity': 'full',
[36m(TaskRunner pid=348037)[0m                                                                               'recompute_method': 'uniform',
[36m(TaskRunner pid=348037)[0m                                                                               'recompute_modules': ['core_attn'],
[36m(TaskRunner pid=348037)[0m                                                                               'recompute_num_layers': 1,
[36m(TaskRunner pid=348037)[0m                                                                               'seq_length': 2048,
[36m(TaskRunner pid=348037)[0m                                                                               'swap_optimizer': True,
[36m(TaskRunner pid=348037)[0m                                                                               'use_cp_send_recv_overlap': True,
[36m(TaskRunner pid=348037)[0m                                                                               'use_flash_attn': True,
[36m(TaskRunner pid=348037)[0m                                                                               'use_fused_ring_attention_update': True,
[36m(TaskRunner pid=348037)[0m                                                                               'use_fused_rotary_pos_emb': True,
[36m(TaskRunner pid=348037)[0m                                                                               'use_fused_swiglu': True},
[36m(TaskRunner pid=348037)[0m                                               'param_offload': True,
[36m(TaskRunner pid=348037)[0m                                               'pipeline_model_parallel_size': 2,
[36m(TaskRunner pid=348037)[0m                                               'seed': 42,
[36m(TaskRunner pid=348037)[0m                                               'sequence_parallel': True,
[36m(TaskRunner pid=348037)[0m                                               'tensor_model_parallel_size': 8,
[36m(TaskRunner pid=348037)[0m                                               'use_dist_checkpointing': False,
[36m(TaskRunner pid=348037)[0m                                               'use_distributed_optimizer': True,
[36m(TaskRunner pid=348037)[0m                                               'use_mbridge': False,
[36m(TaskRunner pid=348037)[0m                                               'virtual_pipeline_model_parallel_size': None},
[36m(TaskRunner pid=348037)[0m                                  'optim': {'_target_': 'verl.workers.config.McoreOptimizerConfig',
[36m(TaskRunner pid=348037)[0m                                            'betas': [0.9, 0.999],
[36m(TaskRunner pid=348037)[0m                                            'clip_grad': 10000,
[36m(TaskRunner pid=348037)[0m                                            'lr': 1e-06,
[36m(TaskRunner pid=348037)[0m                                            'lr_decay_steps': None,
[36m(TaskRunner pid=348037)[0m                                            'lr_decay_style': 'constant',
[36m(TaskRunner pid=348037)[0m                                            'lr_warmup_init': 0.0,
[36m(TaskRunner pid=348037)[0m                                            'lr_warmup_steps': -1,
[36m(TaskRunner pid=348037)[0m                                            'lr_warmup_steps_ratio': 0.0,
[36m(TaskRunner pid=348037)[0m                                            'lr_wsd_decay_steps': None,
[36m(TaskRunner pid=348037)[0m                                            'lr_wsd_decay_style': 'exponential',
[36m(TaskRunner pid=348037)[0m                                            'min_lr': 0.0,
[36m(TaskRunner pid=348037)[0m                                            'optimizer': 'adam',
[36m(TaskRunner pid=348037)[0m                                            'override_optimizer_config': {},
[36m(TaskRunner pid=348037)[0m                                            'total_training_steps': -1,
[36m(TaskRunner pid=348037)[0m                                            'use_checkpoint_opt_param_scheduler': False,
[36m(TaskRunner pid=348037)[0m                                            'weight_decay': 0.01,
[36m(TaskRunner pid=348037)[0m                                            'weight_decay_incr_style': 'constant'},
[36m(TaskRunner pid=348037)[0m                                  'policy_loss': {'_target_': 'verl.workers.config.PolicyLossConfig',
[36m(TaskRunner pid=348037)[0m                                                  'clip_cov_lb': 1.0,
[36m(TaskRunner pid=348037)[0m                                                  'clip_cov_ratio': 0.0002,
[36m(TaskRunner pid=348037)[0m                                                  'clip_cov_ub': 5.0,
[36m(TaskRunner pid=348037)[0m                                                  'kl_cov_ratio': 0.0002,
[36m(TaskRunner pid=348037)[0m                                                  'loss_mode': 'vanilla',
[36m(TaskRunner pid=348037)[0m                                                  'ppo_kl_coef': 0.1},
[36m(TaskRunner pid=348037)[0m                                  'ppo_epochs': 1,
[36m(TaskRunner pid=348037)[0m                                  'ppo_max_token_len_per_gpu': 36864,
[36m(TaskRunner pid=348037)[0m                                  'ppo_micro_batch_size': None,
[36m(TaskRunner pid=348037)[0m                                  'ppo_micro_batch_size_per_gpu': 1,
[36m(TaskRunner pid=348037)[0m                                  'ppo_mini_batch_size': 32,
[36m(TaskRunner pid=348037)[0m                                  'profiler': {'_target_': 'verl.utils.profiler.ProfilerConfig',
[36m(TaskRunner pid=348037)[0m                                               'all_ranks': False,
[36m(TaskRunner pid=348037)[0m                                               'enable': False,
[36m(TaskRunner pid=348037)[0m                                               'ranks': [],
[36m(TaskRunner pid=348037)[0m                                               'save_path': 'outputs/profile',
[36m(TaskRunner pid=348037)[0m                                               'tool': None,
[36m(TaskRunner pid=348037)[0m                                               'tool_config': {'npu': {'_target_': 'verl.utils.profiler.config.NPUToolConfig',
[36m(TaskRunner pid=348037)[0m                                                                       'analysis': True,
[36m(TaskRunner pid=348037)[0m                                                                       'contents': [],
[36m(TaskRunner pid=348037)[0m                                                                       'discrete': False,
[36m(TaskRunner pid=348037)[0m                                                                       'level': 'level1'},
[36m(TaskRunner pid=348037)[0m                                                               'nsys': {'_target_': 'verl.utils.profiler.config.NsightToolConfig',
[36m(TaskRunner pid=348037)[0m                                                                        'discrete': False},
[36m(TaskRunner pid=348037)[0m                                                               'torch': {'_target_': 'verl.utils.profiler.config.TorchProfilerToolConfig',
[36m(TaskRunner pid=348037)[0m                                                                         'step_end': None,
[36m(TaskRunner pid=348037)[0m                                                                         'step_start': 0},
[36m(TaskRunner pid=348037)[0m                                                               'torch_memory': {'_target_': 'verl.utils.profiler.config.TorchMemoryToolConfig',
[36m(TaskRunner pid=348037)[0m                                                                                'stack_depth': 32,
[36m(TaskRunner pid=348037)[0m                                                                                'trace_alloc_max_entries': 100000}}},
[36m(TaskRunner pid=348037)[0m                                  'recompute_old_log_prob': True,
[36m(TaskRunner pid=348037)[0m                                  'shuffle': False,
[36m(TaskRunner pid=348037)[0m                                  'strategy': 'megatron',
[36m(TaskRunner pid=348037)[0m                                  'use_dynamic_bsz': True,
[36m(TaskRunner pid=348037)[0m                                  'use_fused_kernels': False,
[36m(TaskRunner pid=348037)[0m                                  'use_kl_loss': True,
[36m(TaskRunner pid=348037)[0m                                  'use_torch_compile': True},
[36m(TaskRunner pid=348037)[0m                        'hybrid_engine': True,
[36m(TaskRunner pid=348037)[0m                        'model': {'custom_chat_template': None,
[36m(TaskRunner pid=348037)[0m                                  'external_lib': None,
[36m(TaskRunner pid=348037)[0m                                  'override_config': {'model_config': {},
[36m(TaskRunner pid=348037)[0m                                                      'moe_config': {'freeze_moe_router': False}},
[36m(TaskRunner pid=348037)[0m                                  'path': '/home/data/Qwen3-32B',
[36m(TaskRunner pid=348037)[0m                                  'trust_remote_code': False,
[36m(TaskRunner pid=348037)[0m                                  'use_fused_kernels': False,
[36m(TaskRunner pid=348037)[0m                                  'use_remove_padding': False},
[36m(TaskRunner pid=348037)[0m                        'nccl_timeout': 600,
[36m(TaskRunner pid=348037)[0m                        'ref': {'load_weight': True,
[36m(TaskRunner pid=348037)[0m                                'log_prob_max_token_len_per_gpu': 36864,
[36m(TaskRunner pid=348037)[0m                                'log_prob_micro_batch_size': None,
[36m(TaskRunner pid=348037)[0m                                'log_prob_micro_batch_size_per_gpu': 8,
[36m(TaskRunner pid=348037)[0m                                'log_prob_use_dynamic_bsz': True,
[36m(TaskRunner pid=348037)[0m                                'megatron': {'_target_': 'verl.workers.config.MegatronEngineConfig',
[36m(TaskRunner pid=348037)[0m                                             'context_parallel_size': 1,
[36m(TaskRunner pid=348037)[0m                                             'dist_checkpointing_path': None,
[36m(TaskRunner pid=348037)[0m                                             'expert_model_parallel_size': 1,
[36m(TaskRunner pid=348037)[0m                                             'expert_tensor_parallel_size': 1,
[36m(TaskRunner pid=348037)[0m                                             'forward_only': False,
[36m(TaskRunner pid=348037)[0m                                             'grad_offload': False,
[36m(TaskRunner pid=348037)[0m                                             'optimizer_offload': False,
[36m(TaskRunner pid=348037)[0m                                             'override_ddp_config': {},
[36m(TaskRunner pid=348037)[0m                                             'override_mcore_model_config': {},
[36m(TaskRunner pid=348037)[0m                                             'override_transformer_config': {'attention_backend': 'flash',
[36m(TaskRunner pid=348037)[0m                                                                             'context_parallel_algo': 'megatron_cp_algo',
[36m(TaskRunner pid=348037)[0m                                                                             'context_parallel_size': 1,
[36m(TaskRunner pid=348037)[0m                                                                             'cp_window_size': 1,
[36m(TaskRunner pid=348037)[0m                                                                             'recompute_granularity': 'full',
[36m(TaskRunner pid=348037)[0m                                                                             'recompute_method': 'uniform',
[36m(TaskRunner pid=348037)[0m                                                                             'recompute_modules': ['core_attn'],
[36m(TaskRunner pid=348037)[0m                                                                             'recompute_num_layers': 1,
[36m(TaskRunner pid=348037)[0m                                                                             'seq_length': 2048,
[36m(TaskRunner pid=348037)[0m                                                                             'swap_optimizer': True,
[36m(TaskRunner pid=348037)[0m                                                                             'use_cp_send_recv_overlap': True,
[36m(TaskRunner pid=348037)[0m                                                                             'use_flash_attn': True,
[36m(TaskRunner pid=348037)[0m                                                                             'use_fused_ring_attention_update': True,
[36m(TaskRunner pid=348037)[0m                                                                             'use_fused_rotary_pos_emb': True,
[36m(TaskRunner pid=348037)[0m                                                                             'use_fused_swiglu': True},
[36m(TaskRunner pid=348037)[0m                                             'param_offload': True,
[36m(TaskRunner pid=348037)[0m                                             'pipeline_model_parallel_size': 1,
[36m(TaskRunner pid=348037)[0m                                             'seed': 42,
[36m(TaskRunner pid=348037)[0m                                             'sequence_parallel': True,
[36m(TaskRunner pid=348037)[0m                                             'tensor_model_parallel_size': 1,
[36m(TaskRunner pid=348037)[0m                                             'use_dist_checkpointing': False,
[36m(TaskRunner pid=348037)[0m                                             'use_distributed_optimizer': True,
[36m(TaskRunner pid=348037)[0m                                             'use_mbridge': False,
[36m(TaskRunner pid=348037)[0m                                             'virtual_pipeline_model_parallel_size': None},
[36m(TaskRunner pid=348037)[0m                                'profiler': {'_target_': 'verl.utils.profiler.ProfilerConfig',
[36m(TaskRunner pid=348037)[0m                                             'all_ranks': False,
[36m(TaskRunner pid=348037)[0m                                             'enable': False,
[36m(TaskRunner pid=348037)[0m                                             'ranks': [],
[36m(TaskRunner pid=348037)[0m                                             'save_path': 'outputs/profile',
[36m(TaskRunner pid=348037)[0m                                             'tool': None,
[36m(TaskRunner pid=348037)[0m                                             'tool_config': {'npu': {'_target_': 'verl.utils.profiler.config.NPUToolConfig',
[36m(TaskRunner pid=348037)[0m                                                                     'analysis': True,
[36m(TaskRunner pid=348037)[0m                                                                     'contents': [],
[36m(TaskRunner pid=348037)[0m                                                                     'discrete': False,
[36m(TaskRunner pid=348037)[0m                                                                     'level': 'level1'},
[36m(TaskRunner pid=348037)[0m                                                             'nsys': {'_target_': 'verl.utils.profiler.config.NsightToolConfig',
[36m(TaskRunner pid=348037)[0m                                                                      'discrete': False},
[36m(TaskRunner pid=348037)[0m                                                             'torch': {'_target_': 'verl.utils.profiler.config.TorchProfilerToolConfig',
[36m(TaskRunner pid=348037)[0m                                                                       'step_end': None,
[36m(TaskRunner pid=348037)[0m                                                                       'step_start': 0},
[36m(TaskRunner pid=348037)[0m                                                             'torch_memory': {'_target_': 'verl.utils.profiler.config.TorchMemoryToolConfig',
[36m(TaskRunner pid=348037)[0m                                                                              'stack_depth': 32,
[36m(TaskRunner pid=348037)[0m                                                                              'trace_alloc_max_entries': 100000}}},
[36m(TaskRunner pid=348037)[0m                                'strategy': 'megatron',
[36m(TaskRunner pid=348037)[0m                                'use_torch_compile': True},
[36m(TaskRunner pid=348037)[0m                        'rollout': {'_target_': 'verl.workers.config.RolloutConfig',
[36m(TaskRunner pid=348037)[0m                                    'agent': {'_target_': 'verl.workers.config.AgentLoopConfig',
[36m(TaskRunner pid=348037)[0m                                              'agent_loop_config_path': None,
[36m(TaskRunner pid=348037)[0m                                              'custom_async_server': {'_target_': 'verl.workers.config.CustomAsyncServerConfig',
[36m(TaskRunner pid=348037)[0m                                                                      'name': None,
[36m(TaskRunner pid=348037)[0m                                                                      'path': None},
[36m(TaskRunner pid=348037)[0m                                              'default_agent_loop': 'single_turn_agent',
[36m(TaskRunner pid=348037)[0m                                              'num_workers': 8},
[36m(TaskRunner pid=348037)[0m                                    'calculate_log_probs': False,
[36m(TaskRunner pid=348037)[0m                                    'cudagraph_capture_sizes': None,
[36m(TaskRunner pid=348037)[0m                                    'data_parallel_size': 1,
[36m(TaskRunner pid=348037)[0m                                    'disable_log_stats': True,
[36m(TaskRunner pid=348037)[0m                                    'do_sample': True,
[36m(TaskRunner pid=348037)[0m                                    'dtype': 'bfloat16',
[36m(TaskRunner pid=348037)[0m                                    'enable_chunked_prefill': True,
[36m(TaskRunner pid=348037)[0m                                    'enable_prefix_caching': True,
[36m(TaskRunner pid=348037)[0m                                    'enforce_eager': True,
[36m(TaskRunner pid=348037)[0m                                    'engine_kwargs': {'sglang': {},
[36m(TaskRunner pid=348037)[0m                                                      'vllm': {'speculative_config': {'method': 'sam',
[36m(TaskRunner pid=348037)[0m                                                                                      'num_speculative_tokens': 3}}},
[36m(TaskRunner pid=348037)[0m                                    'expert_parallel_size': 1,
[36m(TaskRunner pid=348037)[0m                                    'free_cache_engine': True,
[36m(TaskRunner pid=348037)[0m                                    'gpu_memory_utilization': 0.85,
[36m(TaskRunner pid=348037)[0m                                    'ignore_eos': False,
[36m(TaskRunner pid=348037)[0m                                    'layer_name_map': {'gate_proj_layer_name': 'gate_up',
[36m(TaskRunner pid=348037)[0m                                                       'qkv_layer_name': 'qkv'},
[36m(TaskRunner pid=348037)[0m                                    'load_format': 'dummy',
[36m(TaskRunner pid=348037)[0m                                    'log_prob_max_token_len_per_gpu': 36864,
[36m(TaskRunner pid=348037)[0m                                    'log_prob_micro_batch_size': None,
[36m(TaskRunner pid=348037)[0m                                    'log_prob_micro_batch_size_per_gpu': None,
[36m(TaskRunner pid=348037)[0m                                    'log_prob_use_dynamic_bsz': True,
[36m(TaskRunner pid=348037)[0m                                    'max_model_len': None,
[36m(TaskRunner pid=348037)[0m                                    'max_num_batched_tokens': 36864,
[36m(TaskRunner pid=348037)[0m                                    'max_num_seqs': 128,
[36m(TaskRunner pid=348037)[0m                                    'mode': 'sync',
[36m(TaskRunner pid=348037)[0m                                    'multi_stage_wake_up': False,
[36m(TaskRunner pid=348037)[0m                                    'multi_turn': {'_target_': 'verl.workers.config.MultiTurnConfig',
[36m(TaskRunner pid=348037)[0m                                                   'enable': False,
[36m(TaskRunner pid=348037)[0m                                                   'format': 'hermes',
[36m(TaskRunner pid=348037)[0m                                                   'interaction_config_path': None,
[36m(TaskRunner pid=348037)[0m                                                   'max_assistant_turns': None,
[36m(TaskRunner pid=348037)[0m                                                   'max_parallel_calls': 1,
[36m(TaskRunner pid=348037)[0m                                                   'max_tool_response_length': 256,
[36m(TaskRunner pid=348037)[0m                                                   'max_user_turns': None,
[36m(TaskRunner pid=348037)[0m                                                   'num_repeat_rollouts': None,
[36m(TaskRunner pid=348037)[0m                                                   'tokenization_sanity_check_mode': 'strict',
[36m(TaskRunner pid=348037)[0m                                                   'tool_config_path': None,
[36m(TaskRunner pid=348037)[0m                                                   'tool_response_truncate_side': 'middle',
[36m(TaskRunner pid=348037)[0m                                                   'use_inference_chat_template': False},
[36m(TaskRunner pid=348037)[0m                                    'n': 16,
[36m(TaskRunner pid=348037)[0m                                    'name': 'vllm',
[36m(TaskRunner pid=348037)[0m                                    'over_sample_rate': 0,
[36m(TaskRunner pid=348037)[0m                                    'pipeline_model_parallel_size': 1,
[36m(TaskRunner pid=348037)[0m                                    'profiler': {'_target_': 'verl.utils.profiler.ProfilerConfig',
[36m(TaskRunner pid=348037)[0m                                                 'all_ranks': False,
[36m(TaskRunner pid=348037)[0m                                                 'enable': False,
[36m(TaskRunner pid=348037)[0m                                                 'ranks': [],
[36m(TaskRunner pid=348037)[0m                                                 'save_path': 'outputs/profile',
[36m(TaskRunner pid=348037)[0m                                                 'tool': None,
[36m(TaskRunner pid=348037)[0m                                                 'tool_config': {'npu': {'_target_': 'verl.utils.profiler.config.NPUToolConfig',
[36m(TaskRunner pid=348037)[0m                                                                         'analysis': True,
[36m(TaskRunner pid=348037)[0m                                                                         'contents': [],
[36m(TaskRunner pid=348037)[0m                                                                         'discrete': False,
[36m(TaskRunner pid=348037)[0m                                                                         'level': 'level1'},
[36m(TaskRunner pid=348037)[0m                                                                 'nsys': {'_target_': 'verl.utils.profiler.config.NsightToolConfig',
[36m(TaskRunner pid=348037)[0m                                                                          'discrete': False},
[36m(TaskRunner pid=348037)[0m                                                                 'torch': {'_target_': 'verl.utils.profiler.config.TorchProfilerToolConfig',
[36m(TaskRunner pid=348037)[0m                                                                           'step_end': None,
[36m(TaskRunner pid=348037)[0m                                                                           'step_start': 0},
[36m(TaskRunner pid=348037)[0m                                                                 'torch_memory': {'_target_': 'verl.utils.profiler.config.TorchMemoryToolConfig',
[36m(TaskRunner pid=348037)[0m                                                                                  'stack_depth': 32,
[36m(TaskRunner pid=348037)[0m                                                                                  'trace_alloc_max_entries': 100000}}},
[36m(TaskRunner pid=348037)[0m                                    'prompt_length': 2048,
[36m(TaskRunner pid=348037)[0m                                    'response_length': 34816,
[36m(TaskRunner pid=348037)[0m                                    'skip_dump_dir': '/tmp/rollout_dump',
[36m(TaskRunner pid=348037)[0m                                    'skip_rollout': False,
[36m(TaskRunner pid=348037)[0m                                    'skip_tokenizer_init': True,
[36m(TaskRunner pid=348037)[0m                                    'temperature': 0.9,
[36m(TaskRunner pid=348037)[0m                                    'tensor_model_parallel_size': 8,
[36m(TaskRunner pid=348037)[0m                                    'top_k': -1,
[36m(TaskRunner pid=348037)[0m                                    'top_p': 1.0,
[36m(TaskRunner pid=348037)[0m                                    'trace': {'_target_': 'verl.workers.config.TraceConfig',
[36m(TaskRunner pid=348037)[0m                                              'backend': None,
[36m(TaskRunner pid=348037)[0m                                              'token2text': False},
[36m(TaskRunner pid=348037)[0m                                    'update_weights_bucket_megabytes': 512,
[36m(TaskRunner pid=348037)[0m                                    'val_kwargs': {'_target_': 'verl.workers.config.SamplingConfig',
[36m(TaskRunner pid=348037)[0m                                                   'do_sample': False,
[36m(TaskRunner pid=348037)[0m                                                   'n': 1,
[36m(TaskRunner pid=348037)[0m                                                   'temperature': 0,
[36m(TaskRunner pid=348037)[0m                                                   'top_k': -1,
[36m(TaskRunner pid=348037)[0m                                                   'top_p': 1.0}}},
[36m(TaskRunner pid=348037)[0m  'algorithm': {'_target_': 'verl.trainer.config.AlgoConfig',
[36m(TaskRunner pid=348037)[0m                'adv_estimator': 'grpo',
[36m(TaskRunner pid=348037)[0m                'gamma': 1.0,
[36m(TaskRunner pid=348037)[0m                'kl_ctrl': {'_target_': 'verl.trainer.config.KLControlConfig',
[36m(TaskRunner pid=348037)[0m                            'horizon': 10000,
[36m(TaskRunner pid=348037)[0m                            'kl_coef': 0.001,
[36m(TaskRunner pid=348037)[0m                            'target_kl': 0.1,
[36m(TaskRunner pid=348037)[0m                            'type': 'fixed'},
[36m(TaskRunner pid=348037)[0m                'kl_penalty': 'kl',
[36m(TaskRunner pid=348037)[0m                'lam': 1.0,
[36m(TaskRunner pid=348037)[0m                'norm_adv_by_std_in_grpo': True,
[36m(TaskRunner pid=348037)[0m                'pf_ppo': {'reweight_method': 'pow', 'weight_pow': 2.0},
[36m(TaskRunner pid=348037)[0m                'rollout_is': False,
[36m(TaskRunner pid=348037)[0m                'rollout_is_level': 'token',
[36m(TaskRunner pid=348037)[0m                'rollout_is_mode': 'truncate',
[36m(TaskRunner pid=348037)[0m                'rollout_is_threshold': None,
[36m(TaskRunner pid=348037)[0m                'rollout_is_threshold_lower': None,
[36m(TaskRunner pid=348037)[0m                'rollout_is_veto_threshold': 0.0001,
[36m(TaskRunner pid=348037)[0m                'use_kl_in_reward': False,
[36m(TaskRunner pid=348037)[0m                'use_pf_ppo': False},
[36m(TaskRunner pid=348037)[0m  'critic': {'_target_': 'verl.workers.config.McoreCriticConfig',
[36m(TaskRunner pid=348037)[0m             'checkpoint': {'_target_': 'verl.trainer.config.CheckpointConfig',
[36m(TaskRunner pid=348037)[0m                            'async_save': False,
[36m(TaskRunner pid=348037)[0m                            'load_contents': ['model', 'optimizer', 'extra'],
[36m(TaskRunner pid=348037)[0m                            'save_contents': ['model', 'optimizer', 'extra']},
[36m(TaskRunner pid=348037)[0m             'cliprange_value': 0.5,
[36m(TaskRunner pid=348037)[0m             'data_loader_seed': None,
[36m(TaskRunner pid=348037)[0m             'enable': None,
[36m(TaskRunner pid=348037)[0m             'forward_max_token_len_per_gpu': 32768,
[36m(TaskRunner pid=348037)[0m             'load_weight': True,
[36m(TaskRunner pid=348037)[0m             'loss_agg_mode': 'token-mean',
[36m(TaskRunner pid=348037)[0m             'megatron': {'_target_': 'verl.workers.config.McoreEngineConfig',
[36m(TaskRunner pid=348037)[0m                          'context_parallel_size': 1,
[36m(TaskRunner pid=348037)[0m                          'dist_checkpointing_path': None,
[36m(TaskRunner pid=348037)[0m                          'expert_model_parallel_size': 1,
[36m(TaskRunner pid=348037)[0m                          'expert_tensor_parallel_size': 1,
[36m(TaskRunner pid=348037)[0m                          'forward_only': False,
[36m(TaskRunner pid=348037)[0m                          'grad_offload': False,
[36m(TaskRunner pid=348037)[0m                          'optimizer_offload': False,
[36m(TaskRunner pid=348037)[0m                          'override_ddp_config': {},
[36m(TaskRunner pid=348037)[0m                          'override_mcore_model_config': {},
[36m(TaskRunner pid=348037)[0m                          'override_transformer_config': {'attention_backend': 'flash',
[36m(TaskRunner pid=348037)[0m                                                          'recompute_granularity': None,
[36m(TaskRunner pid=348037)[0m                                                          'recompute_method': None,
[36m(TaskRunner pid=348037)[0m                                                          'recompute_modules': ['core_attn'],
[36m(TaskRunner pid=348037)[0m                                                          'recompute_num_layers': None},
[36m(TaskRunner pid=348037)[0m                          'param_offload': False,
[36m(TaskRunner pid=348037)[0m                          'pipeline_model_parallel_size': 1,
[36m(TaskRunner pid=348037)[0m                          'seed': 42,
[36m(TaskRunner pid=348037)[0m                          'sequence_parallel': True,
[36m(TaskRunner pid=348037)[0m                          'tensor_model_parallel_size': 1,
[36m(TaskRunner pid=348037)[0m                          'use_dist_checkpointing': False,
[36m(TaskRunner pid=348037)[0m                          'use_distributed_optimizer': True,
[36m(TaskRunner pid=348037)[0m                          'use_mbridge': False,
[36m(TaskRunner pid=348037)[0m                          'virtual_pipeline_model_parallel_size': None},
[36m(TaskRunner pid=348037)[0m             'model': {'_target_': 'verl.trainer.config.BaseModelConfig',
[36m(TaskRunner pid=348037)[0m                       'external_lib': None,
[36m(TaskRunner pid=348037)[0m                       'override_config': {'model_config': {},
[36m(TaskRunner pid=348037)[0m                                           'moe_config': {'freeze_moe_router': False}},
[36m(TaskRunner pid=348037)[0m                       'path': '~/models/deepseek-llm-7b-chat',
[36m(TaskRunner pid=348037)[0m                       'tokenizer_path': '/home/data/Qwen3-32B',
[36m(TaskRunner pid=348037)[0m                       'trust_remote_code': False},
[36m(TaskRunner pid=348037)[0m             'nccl_timeout': 600,
[36m(TaskRunner pid=348037)[0m             'optim': {'_target_': 'verl.workers.config.McoreOptimizerConfig',
[36m(TaskRunner pid=348037)[0m                       'betas': [0.9, 0.999],
[36m(TaskRunner pid=348037)[0m                       'clip_grad': 1.0,
[36m(TaskRunner pid=348037)[0m                       'lr': 1e-05,
[36m(TaskRunner pid=348037)[0m                       'lr_decay_steps': None,
[36m(TaskRunner pid=348037)[0m                       'lr_decay_style': 'constant',
[36m(TaskRunner pid=348037)[0m                       'lr_warmup_init': 0.0,
[36m(TaskRunner pid=348037)[0m                       'lr_warmup_steps': -1,
[36m(TaskRunner pid=348037)[0m                       'lr_warmup_steps_ratio': 0.0,
[36m(TaskRunner pid=348037)[0m                       'lr_wsd_decay_steps': None,
[36m(TaskRunner pid=348037)[0m                       'lr_wsd_decay_style': 'exponential',
[36m(TaskRunner pid=348037)[0m                       'min_lr': 0.0,
[36m(TaskRunner pid=348037)[0m                       'optimizer': 'adam',
[36m(TaskRunner pid=348037)[0m                       'override_optimizer_config': {},
[36m(TaskRunner pid=348037)[0m                       'total_training_steps': -1,
[36m(TaskRunner pid=348037)[0m                       'use_checkpoint_opt_param_scheduler': False,
[36m(TaskRunner pid=348037)[0m                       'weight_decay': 0.01,
[36m(TaskRunner pid=348037)[0m                       'weight_decay_incr_style': 'constant'},
[36m(TaskRunner pid=348037)[0m             'ppo_epochs': 1,
[36m(TaskRunner pid=348037)[0m             'ppo_max_token_len_per_gpu': 32768,
[36m(TaskRunner pid=348037)[0m             'ppo_micro_batch_size': None,
[36m(TaskRunner pid=348037)[0m             'ppo_micro_batch_size_per_gpu': None,
[36m(TaskRunner pid=348037)[0m             'ppo_mini_batch_size': 32,
[36m(TaskRunner pid=348037)[0m             'profiler': {'_target_': 'verl.utils.profiler.ProfilerConfig',
[36m(TaskRunner pid=348037)[0m                          'all_ranks': False,
[36m(TaskRunner pid=348037)[0m                          'enable': False,
[36m(TaskRunner pid=348037)[0m                          'ranks': [],
[36m(TaskRunner pid=348037)[0m                          'save_path': 'outputs/profile',
[36m(TaskRunner pid=348037)[0m                          'tool': None,
[36m(TaskRunner pid=348037)[0m                          'tool_config': {'npu': {'_target_': 'verl.utils.profiler.config.NPUToolConfig',
[36m(TaskRunner pid=348037)[0m                                                  'analysis': True,
[36m(TaskRunner pid=348037)[0m                                                  'contents': [],
[36m(TaskRunner pid=348037)[0m                                                  'discrete': False,
[36m(TaskRunner pid=348037)[0m                                                  'level': 'level1'},
[36m(TaskRunner pid=348037)[0m                                          'nsys': {'_target_': 'verl.utils.profiler.config.NsightToolConfig',
[36m(TaskRunner pid=348037)[0m                                                   'discrete': False},
[36m(TaskRunner pid=348037)[0m                                          'torch': {'_target_': 'verl.utils.profiler.config.TorchProfilerToolConfig',
[36m(TaskRunner pid=348037)[0m                                                    'step_end': None,
[36m(TaskRunner pid=348037)[0m                                                    'step_start': 0},
[36m(TaskRunner pid=348037)[0m                                          'torch_memory': {'_target_': 'verl.utils.profiler.config.TorchMemoryToolConfig',
[36m(TaskRunner pid=348037)[0m                                                           'stack_depth': 32,
[36m(TaskRunner pid=348037)[0m                                                           'trace_alloc_max_entries': 100000}}},
[36m(TaskRunner pid=348037)[0m             'rollout_n': 16,
[36m(TaskRunner pid=348037)[0m             'shuffle': False,
[36m(TaskRunner pid=348037)[0m             'strategy': 'megatron',
[36m(TaskRunner pid=348037)[0m             'use_dynamic_bsz': True},
[36m(TaskRunner pid=348037)[0m  'custom_reward_function': {'name': 'compute_score', 'path': 'deepscaler.py'},
[36m(TaskRunner pid=348037)[0m  'data': {'apply_chat_template_kwargs': {},
[36m(TaskRunner pid=348037)[0m           'custom_cls': {'name': None, 'path': None},
[36m(TaskRunner pid=348037)[0m           'datagen': {'name': None, 'path': None},
[36m(TaskRunner pid=348037)[0m           'dataloader_num_workers': 8,
[36m(TaskRunner pid=348037)[0m           'dataset_fraction': 0.001,
[36m(TaskRunner pid=348037)[0m           'filter_overlong_prompts': True,
[36m(TaskRunner pid=348037)[0m           'filter_overlong_prompts_workers': 1,
[36m(TaskRunner pid=348037)[0m           'image_key': 'images',
[36m(TaskRunner pid=348037)[0m           'max_prompt_length': 2048,
[36m(TaskRunner pid=348037)[0m           'max_response_length': 34816,
[36m(TaskRunner pid=348037)[0m           'prompt_key': 'prompt',
[36m(TaskRunner pid=348037)[0m           'return_full_prompt': False,
[36m(TaskRunner pid=348037)[0m           'return_multi_modal_inputs': True,
[36m(TaskRunner pid=348037)[0m           'return_raw_chat': False,
[36m(TaskRunner pid=348037)[0m           'return_raw_input_ids': False,
[36m(TaskRunner pid=348037)[0m           'reward_fn_key': 'data_source',
[36m(TaskRunner pid=348037)[0m           'sampler': {'class_name': None, 'class_path': None},
[36m(TaskRunner pid=348037)[0m           'shuffle': False,
[36m(TaskRunner pid=348037)[0m           'tokenizer': None,
[36m(TaskRunner pid=348037)[0m           'train_batch_size': 32,
[36m(TaskRunner pid=348037)[0m           'train_files': '/workspace/data/deepscaler/train.parquet',
[36m(TaskRunner pid=348037)[0m           'truncation': 'error',
[36m(TaskRunner pid=348037)[0m           'trust_remote_code': False,
[36m(TaskRunner pid=348037)[0m           'use_shm': False,
[36m(TaskRunner pid=348037)[0m           'val_batch_size': None,
[36m(TaskRunner pid=348037)[0m           'val_files': '/workspace/data/deepscaler/test.parquet',
[36m(TaskRunner pid=348037)[0m           'validation_shuffle': False,
[36m(TaskRunner pid=348037)[0m           'video_key': 'videos'},
[36m(TaskRunner pid=348037)[0m  'global_profiler': {'_target_': 'verl.utils.profiler.ProfilerConfig',
[36m(TaskRunner pid=348037)[0m                      'global_tool_config': {'nsys': {'controller_nsight_options': {'cuda-graph-trace': 'graph',
[36m(TaskRunner pid=348037)[0m                                                                                    'cuda-memory-usage': 'true',
[36m(TaskRunner pid=348037)[0m                                                                                    'trace': 'cuda,nvtx,cublas,ucx'},
[36m(TaskRunner pid=348037)[0m                                                      'discrete': False,
[36m(TaskRunner pid=348037)[0m                                                      'worker_nsight_options': {'capture-range': 'cudaProfilerApi',
[36m(TaskRunner pid=348037)[0m                                                                                'capture-range-end': None,
[36m(TaskRunner pid=348037)[0m                                                                                'cuda-graph-trace': 'graph',
[36m(TaskRunner pid=348037)[0m                                                                                'cuda-memory-usage': 'true',
[36m(TaskRunner pid=348037)[0m                                                                                'kill': 'none',
[36m(TaskRunner pid=348037)[0m                                                                                'trace': 'cuda,nvtx,cublas,ucx'}},
[36m(TaskRunner pid=348037)[0m                                             'torch_memory': {'context': 'all',
[36m(TaskRunner pid=348037)[0m                                                              'kw_args': {},
[36m(TaskRunner pid=348037)[0m                                                              'stack_depth': 32,
[36m(TaskRunner pid=348037)[0m                                                              'stacks': 'all',
[36m(TaskRunner pid=348037)[0m                                                              'trace_alloc_max_entries': 100000}},
[36m(TaskRunner pid=348037)[0m                      'profile_continuous_steps': False,
[36m(TaskRunner pid=348037)[0m                      'save_path': 'outputs/profile',
[36m(TaskRunner pid=348037)[0m                      'steps': None,
[36m(TaskRunner pid=348037)[0m                      'tool': None},
[36m(TaskRunner pid=348037)[0m  'ray_kwargs': {'ray_init': {'num_cpus': None}, 'timeline_json_file': None},
[36m(TaskRunner pid=348037)[0m  'reward_model': {'enable': False,
[36m(TaskRunner pid=348037)[0m                   'enable_resource_pool': False,
[36m(TaskRunner pid=348037)[0m                   'forward_max_token_len_per_gpu': 32768,
[36m(TaskRunner pid=348037)[0m                   'launch_reward_fn_async': False,
[36m(TaskRunner pid=348037)[0m                   'load_weight': True,
[36m(TaskRunner pid=348037)[0m                   'max_length': None,
[36m(TaskRunner pid=348037)[0m                   'megatron': {'_target_': 'verl.workers.config.MegatronEngineConfig',
[36m(TaskRunner pid=348037)[0m                                'context_parallel_size': 1,
[36m(TaskRunner pid=348037)[0m                                'dist_checkpointing_path': None,
[36m(TaskRunner pid=348037)[0m                                'expert_model_parallel_size': 1,
[36m(TaskRunner pid=348037)[0m                                'expert_tensor_parallel_size': 1,
[36m(TaskRunner pid=348037)[0m                                'override_transformer_config': {'attention_backend': 'flash',
[36m(TaskRunner pid=348037)[0m                                                                'context_parallel_algo': 'megatron_cp_algo',
[36m(TaskRunner pid=348037)[0m                                                                'context_parallel_size': 1,
[36m(TaskRunner pid=348037)[0m                                                                'cp_window_size': 1,
[36m(TaskRunner pid=348037)[0m                                                                'recompute_granularity': 'full',
[36m(TaskRunner pid=348037)[0m                                                                'recompute_method': 'uniform',
[36m(TaskRunner pid=348037)[0m                                                                'recompute_modules': ['core_attn'],
[36m(TaskRunner pid=348037)[0m                                                                'recompute_num_layers': 1,
[36m(TaskRunner pid=348037)[0m                                                                'seq_length': 2048,
[36m(TaskRunner pid=348037)[0m                                                                'swap_optimizer': True,
[36m(TaskRunner pid=348037)[0m                                                                'use_cp_send_recv_overlap': True,
[36m(TaskRunner pid=348037)[0m                                                                'use_flash_attn': True,
[36m(TaskRunner pid=348037)[0m                                                                'use_fused_ring_attention_update': True,
[36m(TaskRunner pid=348037)[0m                                                                'use_fused_rotary_pos_emb': True,
[36m(TaskRunner pid=348037)[0m                                                                'use_fused_swiglu': True},
[36m(TaskRunner pid=348037)[0m                                'param_offload': False,
[36m(TaskRunner pid=348037)[0m                                'pipeline_model_parallel_size': 1,
[36m(TaskRunner pid=348037)[0m                                'seed': 42,
[36m(TaskRunner pid=348037)[0m                                'sequence_parallel': True,
[36m(TaskRunner pid=348037)[0m                                'tensor_model_parallel_size': 1,
[36m(TaskRunner pid=348037)[0m                                'use_dist_checkpointing': False,
[36m(TaskRunner pid=348037)[0m                                'use_distributed_optimizer': False,
[36m(TaskRunner pid=348037)[0m                                'use_mbridge': False,
[36m(TaskRunner pid=348037)[0m                                'virtual_pipeline_model_parallel_size': None},
[36m(TaskRunner pid=348037)[0m                   'micro_batch_size': None,
[36m(TaskRunner pid=348037)[0m                   'micro_batch_size_per_gpu': None,
[36m(TaskRunner pid=348037)[0m                   'model': {'external_lib': None,
[36m(TaskRunner pid=348037)[0m                             'input_tokenizer': '/home/data/Qwen3-32B',
[36m(TaskRunner pid=348037)[0m                             'path': '~/models/FsfairX-LLaMA3-RM-v0.1',
[36m(TaskRunner pid=348037)[0m                             'trust_remote_code': False},
[36m(TaskRunner pid=348037)[0m                   'n_gpus_per_node': 0,
[36m(TaskRunner pid=348037)[0m                   'nccl_timeout': 600,
[36m(TaskRunner pid=348037)[0m                   'nnodes': 0,
[36m(TaskRunner pid=348037)[0m                   'profiler': {'_target_': 'verl.utils.profiler.ProfilerConfig',
[36m(TaskRunner pid=348037)[0m                                'all_ranks': False,
[36m(TaskRunner pid=348037)[0m                                'enable': False,
[36m(TaskRunner pid=348037)[0m                                'ranks': [],
[36m(TaskRunner pid=348037)[0m                                'save_path': 'outputs/profile',
[36m(TaskRunner pid=348037)[0m                                'tool': None,
[36m(TaskRunner pid=348037)[0m                                'tool_config': {'npu': {'_target_': 'verl.utils.profiler.config.NPUToolConfig',
[36m(TaskRunner pid=348037)[0m                                                        'analysis': True,
[36m(TaskRunner pid=348037)[0m                                                        'contents': [],
[36m(TaskRunner pid=348037)[0m                                                        'discrete': False,
[36m(TaskRunner pid=348037)[0m                                                        'level': 'level1'},
[36m(TaskRunner pid=348037)[0m                                                'nsys': {'_target_': 'verl.utils.profiler.config.NsightToolConfig',
[36m(TaskRunner pid=348037)[0m                                                         'discrete': False},
[36m(TaskRunner pid=348037)[0m                                                'torch': {'_target_': 'verl.utils.profiler.config.TorchProfilerToolConfig',
[36m(TaskRunner pid=348037)[0m                                                          'step_end': None,
[36m(TaskRunner pid=348037)[0m                                                          'step_start': 0},
[36m(TaskRunner pid=348037)[0m                                                'torch_memory': {'_target_': 'verl.utils.profiler.config.TorchMemoryToolConfig',
[36m(TaskRunner pid=348037)[0m                                                                 'stack_depth': 32,
[36m(TaskRunner pid=348037)[0m                                                                 'trace_alloc_max_entries': 100000}}},
[36m(TaskRunner pid=348037)[0m                   'reward_manager': 'naive',
[36m(TaskRunner pid=348037)[0m                   'sandbox_fusion': {'max_concurrent': 64,
[36m(TaskRunner pid=348037)[0m                                      'memory_limit_mb': 1024,
[36m(TaskRunner pid=348037)[0m                                      'url': None},
[36m(TaskRunner pid=348037)[0m                   'strategy': 'megatron',
[36m(TaskRunner pid=348037)[0m                   'use_dynamic_bsz': True},
[36m(TaskRunner pid=348037)[0m  'trainer': {'balance_batch': False,
[36m(TaskRunner pid=348037)[0m              'critic_warmup': 0,
[36m(TaskRunner pid=348037)[0m              'default_hdfs_dir': None,
[36m(TaskRunner pid=348037)[0m              'default_local_dir': 'checkpoints/verl_grpo_example_deepscaler/qwen3_32b_verl_true_weights',
[36m(TaskRunner pid=348037)[0m              'del_local_ckpt_after_load': False,
[36m(TaskRunner pid=348037)[0m              'device': 'npu',
[36m(TaskRunner pid=348037)[0m              'esi_redundant_time': 0,
[36m(TaskRunner pid=348037)[0m              'experiment_name': 'qwen3_32b_verl_true_weights',
[36m(TaskRunner pid=348037)[0m              'log_val_generations': 0,
[36m(TaskRunner pid=348037)[0m              'logger': ['console', 'tensorboard'],
[36m(TaskRunner pid=348037)[0m              'max_actor_ckpt_to_keep': None,
[36m(TaskRunner pid=348037)[0m              'max_critic_ckpt_to_keep': None,
[36m(TaskRunner pid=348037)[0m              'n_gpus_per_node': 16,
[36m(TaskRunner pid=348037)[0m              'nnodes': 1,
[36m(TaskRunner pid=348037)[0m              'project_name': 'verl_grpo_example_deepscaler',
[36m(TaskRunner pid=348037)[0m              'ray_wait_register_center_timeout': 300,
[36m(TaskRunner pid=348037)[0m              'resume_from_path': None,
[36m(TaskRunner pid=348037)[0m              'resume_mode': 'auto',
[36m(TaskRunner pid=348037)[0m              'rollout_data_dir': '/workspace/data/dump',
[36m(TaskRunner pid=348037)[0m              'rollout_length_dir': '/workspace/data/dump',
[36m(TaskRunner pid=348037)[0m              'save_freq': -1,
[36m(TaskRunner pid=348037)[0m              'test_freq': -1,
[36m(TaskRunner pid=348037)[0m              'total_epochs': 5,
[36m(TaskRunner pid=348037)[0m              'total_training_steps': None,
[36m(TaskRunner pid=348037)[0m              'val_before_train': False}}
[36m(TaskRunner pid=348037)[0m WARNING 12-13 15:21:44 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(TaskRunner pid=348037)[0m [validate_config] All configuration checks passed successfully!
[36m(TaskRunner pid=348037)[0m using customized reward function 'compute_score' from '/workspace/cann-recipes-train/llm_rl/qwen3/deepscaler.py'
[36m(TaskRunner pid=348037)[0m using customized reward function 'compute_score' from '/workspace/cann-recipes-train/llm_rl/qwen3/deepscaler.py'
[36m(TaskRunner pid=348037)[0m Using dataset class: RLHFDataset
[36m(TaskRunner pid=348037)[0m dataset len: 36283
[36m(TaskRunner pid=348037)[0m Sampled dataset len: 36 (fraction: 0.001)
[36m(TaskRunner pid=348037)[0m filter dataset len: 36
[36m(TaskRunner pid=348037)[0m Using dataset class: RLHFDataset
[36m(TaskRunner pid=348037)[0m dataset len: 4032
[36m(TaskRunner pid=348037)[0m Sampled dataset len: 4 (fraction: 0.001)
[36m(TaskRunner pid=348037)[0m filter dataset len: 4
[36m(TaskRunner pid=348037)[0m Size of train dataloader: 1, Size of val dataloader: 1
[36m(TaskRunner pid=348037)[0m Total training steps: 5
[36m(TaskRunner pid=348037)[0m colocated worker base class <class 'verl.workers.megatron_workers.MegatronWorker'>
[36m(pid=375734)[0m [Rank 0 | Local Rank 0] 2025-12-13 15:21:56,913 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(pid=375742)[0m [Rank 2 | Local Rank 0] 2025-12-13 15:21:57,848 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(pid=375740)[0m [Rank 1 | Local Rank 0] 2025-12-13 15:21:58,138 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(pid=375734)[0m INFO 12-13 15:21:58 [__init__.py:36] Available plugins for group vllm.platform_plugins:
[36m(pid=375734)[0m INFO 12-13 15:21:58 [__init__.py:38] - ascend -> vllm_ascend:register
[36m(pid=375734)[0m INFO 12-13 15:21:58 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[36m(pid=375734)[0m INFO 12-13 15:21:58 [__init__.py:207] Platform plugin ascend is activated
[36m(pid=375742)[0m INFO 12-13 15:21:59 [__init__.py:36] Available plugins for group vllm.platform_plugins:
[36m(pid=375742)[0m INFO 12-13 15:21:59 [__init__.py:38] - ascend -> vllm_ascend:register
[36m(pid=375742)[0m INFO 12-13 15:21:59 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[36m(pid=375742)[0m INFO 12-13 15:21:59 [__init__.py:207] Platform plugin ascend is activated
[36m(pid=375743)[0m [Rank 3 | Local Rank 0] 2025-12-13 15:21:59,210 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(pid=375744)[0m [Rank 4 | Local Rank 0] 2025-12-13 15:21:59,208 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(pid=375740)[0m INFO 12-13 15:21:59 [__init__.py:36] Available plugins for group vllm.platform_plugins:
[36m(pid=375740)[0m INFO 12-13 15:21:59 [__init__.py:38] - ascend -> vllm_ascend:register
[36m(pid=375740)[0m INFO 12-13 15:21:59 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[36m(pid=375746)[0m [Rank 6 | Local Rank 0] 2025-12-13 15:21:59,521 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(pid=375740)[0m INFO 12-13 15:21:59 [__init__.py:207] Platform plugin ascend is activated
[36m(pid=375734)[0m WARNING 12-13 15:21:59 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(pid=375745)[0m [Rank 5 | Local Rank 0] 2025-12-13 15:21:59,639 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(pid=375752)[0m [Rank 15 | Local Rank 0] 2025-12-13 15:22:00,025 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(pid=375750)[0m [Rank 10 | Local Rank 0] 2025-12-13 15:22:00,187 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(pid=375754)[0m [Rank 13 | Local Rank 0] 2025-12-13 15:22:00,139 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(pid=375747)[0m [Rank 7 | Local Rank 0] 2025-12-13 15:22:00,257 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(pid=375753)[0m [Rank 14 | Local Rank 0] 2025-12-13 15:22:00,366 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(pid=375742)[0m WARNING 12-13 15:22:00 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(pid=375751)[0m [Rank 11 | Local Rank 0] 2025-12-13 15:22:00,496 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(pid=375743)[0m INFO 12-13 15:22:00 [__init__.py:36] Available plugins for group vllm.platform_plugins:
[36m(pid=375743)[0m INFO 12-13 15:22:00 [__init__.py:38] - ascend -> vllm_ascend:register
[36m(pid=375743)[0m INFO 12-13 15:22:00 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[36m(pid=375743)[0m INFO 12-13 15:22:00 [__init__.py:207] Platform plugin ascend is activated
[36m(pid=375744)[0m INFO 12-13 15:22:00 [__init__.py:36] Available plugins for group vllm.platform_plugins:
[36m(pid=375744)[0m INFO 12-13 15:22:00 [__init__.py:38] - ascend -> vllm_ascend:register
[36m(pid=375744)[0m INFO 12-13 15:22:00 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[36m(pid=375744)[0m INFO 12-13 15:22:00 [__init__.py:207] Platform plugin ascend is activated
[36m(pid=375748)[0m [Rank 8 | Local Rank 0] 2025-12-13 15:22:00,632 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(pid=375749)[0m [Rank 9 | Local Rank 0] 2025-12-13 15:22:00,607 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(pid=375758)[0m [Rank 12 | Local Rank 0] 2025-12-13 15:22:00,736 INFO [mindspeed.megatron_adaptor:33] => start to patch features in megatron adaptor.
[36m(pid=375740)[0m WARNING 12-13 15:22:00 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(WorkerDict pid=375734)[0m [Rank 0 | Local Rank 0] 2025-12-13 15:22:00,836 WARNING [mindspeed.args_utils:70] => Failed from megatron.training import get_args, use mindspeed arguments.
[36m(pid=375746)[0m INFO 12-13 15:22:00 [__init__.py:36] Available plugins for group vllm.platform_plugins:
[36m(pid=375746)[0m INFO 12-13 15:22:00 [__init__.py:38] - ascend -> vllm_ascend:register
[36m(pid=375746)[0m INFO 12-13 15:22:00 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[36m(pid=375746)[0m INFO 12-13 15:22:00 [__init__.py:207] Platform plugin ascend is activated
[36m(pid=375745)[0m INFO 12-13 15:22:01 [__init__.py:36] Available plugins for group vllm.platform_plugins:
[36m(pid=375745)[0m INFO 12-13 15:22:01 [__init__.py:38] - ascend -> vllm_ascend:register
[36m(pid=375745)[0m INFO 12-13 15:22:01 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[36m(pid=375745)[0m INFO 12-13 15:22:01 [__init__.py:207] Platform plugin ascend is activated
[36m(pid=375752)[0m INFO 12-13 15:22:01 [__init__.py:36] Available plugins for group vllm.platform_plugins:
[36m(pid=375752)[0m INFO 12-13 15:22:01 [__init__.py:38] - ascend -> vllm_ascend:register
[36m(pid=375752)[0m INFO 12-13 15:22:01 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[36m(pid=375752)[0m INFO 12-13 15:22:01 [__init__.py:207] Platform plugin ascend is activated
[36m(WorkerDict pid=375742)[0m [Rank 2 | Local Rank 0] 2025-12-13 15:22:01,480 WARNING [mindspeed.args_utils:70] => Failed from megatron.training import get_args, use mindspeed arguments.
[36m(pid=375754)[0m INFO 12-13 15:22:01 [__init__.py:36] Available plugins for group vllm.platform_plugins:
[36m(pid=375754)[0m INFO 12-13 15:22:01 [__init__.py:38] - ascend -> vllm_ascend:register
[36m(pid=375754)[0m INFO 12-13 15:22:01 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[36m(pid=375754)[0m INFO 12-13 15:22:01 [__init__.py:207] Platform plugin ascend is activated
[36m(pid=375747)[0m INFO 12-13 15:22:01 [__init__.py:36] Available plugins for group vllm.platform_plugins:
[36m(pid=375747)[0m INFO 12-13 15:22:01 [__init__.py:38] - ascend -> vllm_ascend:register
[36m(pid=375747)[0m INFO 12-13 15:22:01 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[36m(pid=375747)[0m INFO 12-13 15:22:01 [__init__.py:207] Platform plugin ascend is activated
[36m(pid=375750)[0m INFO 12-13 15:22:01 [__init__.py:36] Available plugins for group vllm.platform_plugins:
[36m(pid=375750)[0m INFO 12-13 15:22:01 [__init__.py:38] - ascend -> vllm_ascend:register
[36m(pid=375750)[0m INFO 12-13 15:22:01 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[36m(pid=375750)[0m INFO 12-13 15:22:01 [__init__.py:207] Platform plugin ascend is activated
[36m(pid=375744)[0m WARNING 12-13 15:22:01 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(WorkerDict pid=375740)[0m [Rank 1 | Local Rank 0] 2025-12-13 15:22:01,940 WARNING [mindspeed.args_utils:70] => Failed from megatron.training import get_args, use mindspeed arguments.
[36m(pid=375743)[0m WARNING 12-13 15:22:01 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(pid=375753)[0m INFO 12-13 15:22:02 [__init__.py:36] Available plugins for group vllm.platform_plugins:
[36m(pid=375753)[0m INFO 12-13 15:22:02 [__init__.py:38] - ascend -> vllm_ascend:register
[36m(pid=375753)[0m INFO 12-13 15:22:02 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[36m(pid=375753)[0m INFO 12-13 15:22:02 [__init__.py:207] Platform plugin ascend is activated
[36m(pid=375746)[0m WARNING 12-13 15:22:02 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(pid=375751)[0m INFO 12-13 15:22:02 [__init__.py:36] Available plugins for group vllm.platform_plugins:
[36m(pid=375751)[0m INFO 12-13 15:22:02 [__init__.py:38] - ascend -> vllm_ascend:register
[36m(pid=375751)[0m INFO 12-13 15:22:02 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[36m(pid=375751)[0m INFO 12-13 15:22:02 [__init__.py:207] Platform plugin ascend is activated
[36m(pid=375758)[0m INFO 12-13 15:22:02 [__init__.py:36] Available plugins for group vllm.platform_plugins:
[36m(pid=375758)[0m INFO 12-13 15:22:02 [__init__.py:38] - ascend -> vllm_ascend:register
[36m(pid=375758)[0m INFO 12-13 15:22:02 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[36m(pid=375758)[0m INFO 12-13 15:22:02 [__init__.py:207] Platform plugin ascend is activated
[36m(pid=375745)[0m WARNING 12-13 15:22:02 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(pid=375748)[0m INFO 12-13 15:22:02 [__init__.py:36] Available plugins for group vllm.platform_plugins:
[36m(pid=375748)[0m INFO 12-13 15:22:02 [__init__.py:38] - ascend -> vllm_ascend:register
[36m(pid=375748)[0m INFO 12-13 15:22:02 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[36m(pid=375748)[0m INFO 12-13 15:22:02 [__init__.py:207] Platform plugin ascend is activated
[36m(pid=375752)[0m WARNING 12-13 15:22:02 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(pid=375749)[0m INFO 12-13 15:22:02 [__init__.py:36] Available plugins for group vllm.platform_plugins:
[36m(pid=375749)[0m INFO 12-13 15:22:02 [__init__.py:38] - ascend -> vllm_ascend:register
[36m(pid=375749)[0m INFO 12-13 15:22:02 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[36m(pid=375749)[0m INFO 12-13 15:22:02 [__init__.py:207] Platform plugin ascend is activated
[36m(WorkerDict pid=375744)[0m [Rank 4 | Local Rank 0] 2025-12-13 15:22:02,839 WARNING [mindspeed.args_utils:70] => Failed from megatron.training import get_args, use mindspeed arguments.
[36m(pid=375754)[0m WARNING 12-13 15:22:02 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(WorkerDict pid=375743)[0m [Rank 3 | Local Rank 0] 2025-12-13 15:22:02,976 WARNING [mindspeed.args_utils:70] => Failed from megatron.training import get_args, use mindspeed arguments.
[36m(pid=375747)[0m WARNING 12-13 15:22:02 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(pid=375750)[0m WARNING 12-13 15:22:02 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(WorkerDict pid=375746)[0m [Rank 6 | Local Rank 0] 2025-12-13 15:22:03,235 WARNING [mindspeed.args_utils:70] => Failed from megatron.training import get_args, use mindspeed arguments.
[36m(pid=375751)[0m WARNING 12-13 15:22:03 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(pid=375753)[0m WARNING 12-13 15:22:03 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(WorkerDict pid=375745)[0m [Rank 5 | Local Rank 0] 2025-12-13 15:22:03,492 WARNING [mindspeed.args_utils:70] => Failed from megatron.training import get_args, use mindspeed arguments.
[36m(WorkerDict pid=375752)[0m [Rank 15 | Local Rank 0] 2025-12-13 15:22:03,495 WARNING [mindspeed.args_utils:70] => Failed from megatron.training import get_args, use mindspeed arguments.
[36m(pid=375748)[0m WARNING 12-13 15:22:03 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(pid=375758)[0m WARNING 12-13 15:22:03 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(pid=375749)[0m WARNING 12-13 15:22:03 [_custom_ops.py:20] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[36m(WorkerDict pid=375750)[0m [Rank 10 | Local Rank 0] 2025-12-13 15:22:04,024 WARNING [mindspeed.args_utils:70] => Failed from megatron.training import get_args, use mindspeed arguments.
[36m(WorkerDict pid=375754)[0m [Rank 13 | Local Rank 0] 2025-12-13 15:22:04,084 WARNING [mindspeed.args_utils:70] => Failed from megatron.training import get_args, use mindspeed arguments.
[36m(WorkerDict pid=375747)[0m [Rank 7 | Local Rank 0] 2025-12-13 15:22:04,163 WARNING [mindspeed.args_utils:70] => Failed from megatron.training import get_args, use mindspeed arguments.
[36m(WorkerDict pid=375753)[0m [Rank 14 | Local Rank 0] 2025-12-13 15:22:04,560 WARNING [mindspeed.args_utils:70] => Failed from megatron.training import get_args, use mindspeed arguments.
[36m(WorkerDict pid=375751)[0m [Rank 11 | Local Rank 0] 2025-12-13 15:22:04,761 WARNING [mindspeed.args_utils:70] => Failed from megatron.training import get_args, use mindspeed arguments.
[36m(WorkerDict pid=375749)[0m [Rank 9 | Local Rank 0] 2025-12-13 15:22:04,771 WARNING [mindspeed.args_utils:70] => Failed from megatron.training import get_args, use mindspeed arguments.
[36m(WorkerDict pid=375758)[0m [Rank 12 | Local Rank 0] 2025-12-13 15:22:04,769 WARNING [mindspeed.args_utils:70] => Failed from megatron.training import get_args, use mindspeed arguments.
[36m(WorkerDict pid=375748)[0m [Rank 8 | Local Rank 0] 2025-12-13 15:22:04,814 WARNING [mindspeed.args_utils:70] => Failed from megatron.training import get_args, use mindspeed arguments.
[36m(WorkerDict pid=375742)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff18561ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff183d7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff183d7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='324', worker_launch_time_ms='1765610508406', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='1459253070', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375743)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff0c15dee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xfffee86a7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xfffee86a7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='325', worker_launch_time_ms='1765610508426', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='1481740090', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375747)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff44161ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff2c6a7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff2c6a7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='329', worker_launch_time_ms='1765610508513', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='1060481726', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375734)[0m Model config after override: Qwen3Config {
[36m(WorkerDict pid=375734)[0m   "architectures": [
[36m(WorkerDict pid=375734)[0m     "Qwen3ForCausalLM"
[36m(WorkerDict pid=375734)[0m   ],
[36m(WorkerDict pid=375734)[0m   "attention_bias": false,
[36m(WorkerDict pid=375734)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=375734)[0m   "eos_token_id": 151645,
[36m(WorkerDict pid=375734)[0m   "head_dim": 128,
[36m(WorkerDict pid=375734)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=375734)[0m   "hidden_size": 5120,
[36m(WorkerDict pid=375734)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=375734)[0m   "intermediate_size": 25600,
[36m(WorkerDict pid=375734)[0m   "layer_types": [
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention"
[36m(WorkerDict pid=375734)[0m   ],
[36m(WorkerDict pid=375734)[0m   "max_position_embeddings": 40960,
[36m(WorkerDict pid=375734)[0m   "max_window_layers": 64,
[36m(WorkerDict pid=375734)[0m   "model_type": "qwen3",
[36m(WorkerDict pid=375734)[0m   "num_attention_heads": 64,
[36m(WorkerDict pid=375734)[0m   "num_hidden_layers": 64,
[36m(WorkerDict pid=375734)[0m   "num_key_value_heads": 8,
[36m(WorkerDict pid=375734)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=375734)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=375734)[0m   "rope_scaling": null,
[36m(WorkerDict pid=375734)[0m   "rope_theta": 1000000,
[36m(WorkerDict pid=375734)[0m   "sliding_window": null,
[36m(WorkerDict pid=375734)[0m   "tie_word_embeddings": false,
[36m(WorkerDict pid=375734)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=375734)[0m   "transformers_version": "4.55.0",
[36m(WorkerDict pid=375734)[0m   "use_cache": true,
[36m(WorkerDict pid=375734)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=375734)[0m   "vocab_size": 151936
[36m(WorkerDict pid=375734)[0m }
[36m(WorkerDict pid=375734)[0m 
[36m(WorkerDict pid=375734)[0m Overridden TransformerConfig init config: {'num_layers': 64, 'hidden_size': 5120, 'num_attention_heads': 64, 'num_query_groups': 8, 'ffn_hidden_size': 25600, 'attention_dropout': 0.0, 'hidden_dropout': 0.0, 'kv_channels': 128, 'layernorm_epsilon': 1e-06, 'add_bias_linear': False, 'activation_func': <function silu at 0xffff00741ee0>, 'normalization': 'RMSNorm', 'gated_linear_unit': True, 'pipeline_dtype': torch.bfloat16, 'params_dtype': torch.bfloat16, 'bf16': True, 'tensor_model_parallel_size': 8, 'pipeline_model_parallel_size': 2, 'expert_model_parallel_size': 1, 'expert_tensor_parallel_size': 1, 'virtual_pipeline_model_parallel_size': None, 'context_parallel_size': 1, 'overlap_p2p_comm': False, 'batch_p2p_comm': False, 'sequence_parallel': True, 'variable_seq_lengths': True, 'masked_softmax_fusion': True, 'moe_token_dispatcher_type': 'alltoall', 'use_cpu_initialization': False, 'add_qkv_bias': False, 'qk_layernorm': True, 'recompute_granularity': 'full', 'recompute_modules': ['core_attn'], 'recompute_method': 'uniform', 'recompute_num_layers': 1, 'attention_backend': <AttnBackend.flash: 1>}
[36m(WorkerDict pid=375734)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff00741ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff005b3e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff005b3e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='322', worker_launch_time_ms='1765610508329', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='-150223706', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375740)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff14565ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff143d7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff143d7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='323', worker_launch_time_ms='1765610508364', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='557683157', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375746)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff18639ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff184abe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff184abe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='328', worker_launch_time_ms='1765610508493', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='-723026129', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375742)[0m  > number of parameters on (tensor, pipeline) model parallel rank (2, 0): 2047926272
[36m(WorkerDict pid=375742)[0m load ref weight start
[36m(WorkerDict pid=375742)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=375745)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xfffefc73dee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xfffefc5abe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xfffefc5abe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='327', worker_launch_time_ms='1765610508473', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='1747680634', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375743)[0m  > number of parameters on (tensor, pipeline) model parallel rank (3, 0): 2047926272
[36m(WorkerDict pid=375744)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff08561ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff083d7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff083d7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='326', worker_launch_time_ms='1765610508444', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='1618432160', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375743)[0m load ref weight start
[36m(WorkerDict pid=375743)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=375734)[0m  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 2047926272
[36m(WorkerDict pid=375747)[0m  > number of parameters on (tensor, pipeline) model parallel rank (7, 0): 2047926272
[36m(WorkerDict pid=375747)[0m load ref weight start
[36m(WorkerDict pid=375747)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=375734)[0m load ref weight start
[36m(WorkerDict pid=375734)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=375745)[0m  > number of parameters on (tensor, pipeline) model parallel rank (5, 0): 2047926272
[36m(WorkerDict pid=375740)[0m  > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 2047926272
[36m(WorkerDict pid=375740)[0m load ref weight start
[36m(WorkerDict pid=375740)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=375746)[0m  > number of parameters on (tensor, pipeline) model parallel rank (6, 0): 2047926272
[36m(WorkerDict pid=375746)[0m load ref weight start
[36m(WorkerDict pid=375746)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=375745)[0m load ref weight start
[36m(WorkerDict pid=375745)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=375744)[0m  > number of parameters on (tensor, pipeline) model parallel rank (4, 0): 2047926272
[36m(WorkerDict pid=375744)[0m load ref weight start
[36m(WorkerDict pid=375744)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=375734)[0m loading embeddings...
[36m(WorkerDict pid=375754)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff04561ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff043d7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff043d7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='336', worker_launch_time_ms='1765610508662', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='2045057609', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375758)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff0c171ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xfffee86bbe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xfffee86bbe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='337', worker_launch_time_ms='1765610508683', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='277292338', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375748)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff50175ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff186bbe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff186bbe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='330', worker_launch_time_ms='1765610508532', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='-613850428', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375749)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff0c639ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff0c4a7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff0c4a7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='331', worker_launch_time_ms='1765610508553', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='915738454', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375751)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff58265ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff580dbe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff580dbe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='333', worker_launch_time_ms='1765610508597', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='-238701692', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375750)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff3c171ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff206bbe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff206bbe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='332', worker_launch_time_ms='1765610508574', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='1331188970', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375758)[0m  > number of parameters on (tensor, pipeline) model parallel rank (4, 1): 2047931392
[36m(WorkerDict pid=375758)[0m load ref weight start
[36m(WorkerDict pid=375758)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=375754)[0m  > number of parameters on (tensor, pipeline) model parallel rank (5, 1): 2047931392
[36m(WorkerDict pid=375754)[0m load ref weight start
[36m(WorkerDict pid=375754)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=375748)[0m  > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 2047931392
[36m(WorkerDict pid=375748)[0m load ref weight start
[36m(WorkerDict pid=375748)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=375752)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff28461ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff282d7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff282d7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='334', worker_launch_time_ms='1765610508620', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='206968578', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375750)[0m  > number of parameters on (tensor, pipeline) model parallel rank (2, 1): 2047931392
[36m(WorkerDict pid=375750)[0m load ref weight start
[36m(WorkerDict pid=375750)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=375749)[0m  > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 2047931392
[36m(WorkerDict pid=375749)[0m load ref weight start
[36m(WorkerDict pid=375749)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=375751)[0m  > number of parameters on (tensor, pipeline) model parallel rank (3, 1): 2047931392
[36m(WorkerDict pid=375751)[0m load ref weight start
[36m(WorkerDict pid=375751)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=375753)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff30561ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff303d7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff303d7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='335', worker_launch_time_ms='1765610508641', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='314639389', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375752)[0m  > number of parameters on (tensor, pipeline) model parallel rank (7, 1): 2047931392
[36m(WorkerDict pid=375752)[0m load ref weight start
[36m(WorkerDict pid=375752)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=375753)[0m  > number of parameters on (tensor, pipeline) model parallel rank (6, 1): 2047931392
[36m(WorkerDict pid=375753)[0m load ref weight start
[36m(WorkerDict pid=375753)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=375734)[0m loading layer #0, with layer_name model.layers.0...
[36m(WorkerDict pid=375734)[0m loading layer #1, with layer_name model.layers.1...
[36m(WorkerDict pid=375734)[0m loading layer #2, with layer_name model.layers.2...
[36m(WorkerDict pid=375734)[0m loading layer #3, with layer_name model.layers.3...
[36m(WorkerDict pid=375734)[0m loading layer #4, with layer_name model.layers.4...
[36m(WorkerDict pid=375734)[0m loading layer #5, with layer_name model.layers.5...
[36m(WorkerDict pid=375734)[0m loading layer #6, with layer_name model.layers.6...
[36m(WorkerDict pid=375734)[0m loading layer #7, with layer_name model.layers.7...
[36m(WorkerDict pid=375734)[0m loading layer #8, with layer_name model.layers.8...
[36m(WorkerDict pid=375734)[0m loading layer #9, with layer_name model.layers.9...
[36m(WorkerDict pid=375734)[0m loading layer #10, with layer_name model.layers.10...
[36m(WorkerDict pid=375734)[0m loading layer #11, with layer_name model.layers.11...
[36m(WorkerDict pid=375734)[0m loading layer #12, with layer_name model.layers.12...
[36m(WorkerDict pid=375734)[0m loading layer #13, with layer_name model.layers.13...
[36m(WorkerDict pid=375734)[0m loading layer #14, with layer_name model.layers.14...
[36m(WorkerDict pid=375734)[0m loading layer #15, with layer_name model.layers.15...
[36m(WorkerDict pid=375734)[0m loading layer #16, with layer_name model.layers.16...
[36m(WorkerDict pid=375734)[0m loading layer #17, with layer_name model.layers.17...
[36m(WorkerDict pid=375734)[0m loading layer #18, with layer_name model.layers.18...
[36m(WorkerDict pid=375734)[0m loading layer #19, with layer_name model.layers.19...
[36m(WorkerDict pid=375734)[0m loading layer #20, with layer_name model.layers.20...
[36m(WorkerDict pid=375734)[0m loading layer #21, with layer_name model.layers.21...
[36m(WorkerDict pid=375734)[0m loading layer #22, with layer_name model.layers.22...
[36m(WorkerDict pid=375734)[0m loading layer #23, with layer_name model.layers.23...
[36m(WorkerDict pid=375734)[0m loading layer #24, with layer_name model.layers.24...
[36m(WorkerDict pid=375734)[0m loading layer #25, with layer_name model.layers.25...
[36m(WorkerDict pid=375734)[0m loading layer #26, with layer_name model.layers.26...
[36m(WorkerDict pid=375734)[0m loading layer #27, with layer_name model.layers.27...
[36m(WorkerDict pid=375734)[0m loading layer #28, with layer_name model.layers.28...
[36m(WorkerDict pid=375734)[0m loading layer #29, with layer_name model.layers.29...
[36m(WorkerDict pid=375734)[0m loading layer #30, with layer_name model.layers.30...
[36m(WorkerDict pid=375734)[0m loading layer #31, with layer_name model.layers.31...
[36m(WorkerDict pid=375734)[0m loading layer #32, with layer_name model.layers.32...
[36m(WorkerDict pid=375734)[0m loading layer #33, with layer_name model.layers.33...
[36m(WorkerDict pid=375734)[0m loading layer #34, with layer_name model.layers.34...
[36m(WorkerDict pid=375734)[0m loading layer #35, with layer_name model.layers.35...
[36m(WorkerDict pid=375734)[0m loading layer #36, with layer_name model.layers.36...
[36m(WorkerDict pid=375734)[0m loading layer #37, with layer_name model.layers.37...
[36m(WorkerDict pid=375734)[0m loading layer #38, with layer_name model.layers.38...
[36m(WorkerDict pid=375734)[0m loading layer #39, with layer_name model.layers.39...
[36m(WorkerDict pid=375734)[0m loading layer #40, with layer_name model.layers.40...
[36m(WorkerDict pid=375734)[0m loading layer #41, with layer_name model.layers.41...
[36m(WorkerDict pid=375734)[0m loading layer #42, with layer_name model.layers.42...
[36m(WorkerDict pid=375734)[0m loading layer #43, with layer_name model.layers.43...
[36m(WorkerDict pid=375734)[0m loading layer #44, with layer_name model.layers.44...
[36m(WorkerDict pid=375734)[0m loading layer #45, with layer_name model.layers.45...
[36m(WorkerDict pid=375734)[0m loading layer #46, with layer_name model.layers.46...
[36m(WorkerDict pid=375734)[0m loading layer #47, with layer_name model.layers.47...
[36m(WorkerDict pid=375734)[0m loading layer #48, with layer_name model.layers.48...
[36m(WorkerDict pid=375734)[0m loading layer #49, with layer_name model.layers.49...
[36m(WorkerDict pid=375734)[0m loading layer #50, with layer_name model.layers.50...
[36m(WorkerDict pid=375734)[0m loading layer #51, with layer_name model.layers.51...
[36m(WorkerDict pid=375734)[0m loading layer #52, with layer_name model.layers.52...
[36m(WorkerDict pid=375734)[0m loading layer #53, with layer_name model.layers.53...
[36m(WorkerDict pid=375734)[0m loading layer #54, with layer_name model.layers.54...
[36m(WorkerDict pid=375734)[0m loading layer #55, with layer_name model.layers.55...
[36m(WorkerDict pid=375734)[0m loading layer #56, with layer_name model.layers.56...
[36m(WorkerDict pid=375734)[0m loading layer #57, with layer_name model.layers.57...
[36m(WorkerDict pid=375734)[0m loading layer #58, with layer_name model.layers.58...
[36m(WorkerDict pid=375734)[0m loading layer #59, with layer_name model.layers.59...
[36m(WorkerDict pid=375734)[0m loading layer #60, with layer_name model.layers.60...
[36m(WorkerDict pid=375734)[0m loading layer #61, with layer_name model.layers.61...
[36m(WorkerDict pid=375734)[0m loading layer #62, with layer_name model.layers.62...
[36m(WorkerDict pid=375734)[0m loading layer #63, with layer_name model.layers.63...
[36m(WorkerDict pid=375734)[0m loading final layernorm...
[36m(WorkerDict pid=375734)[0m loading lm_head...
[36m(WorkerDict pid=375742)[0m [Warining] Because actor tp size == 1, set sp to False
[36m(WorkerDict pid=375742)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff18561ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff183d7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff183d7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='324', worker_launch_time_ms='1765610508406', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='1459253070', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375746)[0m [Warining] Because actor tp size == 1, set sp to False
[36m(WorkerDict pid=375746)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff18639ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff184abe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff184abe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='328', worker_launch_time_ms='1765610508493', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='-723026129', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375743)[0m [Warining] Because actor tp size == 1, set sp to False
[36m(WorkerDict pid=375744)[0m [Warining] Because actor tp size == 1, set sp to False
[36m(WorkerDict pid=375744)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff08561ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff083d7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff083d7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='326', worker_launch_time_ms='1765610508444', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='1618432160', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375747)[0m [Warining] Because actor tp size == 1, set sp to False
[36m(WorkerDict pid=375747)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff44161ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff2c6a7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff2c6a7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='329', worker_launch_time_ms='1765610508513', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='1060481726', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375752)[0m [Warining] Because actor tp size == 1, set sp to False
[36m(WorkerDict pid=375752)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff28461ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff282d7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff282d7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='334', worker_launch_time_ms='1765610508620', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='206968578', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375751)[0m [Warining] Because actor tp size == 1, set sp to False
[36m(WorkerDict pid=375751)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff58265ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff580dbe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff580dbe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='333', worker_launch_time_ms='1765610508597', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='-238701692', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375750)[0m [Warining] Because actor tp size == 1, set sp to False
[36m(WorkerDict pid=375750)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff3c171ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff206bbe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff206bbe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='332', worker_launch_time_ms='1765610508574', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='1331188970', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375748)[0m [Warining] Because actor tp size == 1, set sp to False
[36m(WorkerDict pid=375748)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff50175ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff186bbe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff186bbe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='330', worker_launch_time_ms='1765610508532', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='-613850428', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375749)[0m [Warining] Because actor tp size == 1, set sp to False
[36m(WorkerDict pid=375749)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff0c639ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff0c4a7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff0c4a7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='331', worker_launch_time_ms='1765610508553', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='915738454', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375753)[0m [Warining] Because actor tp size == 1, set sp to False
[36m(WorkerDict pid=375753)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff30561ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff303d7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff303d7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='335', worker_launch_time_ms='1765610508641', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='314639389', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375754)[0m [Warining] Because actor tp size == 1, set sp to False
[36m(WorkerDict pid=375758)[0m [Warining] Because actor tp size == 1, set sp to False
[36m(WorkerDict pid=375758)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff0c171ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xfffee86bbe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xfffee86bbe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='337', worker_launch_time_ms='1765610508683', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='277292338', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375745)[0m [Warining] Because actor tp size == 1, set sp to False
[36m(WorkerDict pid=375745)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xfffefc73dee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xfffefc5abe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xfffefc5abe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='327', worker_launch_time_ms='1765610508473', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='1747680634', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375740)[0m [Warining] Because actor tp size == 1, set sp to False
[36m(WorkerDict pid=375740)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff14565ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff143d7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff143d7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='323', worker_launch_time_ms='1765610508364', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='557683157', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375743)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff0c15dee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xfffee86a7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xfffee86a7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='325', worker_launch_time_ms='1765610508426', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='1481740090', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375754)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff04561ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff043d7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff043d7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='336', worker_launch_time_ms='1765610508662', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='2045057609', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375734)[0m loading megatron ckpt done, time elapsed 37.21698617935181s
[36m(WorkerDict pid=375734)[0m [Warining] Because actor tp size == 1, set sp to False
[36m(WorkerDict pid=375734)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff00741ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff005b3e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff005b3e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='322', worker_launch_time_ms='1765610508329', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='-150223706', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375742)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff18561ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff183d7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff183d7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='324', worker_launch_time_ms='1765610508406', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='1459253070', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375734)[0m Model config after override: Qwen3Config {
[36m(WorkerDict pid=375734)[0m   "architectures": [
[36m(WorkerDict pid=375734)[0m     "Qwen3ForCausalLM"
[36m(WorkerDict pid=375734)[0m   ],
[36m(WorkerDict pid=375734)[0m   "attention_bias": false,
[36m(WorkerDict pid=375734)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=375734)[0m   "eos_token_id": 151645,
[36m(WorkerDict pid=375734)[0m   "head_dim": 128,
[36m(WorkerDict pid=375734)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=375734)[0m   "hidden_size": 5120,
[36m(WorkerDict pid=375734)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=375734)[0m   "intermediate_size": 25600,
[36m(WorkerDict pid=375734)[0m   "layer_types": [
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention",
[36m(WorkerDict pid=375734)[0m     "full_attention"
[36m(WorkerDict pid=375734)[0m   ],
[36m(WorkerDict pid=375734)[0m   "max_position_embeddings": 40960,
[36m(WorkerDict pid=375734)[0m   "max_window_layers": 64,
[36m(WorkerDict pid=375734)[0m   "model_type": "qwen3",
[36m(WorkerDict pid=375734)[0m   "num_attention_heads": 64,
[36m(WorkerDict pid=375734)[0m   "num_hidden_layers": 64,
[36m(WorkerDict pid=375734)[0m   "num_key_value_heads": 8,
[36m(WorkerDict pid=375734)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=375734)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=375734)[0m   "rope_scaling": null,
[36m(WorkerDict pid=375734)[0m   "rope_theta": 1000000,
[36m(WorkerDict pid=375734)[0m   "sliding_window": null,
[36m(WorkerDict pid=375734)[0m   "tie_word_embeddings": false,
[36m(WorkerDict pid=375734)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=375734)[0m   "transformers_version": "4.55.0",
[36m(WorkerDict pid=375734)[0m   "use_cache": true,
[36m(WorkerDict pid=375734)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=375734)[0m   "vocab_size": 151936
[36m(WorkerDict pid=375734)[0m }
[36m(WorkerDict pid=375734)[0m 
[36m(WorkerDict pid=375734)[0m Overridden TransformerConfig init config: {'num_layers': 64, 'hidden_size': 5120, 'num_attention_heads': 64, 'num_query_groups': 8, 'ffn_hidden_size': 25600, 'attention_dropout': 0.0, 'hidden_dropout': 0.0, 'kv_channels': 128, 'layernorm_epsilon': 1e-06, 'add_bias_linear': False, 'activation_func': <function silu at 0xffff00741ee0>, 'normalization': 'RMSNorm', 'gated_linear_unit': True, 'pipeline_dtype': torch.bfloat16, 'params_dtype': torch.bfloat16, 'bf16': True, 'tensor_model_parallel_size': 8, 'pipeline_model_parallel_size': 2, 'expert_model_parallel_size': 1, 'expert_tensor_parallel_size': 1, 'virtual_pipeline_model_parallel_size': None, 'context_parallel_size': 1, 'overlap_p2p_comm': False, 'batch_p2p_comm': False, 'sequence_parallel': True, 'variable_seq_lengths': True, 'masked_softmax_fusion': True, 'moe_token_dispatcher_type': 'alltoall', 'use_cpu_initialization': False, 'add_qkv_bias': False, 'qk_layernorm': True, 'recompute_granularity': 'full', 'recompute_modules': ['core_attn'], 'recompute_method': 'uniform', 'recompute_num_layers': 1, 'attention_backend': <AttnBackend.flash: 1>}
[36m(WorkerDict pid=375734)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff00741ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff005b3e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff005b3e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='322', worker_launch_time_ms='1765610508329', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='-150223706', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375747)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff44161ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff2c6a7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff2c6a7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='329', worker_launch_time_ms='1765610508513', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='1060481726', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375752)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff28461ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff282d7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff282d7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='334', worker_launch_time_ms='1765610508620', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='206968578', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375751)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff58265ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff580dbe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff580dbe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='333', worker_launch_time_ms='1765610508597', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='-238701692', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375750)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff3c171ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff206bbe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff206bbe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='332', worker_launch_time_ms='1765610508574', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='1331188970', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375748)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff50175ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff186bbe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff186bbe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='330', worker_launch_time_ms='1765610508532', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='-613850428', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375753)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff30561ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff303d7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff303d7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='335', worker_launch_time_ms='1765610508641', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='314639389', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375758)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff0c171ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xfffee86bbe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xfffee86bbe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='337', worker_launch_time_ms='1765610508683', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='277292338', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375745)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xfffefc73dee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xfffefc5abe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xfffefc5abe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='327', worker_launch_time_ms='1765610508473', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='1747680634', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375740)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff14565ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff143d7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff143d7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='323', worker_launch_time_ms='1765610508364', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='557683157', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375746)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff18639ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff184abe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff184abe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='328', worker_launch_time_ms='1765610508493', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='-723026129', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375743)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff0c15dee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xfffee86a7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xfffee86a7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='325', worker_launch_time_ms='1765610508426', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='1481740090', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375758)[0m  > number of parameters on (tensor, pipeline) model parallel rank (4, 1): 2047931392
[36m(WorkerDict pid=375748)[0m  > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 2047931392
[36m(WorkerDict pid=375744)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff08561ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff083d7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff083d7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='326', worker_launch_time_ms='1765610508444', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='1618432160', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375742)[0m  > number of parameters on (tensor, pipeline) model parallel rank (2, 0): 2047926272
[36m(WorkerDict pid=375734)[0m  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 2047926272
[36m(WorkerDict pid=375734)[0m [Rank 0 | Local Rank 0] 2025-12-13 15:23:02,538 INFO [megatron.core.distributed.distributed_data_parallel:532] => Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=True, overlap_grad_reduce=False, overlap_param_gather=False, align_param_gather=False, use_distributed_optimizer=True, num_distributed_optimizer_instances=1, check_for_nan_in_grad=False, check_for_large_grads=False, bucket_size=None, pad_buckets_for_high_nccl_busbw=False, average_in_collective=False, fp8_param_gather=False, use_custom_fsdp=False, data_parallel_sharding_strategy='no_shard', gradient_reduce_div_fusion=True, suggested_communication_unit_size=None, preserve_fp32_weights=True, keep_fp8_transpose_cache_when_using_custom_fsdp=False)
[36m(WorkerDict pid=375747)[0m  > number of parameters on (tensor, pipeline) model parallel rank (7, 0): 2047926272
[36m(WorkerDict pid=375752)[0m  > number of parameters on (tensor, pipeline) model parallel rank (7, 1): 2047931392
[36m(WorkerDict pid=375751)[0m  > number of parameters on (tensor, pipeline) model parallel rank (3, 1): 2047931392
[36m(WorkerDict pid=375750)[0m  > number of parameters on (tensor, pipeline) model parallel rank (2, 1): 2047931392
[36m(WorkerDict pid=375748)[0m [Rank 8 | Local Rank 0] 2025-12-13 15:23:02,574 INFO [megatron.core.distributed.param_and_grad_buffer:553] => Number of buckets for gradient all-reduce / reduce-scatter: 1
[36m(WorkerDict pid=375748)[0m Params for bucket 1 (2047931392 elements, 2047931392 padded size):
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.9.mlp.linear_fc1.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.24.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.28.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.20.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.16.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.13.mlp.linear_fc2.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.9.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.9.self_attention.linear_proj.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.6.mlp.linear_fc1.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.11.mlp.linear_fc1.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.5.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.2.mlp.linear_fc1.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.30.mlp.linear_fc1.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.31.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.26.mlp.linear_fc1.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.22.mlp.linear_fc1.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.18.mlp.linear_fc1.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.16.mlp.linear_fc2.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.16.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.15.self_attention.linear_proj.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.14.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.4.mlp.linear_fc2.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.2.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.1.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.4.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.23.mlp.linear_fc2.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.27.mlp.linear_fc2.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.25.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.final_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.30.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.29.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.26.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.22.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.21.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.19.mlp.linear_fc2.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.18.self_attention.linear_proj.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.10.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.3.mlp.linear_fc2.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.16.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.10.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.0.mlp.linear_fc1.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.23.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.27.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.24.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.30.self_attention.linear_proj.weight
[36m(WorkerDict pid=375748)[0m 	module.output_layer.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.28.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.26.self_attention.linear_proj.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.22.self_attention.linear_proj.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.20.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.19.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.8.mlp.linear_fc2.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.0.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.9.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.11.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.31.mlp.linear_fc2.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.29.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.25.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.21.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.17.self_attention.linear_proj.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.15.mlp.linear_fc1.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.15.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.13.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.12.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.11.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.7.mlp.linear_fc1.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.7.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.0.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.11.mlp.linear_fc2.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.2.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.0.mlp.linear_fc2.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.17.mlp.linear_fc2.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.16.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.4.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.31.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.27.mlp.linear_fc1.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.23.mlp.linear_fc1.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.19.mlp.linear_fc1.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.17.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.15.mlp.linear_fc2.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.13.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.12.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.9.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.5.self_attention.linear_proj.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.12.mlp.linear_fc1.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.17.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.15.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.8.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.8.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.27.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.24.mlp.linear_fc2.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.30.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.28.mlp.linear_fc2.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.26.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.23.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.22.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.20.mlp.linear_fc2.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.19.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.18.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.2.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.14.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.11.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.5.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.23.self_attention.linear_proj.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.27.self_attention.linear_proj.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.24.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.28.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.25.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.31.mlp.linear_fc1.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.31.self_attention.linear_proj.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.29.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.21.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.20.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.19.self_attention.linear_proj.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.15.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.11.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.2.self_attention.linear_proj.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.3.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.16.self_attention.linear_proj.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.3.self_attention.linear_proj.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.30.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.31.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.26.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.22.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.18.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.16.mlp.linear_fc1.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.14.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.6.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.6.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.4.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.0.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.17.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.8.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.12.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.24.mlp.linear_fc1.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.28.mlp.linear_fc1.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.20.mlp.linear_fc1.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.12.self_attention.linear_proj.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.10.mlp.linear_fc2.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.8.self_attention.linear_proj.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.7.mlp.linear_fc2.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.7.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.7.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.7.self_attention.linear_proj.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.13.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.17.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.3.mlp.linear_fc1.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.5.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.24.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.25.mlp.linear_fc2.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.31.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.29.mlp.linear_fc2.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.28.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.23.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.27.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.21.mlp.linear_fc2.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.20.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.19.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.13.self_attention.linear_proj.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.9.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.1.mlp.linear_fc1.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.12.mlp.linear_fc2.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.14.mlp.linear_fc2.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.10.self_attention.linear_proj.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.6.self_attention.linear_proj.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.26.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.24.self_attention.linear_proj.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.28.self_attention.linear_proj.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.25.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.30.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.29.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.22.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.21.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.20.self_attention.linear_proj.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.18.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.11.self_attention.linear_proj.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.5.mlp.linear_fc1.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.3.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.1.self_attention.linear_proj.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.6.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.0.self_attention.linear_proj.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.23.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.31.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.27.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.19.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.15.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.14.self_attention.linear_proj.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.13.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.10.mlp.linear_fc1.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.3.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.2.mlp.linear_fc2.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.12.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.29.mlp.linear_fc1.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.25.mlp.linear_fc1.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.21.mlp.linear_fc1.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.17.mlp.linear_fc1.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.16.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.13.mlp.linear_fc1.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.10.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.9.mlp.linear_fc2.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.8.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.18.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.4.mlp.linear_fc1.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.14.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.5.mlp.linear_fc2.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.28.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.25.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.30.mlp.linear_fc2.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.29.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.22.mlp.linear_fc2.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.26.mlp.linear_fc2.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.21.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.20.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.18.mlp.linear_fc2.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.8.mlp.linear_fc1.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.6.mlp.linear_fc2.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.4.self_attention.linear_proj.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.14.mlp.linear_fc1.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.1.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.1.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.10.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.26.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.23.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.25.self_attention.linear_proj.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.27.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.30.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.29.self_attention.linear_proj.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.22.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.21.self_attention.linear_proj.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.19.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.18.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375748)[0m 	module.decoder.layers.1.mlp.linear_fc2.weight
[36m(WorkerDict pid=375753)[0m  > number of parameters on (tensor, pipeline) model parallel rank (6, 1): 2047931392
[36m(WorkerDict pid=375754)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff04561ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff043d7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff043d7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='336', worker_launch_time_ms='1765610508662', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='2045057609', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375734)[0m [Rank 0 | Local Rank 0] 2025-12-13 15:23:02,614 INFO [megatron.core.distributed.param_and_grad_buffer:553] => Number of buckets for gradient all-reduce / reduce-scatter: 1
[36m(WorkerDict pid=375734)[0m Params for bucket 1 (2047926272 elements, 2047926272 padded size):
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.1.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.29.mlp.linear_fc2.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.31.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.27.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.24.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.27.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.22.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.20.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.18.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.14.mlp.linear_fc2.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.14.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.10.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.6.mlp.linear_fc2.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.10.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.23.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.25.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.30.mlp.linear_fc2.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.29.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.28.mlp.linear_fc1.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.28.self_attention.linear_proj.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.22.mlp.linear_fc2.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.23.mlp.linear_fc2.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.16.mlp.linear_fc1.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.15.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.2.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.15.mlp.linear_fc1.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.4.mlp.linear_fc2.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.22.mlp.linear_fc1.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.31.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.27.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.25.mlp.linear_fc1.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.24.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.21.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.21.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.17.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.16.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.16.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.10.self_attention.linear_proj.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.8.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.3.self_attention.linear_proj.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.5.mlp.linear_fc2.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.3.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.30.self_attention.linear_proj.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.28.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.27.self_attention.linear_proj.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.20.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.18.mlp.linear_fc2.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.16.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.15.self_attention.linear_proj.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.13.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.10.mlp.linear_fc2.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.13.self_attention.linear_proj.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.5.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.7.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.6.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.4.self_attention.linear_proj.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.26.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.31.self_attention.linear_proj.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.28.mlp.linear_fc2.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.24.self_attention.linear_proj.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.26.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.21.self_attention.linear_proj.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.18.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.14.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.13.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.12.mlp.linear_fc2.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.11.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.1.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.30.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.29.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.27.mlp.linear_fc1.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.24.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.26.mlp.linear_fc2.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.21.mlp.linear_fc1.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.21.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.20.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.17.mlp.linear_fc1.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.12.mlp.linear_fc1.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.12.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.1.mlp.linear_fc1.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.1.self_attention.linear_proj.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.9.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.2.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.23.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.25.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.30.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.25.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.20.mlp.linear_fc2.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.17.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.16.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.13.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.10.mlp.linear_fc1.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.18.mlp.linear_fc1.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.5.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.14.self_attention.linear_proj.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.7.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.0.mlp.linear_fc1.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.23.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.26.self_attention.linear_proj.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.22.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.24.mlp.linear_fc2.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.29.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.28.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.27.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.19.mlp.linear_fc1.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.19.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.17.self_attention.linear_proj.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.7.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.5.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.11.mlp.linear_fc2.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.30.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.30.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.31.mlp.linear_fc1.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.26.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.23.mlp.linear_fc1.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.25.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.26.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.22.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.20.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.19.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.8.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.6.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.0.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.1.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.8.mlp.linear_fc1.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.25.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.26.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.23.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.20.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.17.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.17.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.12.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.10.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.9.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.17.mlp.linear_fc2.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.7.self_attention.linear_proj.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.4.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.4.mlp.linear_fc1.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.9.mlp.linear_fc2.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.11.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.28.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.24.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.22.self_attention.linear_proj.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.19.mlp.linear_fc2.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.15.mlp.linear_fc2.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.12.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.11.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.8.self_attention.linear_proj.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.8.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.2.mlp.linear_fc2.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.2.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.0.mlp.linear_fc2.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.2.mlp.linear_fc1.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.1.mlp.linear_fc2.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.0.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.22.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.23.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.31.mlp.linear_fc2.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.27.mlp.linear_fc2.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.21.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.19.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.18.self_attention.linear_proj.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.16.mlp.linear_fc2.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.14.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.8.mlp.linear_fc2.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.3.mlp.linear_fc1.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.0.self_attention.linear_proj.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.3.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.7.mlp.linear_fc1.weight
[36m(WorkerDict pid=375734)[0m 	module.embedding.word_embeddings.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.30.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.28.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.23.self_attention.linear_proj.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.26.mlp.linear_fc1.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.22.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.25.mlp.linear_fc2.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.21.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.13.mlp.linear_fc1.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.12.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.11.mlp.linear_fc1.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.3.mlp.linear_fc2.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.2.self_attention.linear_proj.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.9.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.6.mlp.linear_fc1.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.5.self_attention.linear_proj.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.13.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.24.mlp.linear_fc1.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.29.mlp.linear_fc1.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.31.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.29.self_attention.linear_proj.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.27.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.21.mlp.linear_fc2.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.19.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.19.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.18.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.16.self_attention.linear_proj.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.12.self_attention.linear_proj.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.7.mlp.linear_fc2.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.4.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.9.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.0.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.8.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.25.self_attention.linear_proj.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.30.mlp.linear_fc1.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.29.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.29.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.28.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.20.mlp.linear_fc1.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.18.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.15.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.15.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.11.mlp.linear_fc1.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.11.self_attention.linear_proj.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.5.mlp.linear_fc1.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.14.mlp.linear_fc1.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.9.mlp.linear_fc1.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.3.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.14.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.10.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.4.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.31.self_attention.q_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.31.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.20.self_attention.linear_proj.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.19.self_attention.linear_proj.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.18.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.16.self_attention.linear_qkv.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.15.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.13.mlp.linear_fc2.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.9.self_attention.linear_proj.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.6.self_attention.k_layernorm.weight
[36m(WorkerDict pid=375734)[0m 	module.decoder.layers.6.self_attention.linear_proj.weight
[36m(WorkerDict pid=375758)[0m actor_module: 1
[36m(WorkerDict pid=375758)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=375742)[0m actor_module: 1
[36m(WorkerDict pid=375742)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=375734)[0m actor_module: 1
[36m(WorkerDict pid=375734)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=375745)[0m  > number of parameters on (tensor, pipeline) model parallel rank (5, 0): 2047926272
[36m(WorkerDict pid=375740)[0m  > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 2047926272
[36m(WorkerDict pid=375746)[0m  > number of parameters on (tensor, pipeline) model parallel rank (6, 0): 2047926272
[36m(WorkerDict pid=375743)[0m  > number of parameters on (tensor, pipeline) model parallel rank (3, 0): 2047926272
[36m(WorkerDict pid=375744)[0m  > number of parameters on (tensor, pipeline) model parallel rank (4, 0): 2047926272
[36m(WorkerDict pid=375747)[0m actor_module: 1
[36m(WorkerDict pid=375747)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=375752)[0m actor_module: 1
[36m(WorkerDict pid=375752)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=375751)[0m actor_module: 1
[36m(WorkerDict pid=375751)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=375750)[0m actor_module: 1
[36m(WorkerDict pid=375750)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=375748)[0m actor_module: 1
[36m(WorkerDict pid=375748)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=375749)[0m TF config: TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff0c639ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff0c4a7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff0c4a7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=False, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='331', worker_launch_time_ms='1765610508553', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='915738454', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375753)[0m actor_module: 1
[36m(WorkerDict pid=375753)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=375745)[0m actor_module: 1
[36m(WorkerDict pid=375745)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=375740)[0m actor_module: 1
[36m(WorkerDict pid=375740)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=375746)[0m actor_module: 1
[36m(WorkerDict pid=375746)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=375743)[0m actor_module: 1
[36m(WorkerDict pid=375743)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=375754)[0m  > number of parameters on (tensor, pipeline) model parallel rank (5, 1): 2047931392
[36m(WorkerDict pid=375744)[0m actor_module: 1
[36m(WorkerDict pid=375744)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=375749)[0m  > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 2047931392
[36m(WorkerDict pid=375754)[0m actor_module: 1
[36m(WorkerDict pid=375754)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=375734)[0m loading embeddings...
[36m(WorkerDict pid=375749)[0m actor_module: 1
[36m(WorkerDict pid=375749)[0m load from local dir /home/data/Qwen3-32B
[36m(WorkerDict pid=375734)[0m loading layer #0, with layer_name model.layers.0...
[36m(WorkerDict pid=375734)[0m loading layer #1, with layer_name model.layers.1...
[36m(WorkerDict pid=375734)[0m loading layer #2, with layer_name model.layers.2...
[36m(WorkerDict pid=375734)[0m loading layer #3, with layer_name model.layers.3...
[36m(WorkerDict pid=375734)[0m loading layer #4, with layer_name model.layers.4...
[36m(WorkerDict pid=375734)[0m loading layer #5, with layer_name model.layers.5...
[36m(WorkerDict pid=375734)[0m loading layer #6, with layer_name model.layers.6...
[36m(WorkerDict pid=375734)[0m loading layer #7, with layer_name model.layers.7...
[36m(WorkerDict pid=375734)[0m loading layer #8, with layer_name model.layers.8...
[36m(WorkerDict pid=375734)[0m loading layer #9, with layer_name model.layers.9...
[36m(WorkerDict pid=375734)[0m loading layer #10, with layer_name model.layers.10...
[36m(WorkerDict pid=375734)[0m loading layer #11, with layer_name model.layers.11...
[36m(WorkerDict pid=375734)[0m loading layer #12, with layer_name model.layers.12...
[36m(WorkerDict pid=375734)[0m loading layer #13, with layer_name model.layers.13...
[36m(WorkerDict pid=375734)[0m loading layer #14, with layer_name model.layers.14...
[36m(WorkerDict pid=375734)[0m loading layer #15, with layer_name model.layers.15...
[36m(WorkerDict pid=375734)[0m loading layer #16, with layer_name model.layers.16...
[36m(WorkerDict pid=375734)[0m loading layer #17, with layer_name model.layers.17...
[36m(WorkerDict pid=375734)[0m loading layer #18, with layer_name model.layers.18...
[36m(WorkerDict pid=375734)[0m loading layer #19, with layer_name model.layers.19...
[36m(WorkerDict pid=375734)[0m loading layer #20, with layer_name model.layers.20...
[36m(WorkerDict pid=375734)[0m loading layer #21, with layer_name model.layers.21...
[36m(WorkerDict pid=375734)[0m loading layer #22, with layer_name model.layers.22...
[36m(WorkerDict pid=375734)[0m loading layer #23, with layer_name model.layers.23...
[36m(WorkerDict pid=375734)[0m loading layer #24, with layer_name model.layers.24...
[36m(WorkerDict pid=375734)[0m loading layer #25, with layer_name model.layers.25...
[36m(WorkerDict pid=375734)[0m loading layer #26, with layer_name model.layers.26...
[36m(WorkerDict pid=375734)[0m loading layer #27, with layer_name model.layers.27...
[36m(WorkerDict pid=375734)[0m loading layer #28, with layer_name model.layers.28...
[36m(WorkerDict pid=375734)[0m loading layer #29, with layer_name model.layers.29...
[36m(WorkerDict pid=375734)[0m loading layer #30, with layer_name model.layers.30...
[36m(WorkerDict pid=375734)[0m loading layer #31, with layer_name model.layers.31...
[36m(WorkerDict pid=375734)[0m loading layer #32, with layer_name model.layers.32...
[36m(WorkerDict pid=375734)[0m loading layer #33, with layer_name model.layers.33...
[36m(WorkerDict pid=375734)[0m loading layer #34, with layer_name model.layers.34...
[36m(WorkerDict pid=375734)[0m loading layer #35, with layer_name model.layers.35...
[36m(WorkerDict pid=375734)[0m loading layer #36, with layer_name model.layers.36...
[36m(WorkerDict pid=375734)[0m loading layer #37, with layer_name model.layers.37...
[36m(WorkerDict pid=375734)[0m loading layer #38, with layer_name model.layers.38...
[36m(WorkerDict pid=375734)[0m loading layer #39, with layer_name model.layers.39...
[36m(WorkerDict pid=375734)[0m loading layer #40, with layer_name model.layers.40...
[36m(WorkerDict pid=375734)[0m loading layer #41, with layer_name model.layers.41...
[36m(WorkerDict pid=375734)[0m loading layer #42, with layer_name model.layers.42...
[36m(WorkerDict pid=375734)[0m loading layer #43, with layer_name model.layers.43...
[36m(WorkerDict pid=375734)[0m loading layer #44, with layer_name model.layers.44...
[36m(WorkerDict pid=375734)[0m loading layer #45, with layer_name model.layers.45...
[36m(WorkerDict pid=375734)[0m loading layer #46, with layer_name model.layers.46...
[36m(WorkerDict pid=375734)[0m loading layer #47, with layer_name model.layers.47...
[36m(WorkerDict pid=375734)[0m loading layer #48, with layer_name model.layers.48...
[36m(WorkerDict pid=375734)[0m loading layer #49, with layer_name model.layers.49...
[36m(WorkerDict pid=375734)[0m loading layer #50, with layer_name model.layers.50...
[36m(WorkerDict pid=375734)[0m loading layer #51, with layer_name model.layers.51...
[36m(WorkerDict pid=375734)[0m loading layer #52, with layer_name model.layers.52...
[36m(WorkerDict pid=375734)[0m loading layer #53, with layer_name model.layers.53...
[36m(WorkerDict pid=375734)[0m loading layer #54, with layer_name model.layers.54...
[36m(WorkerDict pid=375734)[0m loading layer #55, with layer_name model.layers.55...
[36m(WorkerDict pid=375734)[0m loading layer #56, with layer_name model.layers.56...
[36m(WorkerDict pid=375734)[0m loading layer #57, with layer_name model.layers.57...
[36m(WorkerDict pid=375734)[0m loading layer #58, with layer_name model.layers.58...
[36m(WorkerDict pid=375734)[0m loading layer #59, with layer_name model.layers.59...
[36m(WorkerDict pid=375734)[0m loading layer #60, with layer_name model.layers.60...
[36m(WorkerDict pid=375734)[0m loading layer #61, with layer_name model.layers.61...
[36m(WorkerDict pid=375734)[0m loading layer #62, with layer_name model.layers.62...
[36m(WorkerDict pid=375734)[0m loading layer #63, with layer_name model.layers.63...
[36m(WorkerDict pid=375734)[0m loading final layernorm...
[36m(WorkerDict pid=375734)[0m loading lm_head...
[36m(WorkerDict pid=375734)[0m loading megatron ckpt done, time elapsed 21.276102542877197s
[36m(WorkerDict pid=375747)[0m [Rank 0] swap optimizer: 2047926272 (23436.65625 MB)/2047926272
[36m(WorkerDict pid=375752)[0m [Rank 0] swap optimizer: 2047931392 (23436.71484375 MB)/2047931392
[36m(WorkerDict pid=375751)[0m [Rank 0] swap optimizer: 2047931392 (23436.71484375 MB)/2047931392
[36m(WorkerDict pid=375742)[0m [Rank 0] swap optimizer: 2047926272 (23436.65625 MB)/2047926272
[36m(WorkerDict pid=375745)[0m [Rank 0] swap optimizer: 2047926272 (23436.65625 MB)/2047926272
[36m(WorkerDict pid=375740)[0m [Rank 0] swap optimizer: 2047926272 (23436.65625 MB)/2047926272
[36m(WorkerDict pid=375746)[0m [Rank 0] swap optimizer: 2047926272 (23436.65625 MB)/2047926272
[36m(WorkerDict pid=375743)[0m [Rank 0] swap optimizer: 2047926272 (23436.65625 MB)/2047926272
[36m(WorkerDict pid=375744)[0m [Rank 0] swap optimizer: 2047926272 (23436.65625 MB)/2047926272
[36m(WorkerDict pid=375750)[0m [Rank 0] swap optimizer: 2047931392 (23436.71484375 MB)/2047931392
[36m(WorkerDict pid=375748)[0m [Rank 0] swap optimizer: 2047931392 (23436.71484375 MB)/2047931392
[36m(WorkerDict pid=375749)[0m [Rank 0] swap optimizer: 2047931392 (23436.71484375 MB)/2047931392
[36m(WorkerDict pid=375753)[0m [Rank 0] swap optimizer: 2047931392 (23436.71484375 MB)/2047931392
[36m(WorkerDict pid=375754)[0m [Rank 0] swap optimizer: 2047931392 (23436.71484375 MB)/2047931392
[36m(WorkerDict pid=375758)[0m [Rank 0] swap optimizer: 2047931392 (23436.71484375 MB)/2047931392
[36m(WorkerDict pid=375740)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff14565ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff143d7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff143d7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='323', worker_launch_time_ms='1765610508364', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='557683157', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375752)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff28461ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff282d7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff282d7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='334', worker_launch_time_ms='1765610508620', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='206968578', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375734)[0m DistributedDataParallel contains 2.05B parameters
[36m(WorkerDict pid=375734)[0m optimizer config after override: {'optimizer': 'adam', 'lr': 1e-06, 'min_lr': 0.0, 'clip_grad': 10000, 'weight_decay': 0.01, 'bf16': True, 'params_dtype': torch.bfloat16, 'use_distributed_optimizer': True}
[36m(WorkerDict pid=375734)[0m [Rank 0 | Local Rank 0] 2025-12-13 15:23:28,692 INFO [megatron.core.optimizer:532] => Setting up optimizer with config OptimizerConfig(optimizer='adam', lr=1e-06, min_lr=0.0, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=True, params_dtype=torch.bfloat16, use_precision_aware_optimizer=False, main_grads_dtype=torch.float32, main_params_dtype=torch.float32, exp_avg_dtype=torch.float32, exp_avg_sq_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=True, overlap_param_gather_with_optimizer_step=False, optimizer_cpu_offload=False, optimizer_offload_fraction=0.0, use_torch_optimizer_for_cpu_offload=False, overlap_cpu_optimizer_d2h_h2d=False, pin_cpu_grads=True, pin_cpu_params=True, clip_grad=10000, log_num_zeros_in_grad=False, barrier_with_L1_time=False, timers=None, config_logger_dir='')
[36m(WorkerDict pid=375740)[0m INFO 12-13 15:23:28 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
[36m(WorkerDict pid=375749)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff0c639ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff0c4a7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff0c4a7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='331', worker_launch_time_ms='1765610508553', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='915738454', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375754)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff04561ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff043d7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff043d7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='336', worker_launch_time_ms='1765610508662', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='2045057609', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375740)[0m WARNING 12-13 15:23:28 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375740)[0m WARNING 12-13 15:23:28 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375740)[0m INFO 12-13 15:23:28 [parallel_state.py:1047] Adjusting world_size=2 rank=1 distributed_init_method=tcp://127.0.0.1:0 for DP
[36m(WorkerDict pid=375752)[0m INFO 12-13 15:23:29 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
[36m(WorkerDict pid=375752)[0m WARNING 12-13 15:23:29 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375752)[0m WARNING 12-13 15:23:29 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375752)[0m INFO 12-13 15:23:29 [parallel_state.py:1047] Adjusting world_size=2 rank=15 distributed_init_method=tcp://127.0.0.1:0 for DP
[36m(WorkerDict pid=375734)[0m [Rank 0] swap optimizer: 2047926272 (23436.65625 MB)/2047926272
[36m(WorkerDict pid=375734)[0m [Rank 0 | Local Rank 0] 2025-12-13 15:23:29,213 INFO [megatron.core.optimizer_param_scheduler:532] => > learning rate decay style: constant
[36m(WorkerDict pid=375749)[0m INFO 12-13 15:23:30 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
[36m(WorkerDict pid=375749)[0m WARNING 12-13 15:23:30 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375749)[0m WARNING 12-13 15:23:30 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375749)[0m INFO 12-13 15:23:30 [parallel_state.py:1047] Adjusting world_size=2 rank=9 distributed_init_method=tcp://127.0.0.1:0 for DP
[36m(WorkerDict pid=375754)[0m INFO 12-13 15:23:30 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
[36m(WorkerDict pid=375754)[0m WARNING 12-13 15:23:30 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375754)[0m WARNING 12-13 15:23:30 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375754)[0m INFO 12-13 15:23:30 [parallel_state.py:1047] Adjusting world_size=2 rank=13 distributed_init_method=tcp://127.0.0.1:0 for DP
[36m(WorkerDict pid=375748)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff50175ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff186bbe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff186bbe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='330', worker_launch_time_ms='1765610508532', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='-613850428', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375747)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff44161ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff2c6a7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff2c6a7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='329', worker_launch_time_ms='1765610508513', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='1060481726', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375746)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff18639ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff184abe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff184abe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='328', worker_launch_time_ms='1765610508493', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='-723026129', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375744)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff08561ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff083d7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff083d7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='326', worker_launch_time_ms='1765610508444', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='1618432160', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375758)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff0c171ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xfffee86bbe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xfffee86bbe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='337', worker_launch_time_ms='1765610508683', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='277292338', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375751)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff58265ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff580dbe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff580dbe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='333', worker_launch_time_ms='1765610508597', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='-238701692', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375748)[0m INFO 12-13 15:23:31 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
[36m(WorkerDict pid=375748)[0m WARNING 12-13 15:23:31 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375748)[0m WARNING 12-13 15:23:31 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375748)[0m INFO 12-13 15:23:31 [parallel_state.py:1047] Adjusting world_size=2 rank=8 distributed_init_method=tcp://127.0.0.1:0 for DP
[36m(WorkerDict pid=375753)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff30561ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff303d7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff303d7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='335', worker_launch_time_ms='1765610508641', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='314639389', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375742)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff18561ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff183d7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff183d7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='324', worker_launch_time_ms='1765610508406', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='1459253070', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375743)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff0c15dee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xfffee86a7e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xfffee86a7e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='325', worker_launch_time_ms='1765610508426', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='1481740090', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375750)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff3c171ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff206bbe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff206bbe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='332', worker_launch_time_ms='1765610508574', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='1331188970', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375745)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xfffefc73dee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xfffefc5abe20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xfffefc5abe20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='327', worker_launch_time_ms='1765610508473', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='1747680634', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375744)[0m INFO 12-13 15:23:32 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
[36m(WorkerDict pid=375744)[0m WARNING 12-13 15:23:32 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375747)[0m INFO 12-13 15:23:32 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
[36m(WorkerDict pid=375758)[0m INFO 12-13 15:23:32 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
[36m(WorkerDict pid=375758)[0m WARNING 12-13 15:23:32 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375758)[0m WARNING 12-13 15:23:32 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375758)[0m INFO 12-13 15:23:32 [parallel_state.py:1047] Adjusting world_size=2 rank=12 distributed_init_method=tcp://127.0.0.1:0 for DP
[36m(WorkerDict pid=375744)[0m WARNING 12-13 15:23:32 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375744)[0m INFO 12-13 15:23:32 [parallel_state.py:1047] Adjusting world_size=2 rank=4 distributed_init_method=tcp://127.0.0.1:0 for DP
[36m(WorkerDict pid=375747)[0m WARNING 12-13 15:23:32 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375747)[0m WARNING 12-13 15:23:32 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375747)[0m INFO 12-13 15:23:32 [parallel_state.py:1047] Adjusting world_size=2 rank=7 distributed_init_method=tcp://127.0.0.1:0 for DP
[36m(WorkerDict pid=375746)[0m INFO 12-13 15:23:32 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
[36m(WorkerDict pid=375746)[0m WARNING 12-13 15:23:32 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375746)[0m WARNING 12-13 15:23:32 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375746)[0m INFO 12-13 15:23:32 [parallel_state.py:1047] Adjusting world_size=2 rank=6 distributed_init_method=tcp://127.0.0.1:0 for DP
[36m(WorkerDict pid=375743)[0m INFO 12-13 15:23:32 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
[36m(WorkerDict pid=375751)[0m INFO 12-13 15:23:32 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
[36m(WorkerDict pid=375751)[0m WARNING 12-13 15:23:32 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375751)[0m WARNING 12-13 15:23:32 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375751)[0m INFO 12-13 15:23:32 [parallel_state.py:1047] Adjusting world_size=2 rank=11 distributed_init_method=tcp://127.0.0.1:0 for DP
[36m(WorkerDict pid=375742)[0m INFO 12-13 15:23:32 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
[36m(WorkerDict pid=375743)[0m WARNING 12-13 15:23:32 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375743)[0m WARNING 12-13 15:23:32 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375743)[0m INFO 12-13 15:23:32 [parallel_state.py:1047] Adjusting world_size=2 rank=3 distributed_init_method=tcp://127.0.0.1:0 for DP
[36m(WorkerDict pid=375750)[0m INFO 12-13 15:23:32 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
[36m(WorkerDict pid=375742)[0m WARNING 12-13 15:23:32 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375742)[0m WARNING 12-13 15:23:32 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375742)[0m INFO 12-13 15:23:32 [parallel_state.py:1047] Adjusting world_size=2 rank=2 distributed_init_method=tcp://127.0.0.1:0 for DP
[36m(WorkerDict pid=375750)[0m WARNING 12-13 15:23:32 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375750)[0m WARNING 12-13 15:23:32 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375750)[0m INFO 12-13 15:23:32 [parallel_state.py:1047] Adjusting world_size=2 rank=10 distributed_init_method=tcp://127.0.0.1:0 for DP
[36m(WorkerDict pid=375753)[0m INFO 12-13 15:23:32 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
[36m(WorkerDict pid=375753)[0m WARNING 12-13 15:23:32 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375753)[0m WARNING 12-13 15:23:32 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375753)[0m INFO 12-13 15:23:32 [parallel_state.py:1047] Adjusting world_size=2 rank=14 distributed_init_method=tcp://127.0.0.1:0 for DP
[36m(WorkerDict pid=375745)[0m INFO 12-13 15:23:33 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
[36m(WorkerDict pid=375745)[0m WARNING 12-13 15:23:33 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375745)[0m WARNING 12-13 15:23:33 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375745)[0m INFO 12-13 15:23:33 [parallel_state.py:1047] Adjusting world_size=2 rank=5 distributed_init_method=tcp://127.0.0.1:0 for DP
[36m(WorkerDict pid=375734)[0m TransformerConfig(tensor_model_parallel_size=8, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=2, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=False, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=True, overlap_p2p_comm=False, batch_p2p_comm=False, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=2, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=64, mtp_num_layers=None, mtp_loss_scaling_factor=None, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=5120, num_attention_heads=64, attention_backend=<AttnBackend.flash: 1>, softmax_scale=None, num_query_groups=8, ffn_hidden_size=25600, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0xffff00741ee0>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, init_method=functools.partial(<function normal_ at 0xffff005b3e20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0xffff005b3e20>, mean=0.0, std=0.0017677669529663686), init_method_std=0.02, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=False, masked_softmax_fusion=True, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=None, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=25600, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_dtype=None, moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type=None, enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, heterogeneous_block_specs=False, optimizer_selection='fused_adamw', optimization_level=2, use_fused_rmsnorm=True, use_fused_swiglu=True, context_parallel_algo='megatron_cp_algo', cp_window_size=1, attention_mask_type='causal', use_cp_send_recv_overlap=True, use_fused_ring_attention_update=True, megatron_cp_in_bnsd=False, ulysses_degree_in_cp=None, context_parallel_kv_cache_policy=None, context_parallel_cache_interval=0, use_ulysses_allgather_kv=False, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, async_log_allreduce=False, use_fused_rotary_pos_emb=True, use_fused_moe_token_permute_and_unpermute=False, npu_deterministic=False, op_cal_tflops=False, profile_level='level0', profile_with_cpu=False, profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_save_path='./profile_dir', recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_norm=False, recompute_norm_num_layers=None, enable_recompute_layers_per_pp_rank=False, unaligned_linear=False, use_ascend_mc2=False, use_ascend_coc=False, coc_mode=-1, coc_parallel_num=1, coc_fused_kernel=False, tp_2d=False, tp_x=1, tp_y=1, enable_overlap_ag_with_matmul=False, enable_overlap_matmul_with_rs=False, enable_backward_overlap_ag_with_matmul=False, recompute_in_bubble=False, recompute_in_advance=False, noop_layers=None, use_multiparameter_pipeline_model_parallel=False, optimize_send_recv_comm=False, pipeline_num_transformer_layers=None, schedules_method=None, dualpipev_dw_detach=False, gemm_gradient_accumulation_fusion=False, moe_tp_extend_ep=False, moe_permutation_async_comm=False, n_shared_experts=None, moe_allgather_overlap_comm=False, moe_alltoall_overlap_comm=False, moe_zero_memory='disable', moe_zero_memory_num_layers=None, moe_fb_overlap=False, moe_unperm2_mem_optim_swap=False, hccl_group_buffer=None, hccl_group_buffer_adaptive=False, hccl_ep_group_buffer_adaptive_factor=-1.0, ema_decay=0.9999, virtual_optimizer=None, dist_train=False, tokenizer_name_or_path=None, tokenizer_not_use_fast=True, param_and_grad_buffer_pad=None, layerzero=False, layerzero_config=None, fsdp2_config_path=None, reuse_fp32_param=False, smart_swap=False, swap_attention=False, swap_modules='input_norm,self_attention,post_attention_norm', compress_dense='disable', disable_gloo_group=False, hccl_slice_size=10485760, swap_optimizer=True, swap_optimizer_times=16, pre_tockens=65536, next_tockens=0, sparse_mode=0, use_fusion_attn_v2=False, square_alibi_mask=False, fill_neg_inf=False, alibi_fusion_attn_type=None, alibi_diagonal_opposite=False, multi_head_latent_attention=False, qk_rope_head_dim=None, qk_nope_head_dim=None, ai_framework='pytorch', node_ip_address='192.168.0.163', node_manager_port='45387', object_store_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/plasma_store', raylet_name='/tmp/ray/session_2025-12-13_15-21-02_540574_328208/sockets/raylet', redis_address='None', metrics_agent_port='55450', logging_rotate_bytes='536870912', logging_rotate_backup_count='5', runtime_env_agent_port='61305', gcs_address='192.168.0.163:29445', session_name='session_2025-12-13_15-21-02_540574_328208', temp_dir='/tmp/ray', webui='192.168.0.163:8260', cluster_id='7985701907428296a23d2f2e88354f23ccb4bb6cda4207cfaa86af40', startup_token='322', worker_launch_time_ms='1765610508329', node_id='c1a682134e840c6c2cc074d92314f980f6c29be27d8a7f8693c67a14', runtime_env_hash='-150223706', seq_length=2048, use_flash_attn=True)
[36m(WorkerDict pid=375734)[0m INFO 12-13 15:23:34 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
[36m(WorkerDict pid=375734)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375734)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375734)[0m INFO 12-13 15:23:34 [parallel_state.py:1047] Adjusting world_size=2 rank=0 distributed_init_method=tcp://127.0.0.1:0 for DP
[36m(WorkerDict pid=375742)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375742)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375734)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375734)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375745)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375745)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375740)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375740)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375746)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375743)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375743)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375744)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375744)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375747)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375747)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375752)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375752)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375751)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375751)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375750)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375750)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375748)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375748)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375749)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375749)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375753)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375753)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375754)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375754)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375758)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375758)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375742)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375742)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375734)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375734)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375745)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375745)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375740)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375740)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375746)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375746)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375746)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375743)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375743)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375744)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375744)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375747)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375747)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375752)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375752)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375751)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375751)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375750)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375750)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375748)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375748)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375748)[0m INFO 12-13 15:23:34 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer_handle=(7, 4194304, 6, 'psm_a15550c3'), local_subscribe_addr='ipc:///tmp/63008ef2-2d62-4195-85ee-b132429cc7c0', remote_subscribe_addr=None, remote_addr_ipv6=False)
[36m(WorkerDict pid=375749)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375749)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375753)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375753)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375754)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375754)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375758)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375758)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375734)[0m INFO 12-13 15:23:34 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer_handle=(7, 4194304, 6, 'psm_52963a9e'), local_subscribe_addr='ipc:///tmp/5b2b0105-719a-49aa-8d4a-6bd41694d2f9', remote_subscribe_addr=None, remote_addr_ipv6=False)
[36m(WorkerDict pid=375742)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375734)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375745)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375745)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375740)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375740)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375746)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375746)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375743)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375743)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375744)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375744)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375747)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375747)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375752)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375752)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375751)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375751)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375750)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375750)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375748)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375748)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375749)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375749)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375753)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375753)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375754)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375754)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375758)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375758)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375742)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375742)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375742)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375742)[0m INFO 12-13 15:23:34 [parallel_state.py:1208] rank 2 in world size 16 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
[36m(WorkerDict pid=375734)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375734)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375734)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375734)[0m INFO 12-13 15:23:34 [parallel_state.py:1208] rank 0 in world size 16 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[36m(WorkerDict pid=375745)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375745)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375745)[0m INFO 12-13 15:23:34 [parallel_state.py:1208] rank 5 in world size 16 is assigned as DP rank 0, PP rank 0, TP rank 5, EP rank 5
[36m(WorkerDict pid=375740)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375740)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375740)[0m INFO 12-13 15:23:34 [parallel_state.py:1208] rank 1 in world size 16 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
[36m(WorkerDict pid=375746)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375746)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375746)[0m INFO 12-13 15:23:34 [parallel_state.py:1208] rank 6 in world size 16 is assigned as DP rank 0, PP rank 0, TP rank 6, EP rank 6
[36m(WorkerDict pid=375743)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375743)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375743)[0m INFO 12-13 15:23:34 [parallel_state.py:1208] rank 3 in world size 16 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
[36m(WorkerDict pid=375744)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375744)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375744)[0m INFO 12-13 15:23:34 [parallel_state.py:1208] rank 4 in world size 16 is assigned as DP rank 0, PP rank 0, TP rank 4, EP rank 4
[36m(WorkerDict pid=375747)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375747)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375747)[0m INFO 12-13 15:23:34 [parallel_state.py:1208] rank 7 in world size 16 is assigned as DP rank 0, PP rank 0, TP rank 7, EP rank 7
[36m(WorkerDict pid=375752)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375752)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375752)[0m INFO 12-13 15:23:34 [parallel_state.py:1208] rank 15 in world size 16 is assigned as DP rank 1, PP rank 0, TP rank 7, EP rank 15
[36m(WorkerDict pid=375751)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375751)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375751)[0m INFO 12-13 15:23:34 [parallel_state.py:1208] rank 11 in world size 16 is assigned as DP rank 1, PP rank 0, TP rank 3, EP rank 11
[36m(WorkerDict pid=375750)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375750)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375750)[0m INFO 12-13 15:23:34 [parallel_state.py:1208] rank 10 in world size 16 is assigned as DP rank 1, PP rank 0, TP rank 2, EP rank 10
[36m(WorkerDict pid=375748)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375748)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375748)[0m INFO 12-13 15:23:34 [parallel_state.py:1208] rank 8 in world size 16 is assigned as DP rank 1, PP rank 0, TP rank 0, EP rank 8
[36m(WorkerDict pid=375749)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375749)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375749)[0m INFO 12-13 15:23:34 [parallel_state.py:1208] rank 9 in world size 16 is assigned as DP rank 1, PP rank 0, TP rank 1, EP rank 9
[36m(WorkerDict pid=375753)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375753)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375753)[0m INFO 12-13 15:23:34 [parallel_state.py:1208] rank 14 in world size 16 is assigned as DP rank 1, PP rank 0, TP rank 6, EP rank 14
[36m(WorkerDict pid=375754)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375754)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375754)[0m INFO 12-13 15:23:34 [parallel_state.py:1208] rank 13 in world size 16 is assigned as DP rank 1, PP rank 0, TP rank 5, EP rank 13
[36m(WorkerDict pid=375758)[0m WARNING 12-13 15:23:34 [__init__.py:755] Current vLLM config is not set.
[36m(WorkerDict pid=375758)[0m WARNING 12-13 15:23:34 [platform.py:170] Model config is missing. This may indicate that we are running a test case
[36m(WorkerDict pid=375758)[0m INFO 12-13 15:23:34 [parallel_state.py:1208] rank 12 in world size 16 is assigned as DP rank 1, PP rank 0, TP rank 4, EP rank 12
[36m(WorkerDict pid=375742)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(WorkerDict pid=375742)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(WorkerDict pid=375742)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(WorkerDict pid=375742)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(WorkerDict pid=375742)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
[36m(WorkerDict pid=375742)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=375742)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=375742)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
[36m(WorkerDict pid=375742)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
[36m(WorkerDict pid=375742)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(WorkerDict pid=375742)[0m INFO 12-13 15:23:34 [utils.py:233] non-default args: {'load_format': 'dummy', 'dtype': 'bfloat16', 'seed': 0, 'max_model_len': 36864, 'distributed_executor_backend': 'external_launcher', 'tensor_parallel_size': 8, 'enable_prefix_caching': False, 'gpu_memory_utilization': 0.85, 'max_num_batched_tokens': 36864, 'max_num_seqs': 128, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'speculative_config': {'method': 'sam', 'num_speculative_tokens': 3}, 'additional_config': {'torchair_graph_config': {'enabled': False}, 'ascend_scheduler_config': {'enabled': True, 'enable_chunked_prefill': False}, 'refresh': True, 'dynamic_eplb': False, 'num_iterations_eplb_update': 400, 'gate_eplb': True, 'num_wait_worker_iterations': 30}, 'model': '/home/data/Qwen3-32B'}
[36m(WorkerDict pid=375734)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(WorkerDict pid=375734)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(WorkerDict pid=375734)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(WorkerDict pid=375734)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(WorkerDict pid=375734)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
[36m(WorkerDict pid=375734)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=375734)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=375734)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
[36m(WorkerDict pid=375734)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
[36m(WorkerDict pid=375734)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(WorkerDict pid=375734)[0m INFO 12-13 15:23:34 [utils.py:233] non-default args: {'load_format': 'dummy', 'dtype': 'bfloat16', 'seed': 0, 'max_model_len': 36864, 'distributed_executor_backend': 'external_launcher', 'tensor_parallel_size': 8, 'enable_prefix_caching': False, 'gpu_memory_utilization': 0.85, 'max_num_batched_tokens': 36864, 'max_num_seqs': 128, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'speculative_config': {'method': 'sam', 'num_speculative_tokens': 3}, 'additional_config': {'torchair_graph_config': {'enabled': False}, 'ascend_scheduler_config': {'enabled': True, 'enable_chunked_prefill': False}, 'refresh': True, 'dynamic_eplb': False, 'num_iterations_eplb_update': 400, 'gate_eplb': True, 'num_wait_worker_iterations': 30}, 'model': '/home/data/Qwen3-32B'}
[36m(WorkerDict pid=375745)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(WorkerDict pid=375745)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(WorkerDict pid=375745)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(WorkerDict pid=375745)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(WorkerDict pid=375745)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
[36m(WorkerDict pid=375745)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=375745)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=375745)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
[36m(WorkerDict pid=375745)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
[36m(WorkerDict pid=375745)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(WorkerDict pid=375745)[0m INFO 12-13 15:23:34 [utils.py:233] non-default args: {'load_format': 'dummy', 'dtype': 'bfloat16', 'seed': 0, 'max_model_len': 36864, 'distributed_executor_backend': 'external_launcher', 'tensor_parallel_size': 8, 'enable_prefix_caching': False, 'gpu_memory_utilization': 0.85, 'max_num_batched_tokens': 36864, 'max_num_seqs': 128, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'speculative_config': {'method': 'sam', 'num_speculative_tokens': 3}, 'additional_config': {'torchair_graph_config': {'enabled': False}, 'ascend_scheduler_config': {'enabled': True, 'enable_chunked_prefill': False}, 'refresh': True, 'dynamic_eplb': False, 'num_iterations_eplb_update': 400, 'gate_eplb': True, 'num_wait_worker_iterations': 30}, 'model': '/home/data/Qwen3-32B'}
[36m(WorkerDict pid=375740)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(WorkerDict pid=375740)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(WorkerDict pid=375740)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(WorkerDict pid=375740)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(WorkerDict pid=375740)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
[36m(WorkerDict pid=375740)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=375740)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=375740)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
[36m(WorkerDict pid=375740)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
[36m(WorkerDict pid=375740)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(WorkerDict pid=375740)[0m INFO 12-13 15:23:34 [utils.py:233] non-default args: {'load_format': 'dummy', 'dtype': 'bfloat16', 'seed': 0, 'max_model_len': 36864, 'distributed_executor_backend': 'external_launcher', 'tensor_parallel_size': 8, 'enable_prefix_caching': False, 'gpu_memory_utilization': 0.85, 'max_num_batched_tokens': 36864, 'max_num_seqs': 128, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'speculative_config': {'method': 'sam', 'num_speculative_tokens': 3}, 'additional_config': {'torchair_graph_config': {'enabled': False}, 'ascend_scheduler_config': {'enabled': True, 'enable_chunked_prefill': False}, 'refresh': True, 'dynamic_eplb': False, 'num_iterations_eplb_update': 400, 'gate_eplb': True, 'num_wait_worker_iterations': 30}, 'model': '/home/data/Qwen3-32B'}
[36m(WorkerDict pid=375746)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(WorkerDict pid=375746)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(WorkerDict pid=375746)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(WorkerDict pid=375746)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(WorkerDict pid=375746)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
[36m(WorkerDict pid=375746)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=375746)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=375746)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
[36m(WorkerDict pid=375746)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
[36m(WorkerDict pid=375746)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(WorkerDict pid=375746)[0m INFO 12-13 15:23:34 [utils.py:233] non-default args: {'load_format': 'dummy', 'dtype': 'bfloat16', 'seed': 0, 'max_model_len': 36864, 'distributed_executor_backend': 'external_launcher', 'tensor_parallel_size': 8, 'enable_prefix_caching': False, 'gpu_memory_utilization': 0.85, 'max_num_batched_tokens': 36864, 'max_num_seqs': 128, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'speculative_config': {'method': 'sam', 'num_speculative_tokens': 3}, 'additional_config': {'torchair_graph_config': {'enabled': False}, 'ascend_scheduler_config': {'enabled': True, 'enable_chunked_prefill': False}, 'refresh': True, 'dynamic_eplb': False, 'num_iterations_eplb_update': 400, 'gate_eplb': True, 'num_wait_worker_iterations': 30}, 'model': '/home/data/Qwen3-32B'}
[36m(WorkerDict pid=375743)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(WorkerDict pid=375743)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(WorkerDict pid=375743)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(WorkerDict pid=375743)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(WorkerDict pid=375743)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
[36m(WorkerDict pid=375743)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=375743)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=375743)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
[36m(WorkerDict pid=375743)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
[36m(WorkerDict pid=375743)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(WorkerDict pid=375743)[0m INFO 12-13 15:23:34 [utils.py:233] non-default args: {'load_format': 'dummy', 'dtype': 'bfloat16', 'seed': 0, 'max_model_len': 36864, 'distributed_executor_backend': 'external_launcher', 'tensor_parallel_size': 8, 'enable_prefix_caching': False, 'gpu_memory_utilization': 0.85, 'max_num_batched_tokens': 36864, 'max_num_seqs': 128, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'speculative_config': {'method': 'sam', 'num_speculative_tokens': 3}, 'additional_config': {'torchair_graph_config': {'enabled': False}, 'ascend_scheduler_config': {'enabled': True, 'enable_chunked_prefill': False}, 'refresh': True, 'dynamic_eplb': False, 'num_iterations_eplb_update': 400, 'gate_eplb': True, 'num_wait_worker_iterations': 30}, 'model': '/home/data/Qwen3-32B'}
[36m(WorkerDict pid=375744)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(WorkerDict pid=375744)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(WorkerDict pid=375744)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(WorkerDict pid=375744)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(WorkerDict pid=375744)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
[36m(WorkerDict pid=375744)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=375744)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=375744)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
[36m(WorkerDict pid=375744)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
[36m(WorkerDict pid=375744)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(WorkerDict pid=375744)[0m INFO 12-13 15:23:34 [utils.py:233] non-default args: {'load_format': 'dummy', 'dtype': 'bfloat16', 'seed': 0, 'max_model_len': 36864, 'distributed_executor_backend': 'external_launcher', 'tensor_parallel_size': 8, 'enable_prefix_caching': False, 'gpu_memory_utilization': 0.85, 'max_num_batched_tokens': 36864, 'max_num_seqs': 128, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'speculative_config': {'method': 'sam', 'num_speculative_tokens': 3}, 'additional_config': {'torchair_graph_config': {'enabled': False}, 'ascend_scheduler_config': {'enabled': True, 'enable_chunked_prefill': False}, 'refresh': True, 'dynamic_eplb': False, 'num_iterations_eplb_update': 400, 'gate_eplb': True, 'num_wait_worker_iterations': 30}, 'model': '/home/data/Qwen3-32B'}
[36m(WorkerDict pid=375747)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(WorkerDict pid=375747)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(WorkerDict pid=375747)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(WorkerDict pid=375747)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(WorkerDict pid=375747)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
[36m(WorkerDict pid=375747)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=375747)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=375747)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
[36m(WorkerDict pid=375747)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
[36m(WorkerDict pid=375747)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(WorkerDict pid=375747)[0m INFO 12-13 15:23:34 [utils.py:233] non-default args: {'load_format': 'dummy', 'dtype': 'bfloat16', 'seed': 0, 'max_model_len': 36864, 'distributed_executor_backend': 'external_launcher', 'tensor_parallel_size': 8, 'enable_prefix_caching': False, 'gpu_memory_utilization': 0.85, 'max_num_batched_tokens': 36864, 'max_num_seqs': 128, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'speculative_config': {'method': 'sam', 'num_speculative_tokens': 3}, 'additional_config': {'torchair_graph_config': {'enabled': False}, 'ascend_scheduler_config': {'enabled': True, 'enable_chunked_prefill': False}, 'refresh': True, 'dynamic_eplb': False, 'num_iterations_eplb_update': 400, 'gate_eplb': True, 'num_wait_worker_iterations': 30}, 'model': '/home/data/Qwen3-32B'}
[36m(WorkerDict pid=375752)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(WorkerDict pid=375752)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(WorkerDict pid=375752)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(WorkerDict pid=375752)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(WorkerDict pid=375752)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
[36m(WorkerDict pid=375752)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=375752)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=375752)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
[36m(WorkerDict pid=375752)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
[36m(WorkerDict pid=375752)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(WorkerDict pid=375752)[0m INFO 12-13 15:23:34 [utils.py:233] non-default args: {'load_format': 'dummy', 'dtype': 'bfloat16', 'seed': 0, 'max_model_len': 36864, 'distributed_executor_backend': 'external_launcher', 'tensor_parallel_size': 8, 'enable_prefix_caching': False, 'gpu_memory_utilization': 0.85, 'max_num_batched_tokens': 36864, 'max_num_seqs': 128, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'speculative_config': {'method': 'sam', 'num_speculative_tokens': 3}, 'additional_config': {'torchair_graph_config': {'enabled': False}, 'ascend_scheduler_config': {'enabled': True, 'enable_chunked_prefill': False}, 'refresh': True, 'dynamic_eplb': False, 'num_iterations_eplb_update': 400, 'gate_eplb': True, 'num_wait_worker_iterations': 30}, 'model': '/home/data/Qwen3-32B'}
[36m(WorkerDict pid=375751)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(WorkerDict pid=375751)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(WorkerDict pid=375751)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(WorkerDict pid=375751)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(WorkerDict pid=375751)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
[36m(WorkerDict pid=375751)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=375751)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=375751)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
[36m(WorkerDict pid=375751)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
[36m(WorkerDict pid=375751)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(WorkerDict pid=375751)[0m INFO 12-13 15:23:34 [utils.py:233] non-default args: {'load_format': 'dummy', 'dtype': 'bfloat16', 'seed': 0, 'max_model_len': 36864, 'distributed_executor_backend': 'external_launcher', 'tensor_parallel_size': 8, 'enable_prefix_caching': False, 'gpu_memory_utilization': 0.85, 'max_num_batched_tokens': 36864, 'max_num_seqs': 128, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'speculative_config': {'method': 'sam', 'num_speculative_tokens': 3}, 'additional_config': {'torchair_graph_config': {'enabled': False}, 'ascend_scheduler_config': {'enabled': True, 'enable_chunked_prefill': False}, 'refresh': True, 'dynamic_eplb': False, 'num_iterations_eplb_update': 400, 'gate_eplb': True, 'num_wait_worker_iterations': 30}, 'model': '/home/data/Qwen3-32B'}
[36m(WorkerDict pid=375750)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(WorkerDict pid=375750)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(WorkerDict pid=375750)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(WorkerDict pid=375750)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(WorkerDict pid=375750)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
[36m(WorkerDict pid=375750)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=375750)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=375750)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
[36m(WorkerDict pid=375750)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
[36m(WorkerDict pid=375750)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(WorkerDict pid=375750)[0m INFO 12-13 15:23:34 [utils.py:233] non-default args: {'load_format': 'dummy', 'dtype': 'bfloat16', 'seed': 0, 'max_model_len': 36864, 'distributed_executor_backend': 'external_launcher', 'tensor_parallel_size': 8, 'enable_prefix_caching': False, 'gpu_memory_utilization': 0.85, 'max_num_batched_tokens': 36864, 'max_num_seqs': 128, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'speculative_config': {'method': 'sam', 'num_speculative_tokens': 3}, 'additional_config': {'torchair_graph_config': {'enabled': False}, 'ascend_scheduler_config': {'enabled': True, 'enable_chunked_prefill': False}, 'refresh': True, 'dynamic_eplb': False, 'num_iterations_eplb_update': 400, 'gate_eplb': True, 'num_wait_worker_iterations': 30}, 'model': '/home/data/Qwen3-32B'}
[36m(WorkerDict pid=375748)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(WorkerDict pid=375748)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(WorkerDict pid=375748)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(WorkerDict pid=375748)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(WorkerDict pid=375748)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
[36m(WorkerDict pid=375748)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=375748)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=375748)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
[36m(WorkerDict pid=375748)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
[36m(WorkerDict pid=375748)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(WorkerDict pid=375748)[0m INFO 12-13 15:23:34 [utils.py:233] non-default args: {'load_format': 'dummy', 'dtype': 'bfloat16', 'seed': 0, 'max_model_len': 36864, 'distributed_executor_backend': 'external_launcher', 'tensor_parallel_size': 8, 'enable_prefix_caching': False, 'gpu_memory_utilization': 0.85, 'max_num_batched_tokens': 36864, 'max_num_seqs': 128, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'speculative_config': {'method': 'sam', 'num_speculative_tokens': 3}, 'additional_config': {'torchair_graph_config': {'enabled': False}, 'ascend_scheduler_config': {'enabled': True, 'enable_chunked_prefill': False}, 'refresh': True, 'dynamic_eplb': False, 'num_iterations_eplb_update': 400, 'gate_eplb': True, 'num_wait_worker_iterations': 30}, 'model': '/home/data/Qwen3-32B'}
[36m(WorkerDict pid=375749)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(WorkerDict pid=375749)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(WorkerDict pid=375749)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(WorkerDict pid=375749)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(WorkerDict pid=375749)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
[36m(WorkerDict pid=375749)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=375749)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=375749)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
[36m(WorkerDict pid=375749)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
[36m(WorkerDict pid=375749)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(WorkerDict pid=375749)[0m INFO 12-13 15:23:34 [utils.py:233] non-default args: {'load_format': 'dummy', 'dtype': 'bfloat16', 'seed': 0, 'max_model_len': 36864, 'distributed_executor_backend': 'external_launcher', 'tensor_parallel_size': 8, 'enable_prefix_caching': False, 'gpu_memory_utilization': 0.85, 'max_num_batched_tokens': 36864, 'max_num_seqs': 128, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'speculative_config': {'method': 'sam', 'num_speculative_tokens': 3}, 'additional_config': {'torchair_graph_config': {'enabled': False}, 'ascend_scheduler_config': {'enabled': True, 'enable_chunked_prefill': False}, 'refresh': True, 'dynamic_eplb': False, 'num_iterations_eplb_update': 400, 'gate_eplb': True, 'num_wait_worker_iterations': 30}, 'model': '/home/data/Qwen3-32B'}
[36m(WorkerDict pid=375753)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(WorkerDict pid=375753)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(WorkerDict pid=375753)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(WorkerDict pid=375753)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(WorkerDict pid=375753)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
[36m(WorkerDict pid=375753)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=375753)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=375753)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
[36m(WorkerDict pid=375753)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
[36m(WorkerDict pid=375753)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(WorkerDict pid=375753)[0m INFO 12-13 15:23:34 [utils.py:233] non-default args: {'load_format': 'dummy', 'dtype': 'bfloat16', 'seed': 0, 'max_model_len': 36864, 'distributed_executor_backend': 'external_launcher', 'tensor_parallel_size': 8, 'enable_prefix_caching': False, 'gpu_memory_utilization': 0.85, 'max_num_batched_tokens': 36864, 'max_num_seqs': 128, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'speculative_config': {'method': 'sam', 'num_speculative_tokens': 3}, 'additional_config': {'torchair_graph_config': {'enabled': False}, 'ascend_scheduler_config': {'enabled': True, 'enable_chunked_prefill': False}, 'refresh': True, 'dynamic_eplb': False, 'num_iterations_eplb_update': 400, 'gate_eplb': True, 'num_wait_worker_iterations': 30}, 'model': '/home/data/Qwen3-32B'}
[36m(WorkerDict pid=375754)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(WorkerDict pid=375754)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(WorkerDict pid=375754)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(WorkerDict pid=375754)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(WorkerDict pid=375754)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
[36m(WorkerDict pid=375754)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=375754)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=375754)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
[36m(WorkerDict pid=375754)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
[36m(WorkerDict pid=375754)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(WorkerDict pid=375754)[0m INFO 12-13 15:23:34 [utils.py:233] non-default args: {'load_format': 'dummy', 'dtype': 'bfloat16', 'seed': 0, 'max_model_len': 36864, 'distributed_executor_backend': 'external_launcher', 'tensor_parallel_size': 8, 'enable_prefix_caching': False, 'gpu_memory_utilization': 0.85, 'max_num_batched_tokens': 36864, 'max_num_seqs': 128, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'speculative_config': {'method': 'sam', 'num_speculative_tokens': 3}, 'additional_config': {'torchair_graph_config': {'enabled': False}, 'ascend_scheduler_config': {'enabled': True, 'enable_chunked_prefill': False}, 'refresh': True, 'dynamic_eplb': False, 'num_iterations_eplb_update': 400, 'gate_eplb': True, 'num_wait_worker_iterations': 30}, 'model': '/home/data/Qwen3-32B'}
[36m(WorkerDict pid=375758)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
[36m(WorkerDict pid=375758)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLMoeForConditionalGeneration.
[36m(WorkerDict pid=375758)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl_without_padding:AscendQwen3VLForConditionalGeneration.
[36m(WorkerDict pid=375758)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
[36m(WorkerDict pid=375758)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
[36m(WorkerDict pid=375758)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=375758)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepseekV32ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
[36m(WorkerDict pid=375758)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
[36m(WorkerDict pid=375758)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
[36m(WorkerDict pid=375758)[0m WARNING 12-13 15:23:34 [registry.py:582] Model architecture Qwen3NextForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_next:CustomQwen3NextForCausalLM.
[36m(WorkerDict pid=375758)[0m INFO 12-13 15:23:34 [utils.py:233] non-default args: {'load_format': 'dummy', 'dtype': 'bfloat16', 'seed': 0, 'max_model_len': 36864, 'distributed_executor_backend': 'external_launcher', 'tensor_parallel_size': 8, 'enable_prefix_caching': False, 'gpu_memory_utilization': 0.85, 'max_num_batched_tokens': 36864, 'max_num_seqs': 128, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'speculative_config': {'method': 'sam', 'num_speculative_tokens': 3}, 'additional_config': {'torchair_graph_config': {'enabled': False}, 'ascend_scheduler_config': {'enabled': True, 'enable_chunked_prefill': False}, 'refresh': True, 'dynamic_eplb': False, 'num_iterations_eplb_update': 400, 'gate_eplb': True, 'num_wait_worker_iterations': 30}, 'model': '/home/data/Qwen3-32B'}
[36m(WorkerDict pid=375742)[0m INFO 12-13 15:23:34 [model.py:547] Resolved architecture: Qwen3ForCausalLM
[36m(WorkerDict pid=375742)[0m INFO 12-13 15:23:34 [model.py:1510] Using max model len 36864
[36m(WorkerDict pid=375742)[0m INFO 12-13 15:23:34 [arg_utils.py:1215] Using ray runtime env: {'env_vars': {'MASTER_ADDR': '192.168.0.163', 'MASTER_PORT': '38407', 'NCCL_CUMEM_ENABLE': '0', 'NCCL_DEBUG': 'WARN', 'RANK': '2', 'RAY_LOCAL_WORLD_SIZE': '16', 'TOKENIZERS_PARALLELISM': 'true', 'VLLM_ALLOW_RUNTIME_LORA_UPDATING': 'true', 'WG_BACKEND': 'ray', 'WG_PREFIX': 'Ux93Mb', 'WORLD_SIZE': '16'}}
[36m(WorkerDict pid=375742)[0m INFO 12-13 15:23:34 [parallel.py:380] Using external launcher for distributed inference.
[36m(WorkerDict pid=375742)[0m INFO 12-13 15:23:34 [parallel.py:420] Disabling V1 multiprocessing for external launcher.
[36m(WorkerDict pid=375742)[0m INFO 12-13 15:23:34 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=36864.
[36m(WorkerDict pid=375742)[0m INFO 12-13 15:23:34 [__init__.py:381] Cudagraph is disabled under eager mode
[36m(WorkerDict pid=375734)[0m INFO 12-13 15:23:34 [model.py:547] Resolved architecture: Qwen3ForCausalLM
[36m(WorkerDict pid=375734)[0m INFO 12-13 15:23:34 [model.py:1510] Using max model len 36864
[36m(WorkerDict pid=375734)[0m INFO 12-13 15:23:34 [arg_utils.py:1215] Using ray runtime env: {'env_vars': {'MASTER_ADDR': '192.168.0.163', 'MASTER_PORT': '38407', 'NCCL_CUMEM_ENABLE': '0', 'NCCL_DEBUG': 'WARN', 'RANK': '0', 'RAY_LOCAL_WORLD_SIZE': '16', 'TOKENIZERS_PARALLELISM': 'true', 'VLLM_ALLOW_RUNTIME_LORA_UPDATING': 'true', 'WG_BACKEND': 'ray', 'WG_PREFIX': 'Ux93Mb', 'WORLD_SIZE': '16'}}
[36m(WorkerDict pid=375734)[0m INFO 12-13 15:23:34 [parallel.py:380] Using external launcher for distributed inference.
[36m(WorkerDict pid=375734)[0m INFO 12-13 15:23:34 [parallel.py:420] Disabling V1 multiprocessing for external launcher.
[36m(WorkerDict pid=375734)[0m INFO 12-13 15:23:34 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=36864.
[36m(WorkerDict pid=375734)[0m INFO 12-13 15:23:34 [__init__.py:381] Cudagraph is disabled under eager mode
[36m(WorkerDict pid=375734)[0m INFO 12-13 15:23:34 [platform.py:141] Non-MLA LLMs forcibly disable the chunked prefill feature,as the performance of operators supporting this feature functionality is currently suboptimal.
[36m(WorkerDict pid=375734)[0m INFO 12-13 15:23:34 [platform.py:179] Compilation disabled, using eager mode by default
[36m(WorkerDict pid=375745)[0m INFO 12-13 15:23:34 [model.py:547] Resolved architecture: Qwen3ForCausalLM
[36m(WorkerDict pid=375745)[0m INFO 12-13 15:23:34 [model.py:1510] Using max model len 36864
[36m(WorkerDict pid=375740)[0m INFO 12-13 15:23:34 [model.py:547] Resolved architecture: Qwen3ForCausalLM
[36m(WorkerDict pid=375740)[0m INFO 12-13 15:23:34 [model.py:1510] Using max model len 36864
[36m(WorkerDict pid=375740)[0m INFO 12-13 15:23:34 [arg_utils.py:1215] Using ray runtime env: {'env_vars': {'MASTER_ADDR': '192.168.0.163', 'MASTER_PORT': '38407', 'NCCL_CUMEM_ENABLE': '0', 'NCCL_DEBUG': 'WARN', 'RANK': '1', 'RAY_LOCAL_WORLD_SIZE': '16', 'TOKENIZERS_PARALLELISM': 'true', 'VLLM_ALLOW_RUNTIME_LORA_UPDATING': 'true', 'WG_BACKEND': 'ray', 'WG_PREFIX': 'Ux93Mb', 'WORLD_SIZE': '16'}}
[36m(WorkerDict pid=375740)[0m INFO 12-13 15:23:34 [parallel.py:380] Using external launcher for distributed inference.
[36m(WorkerDict pid=375740)[0m INFO 12-13 15:23:34 [parallel.py:420] Disabling V1 multiprocessing for external launcher.
[36m(WorkerDict pid=375740)[0m INFO 12-13 15:23:34 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=36864.
[36m(WorkerDict pid=375740)[0m INFO 12-13 15:23:34 [__init__.py:381] Cudagraph is disabled under eager mode
[36m(WorkerDict pid=375740)[0m INFO 12-13 15:23:34 [platform.py:141] Non-MLA LLMs forcibly disable the chunked prefill feature,as the performance of operators supporting this feature functionality is currently suboptimal.
[36m(WorkerDict pid=375740)[0m INFO 12-13 15:23:34 [platform.py:179] Compilation disabled, using eager mode by default
[36m(WorkerDict pid=375746)[0m INFO 12-13 15:23:34 [model.py:547] Resolved architecture: Qwen3ForCausalLM
[36m(WorkerDict pid=375746)[0m INFO 12-13 15:23:34 [model.py:1510] Using max model len 36864
[36m(WorkerDict pid=375746)[0m INFO 12-13 15:23:34 [arg_utils.py:1215] Using ray runtime env: {'env_vars': {'MASTER_ADDR': '192.168.0.163', 'MASTER_PORT': '38407', 'NCCL_CUMEM_ENABLE': '0', 'NCCL_DEBUG': 'WARN', 'RANK': '6', 'RAY_LOCAL_WORLD_SIZE': '16', 'TOKENIZERS_PARALLELISM': 'true', 'VLLM_ALLOW_RUNTIME_LORA_UPDATING': 'true', 'WG_BACKEND': 'ray', 'WG_PREFIX': 'Ux93Mb', 'WORLD_SIZE': '16'}}
[36m(WorkerDict pid=375746)[0m INFO 12-13 15:23:34 [parallel.py:380] Using external launcher for distributed inference.
[36m(WorkerDict pid=375746)[0m INFO 12-13 15:23:34 [parallel.py:420] Disabling V1 multiprocessing for external launcher.
[36m(WorkerDict pid=375746)[0m INFO 12-13 15:23:34 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=36864.
[36m(WorkerDict pid=375746)[0m INFO 12-13 15:23:34 [__init__.py:381] Cudagraph is disabled under eager mode
[36m(WorkerDict pid=375746)[0m INFO 12-13 15:23:34 [platform.py:141] Non-MLA LLMs forcibly disable the chunked prefill feature,as the performance of operators supporting this feature functionality is currently suboptimal.
[36m(WorkerDict pid=375746)[0m INFO 12-13 15:23:34 [platform.py:179] Compilation disabled, using eager mode by default
[36m(WorkerDict pid=375743)[0m INFO 12-13 15:23:34 [model.py:547] Resolved architecture: Qwen3ForCausalLM
[36m(WorkerDict pid=375743)[0m INFO 12-13 15:23:34 [model.py:1510] Using max model len 36864
[36m(WorkerDict pid=375743)[0m INFO 12-13 15:23:34 [arg_utils.py:1215] Using ray runtime env: {'env_vars': {'MASTER_ADDR': '192.168.0.163', 'MASTER_PORT': '38407', 'NCCL_CUMEM_ENABLE': '0', 'NCCL_DEBUG': 'WARN', 'RANK': '3', 'RAY_LOCAL_WORLD_SIZE': '16', 'TOKENIZERS_PARALLELISM': 'true', 'VLLM_ALLOW_RUNTIME_LORA_UPDATING': 'true', 'WG_BACKEND': 'ray', 'WG_PREFIX': 'Ux93Mb', 'WORLD_SIZE': '16'}}
[36m(WorkerDict pid=375743)[0m INFO 12-13 15:23:34 [parallel.py:380] Using external launcher for distributed inference.
[36m(WorkerDict pid=375743)[0m INFO 12-13 15:23:34 [parallel.py:420] Disabling V1 multiprocessing for external launcher.
[36m(WorkerDict pid=375743)[0m INFO 12-13 15:23:34 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=36864.
[36m(WorkerDict pid=375744)[0m INFO 12-13 15:23:34 [model.py:547] Resolved architecture: Qwen3ForCausalLM
[36m(WorkerDict pid=375744)[0m INFO 12-13 15:23:34 [model.py:1510] Using max model len 36864
[36m(WorkerDict pid=375744)[0m INFO 12-13 15:23:34 [arg_utils.py:1215] Using ray runtime env: {'env_vars': {'MASTER_ADDR': '192.168.0.163', 'MASTER_PORT': '38407', 'NCCL_CUMEM_ENABLE': '0', 'NCCL_DEBUG': 'WARN', 'RANK': '4', 'RAY_LOCAL_WORLD_SIZE': '16', 'TOKENIZERS_PARALLELISM': 'true', 'VLLM_ALLOW_RUNTIME_LORA_UPDATING': 'true', 'WG_BACKEND': 'ray', 'WG_PREFIX': 'Ux93Mb', 'WORLD_SIZE': '16'}}
[36m(WorkerDict pid=375744)[0m INFO 12-13 15:23:34 [parallel.py:380] Using external launcher for distributed inference.
[36m(WorkerDict pid=375744)[0m INFO 12-13 15:23:34 [parallel.py:420] Disabling V1 multiprocessing for external launcher.
[36m(WorkerDict pid=375744)[0m INFO 12-13 15:23:34 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=36864.
[36m(WorkerDict pid=375744)[0m INFO 12-13 15:23:34 [__init__.py:381] Cudagraph is disabled under eager mode
[36m(WorkerDict pid=375744)[0m INFO 12-13 15:23:34 [platform.py:141] Non-MLA LLMs forcibly disable the chunked prefill feature,as the performance of operators supporting this feature functionality is currently suboptimal.
[36m(WorkerDict pid=375744)[0m INFO 12-13 15:23:34 [platform.py:179] Compilation disabled, using eager mode by default
[36m(WorkerDict pid=375747)[0m INFO 12-13 15:23:34 [model.py:547] Resolved architecture: Qwen3ForCausalLM
[36m(WorkerDict pid=375747)[0m INFO 12-13 15:23:34 [model.py:1510] Using max model len 36864
[36m(WorkerDict pid=375747)[0m INFO 12-13 15:23:34 [arg_utils.py:1215] Using ray runtime env: {'env_vars': {'MASTER_ADDR': '192.168.0.163', 'MASTER_PORT': '38407', 'NCCL_CUMEM_ENABLE': '0', 'NCCL_DEBUG': 'WARN', 'RANK': '7', 'RAY_LOCAL_WORLD_SIZE': '16', 'TOKENIZERS_PARALLELISM': 'true', 'VLLM_ALLOW_RUNTIME_LORA_UPDATING': 'true', 'WG_BACKEND': 'ray', 'WG_PREFIX': 'Ux93Mb', 'WORLD_SIZE': '16'}}
[36m(WorkerDict pid=375747)[0m INFO 12-13 15:23:34 [parallel.py:380] Using external launcher for distributed inference.
[36m(WorkerDict pid=375747)[0m INFO 12-13 15:23:34 [parallel.py:420] Disabling V1 multiprocessing for external launcher.
[36m(WorkerDict pid=375747)[0m INFO 12-13 15:23:34 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=36864.
[36m(WorkerDict pid=375747)[0m INFO 12-13 15:23:34 [__init__.py:381] Cudagraph is disabled under eager mode
[36m(WorkerDict pid=375747)[0m INFO 12-13 15:23:34 [platform.py:141] Non-MLA LLMs forcibly disable the chunked prefill feature,as the performance of operators supporting this feature functionality is currently suboptimal.
[36m(WorkerDict pid=375747)[0m INFO 12-13 15:23:34 [platform.py:179] Compilation disabled, using eager mode by default
[36m(WorkerDict pid=375752)[0m INFO 12-13 15:23:34 [model.py:547] Resolved architecture: Qwen3ForCausalLM
[36m(WorkerDict pid=375752)[0m INFO 12-13 15:23:34 [model.py:1510] Using max model len 36864
[36m(WorkerDict pid=375752)[0m INFO 12-13 15:23:34 [arg_utils.py:1215] Using ray runtime env: {'env_vars': {'MASTER_ADDR': '192.168.0.163', 'MASTER_PORT': '38407', 'NCCL_CUMEM_ENABLE': '0', 'NCCL_DEBUG': 'WARN', 'RANK': '15', 'RAY_LOCAL_WORLD_SIZE': '16', 'TOKENIZERS_PARALLELISM': 'true', 'VLLM_ALLOW_RUNTIME_LORA_UPDATING': 'true', 'WG_BACKEND': 'ray', 'WG_PREFIX': 'Ux93Mb', 'WORLD_SIZE': '16'}}
[36m(WorkerDict pid=375752)[0m INFO 12-13 15:23:34 [parallel.py:380] Using external launcher for distributed inference.
[36m(WorkerDict pid=375752)[0m INFO 12-13 15:23:34 [parallel.py:420] Disabling V1 multiprocessing for external launcher.
[36m(WorkerDict pid=375752)[0m INFO 12-13 15:23:34 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=36864.
[36m(WorkerDict pid=375752)[0m INFO 12-13 15:23:34 [__init__.py:381] Cudagraph is disabled under eager mode
[36m(WorkerDict pid=375752)[0m INFO 12-13 15:23:34 [platform.py:141] Non-MLA LLMs forcibly disable the chunked prefill feature,as the performance of operators supporting this feature functionality is currently suboptimal.
[36m(WorkerDict pid=375752)[0m INFO 12-13 15:23:34 [platform.py:179] Compilation disabled, using eager mode by default
[36m(WorkerDict pid=375751)[0m INFO 12-13 15:23:34 [model.py:547] Resolved architecture: Qwen3ForCausalLM
[36m(WorkerDict pid=375751)[0m INFO 12-13 15:23:34 [model.py:1510] Using max model len 36864
[36m(WorkerDict pid=375751)[0m INFO 12-13 15:23:34 [arg_utils.py:1215] Using ray runtime env: {'env_vars': {'MASTER_ADDR': '192.168.0.163', 'MASTER_PORT': '38407', 'NCCL_CUMEM_ENABLE': '0', 'NCCL_DEBUG': 'WARN', 'RANK': '11', 'RAY_LOCAL_WORLD_SIZE': '16', 'TOKENIZERS_PARALLELISM': 'true', 'VLLM_ALLOW_RUNTIME_LORA_UPDATING': 'true', 'WG_BACKEND': 'ray', 'WG_PREFIX': 'Ux93Mb', 'WORLD_SIZE': '16'}}
[36m(WorkerDict pid=375751)[0m INFO 12-13 15:23:34 [parallel.py:380] Using external launcher for distributed inference.
[36m(WorkerDict pid=375751)[0m INFO 12-13 15:23:34 [parallel.py:420] Disabling V1 multiprocessing for external launcher.
[36m(WorkerDict pid=375751)[0m INFO 12-13 15:23:34 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=36864.
[36m(WorkerDict pid=375751)[0m INFO 12-13 15:23:34 [__init__.py:381] Cudagraph is disabled under eager mode
[36m(WorkerDict pid=375751)[0m INFO 12-13 15:23:34 [platform.py:141] Non-MLA LLMs forcibly disable the chunked prefill feature,as the performance of operators supporting this feature functionality is currently suboptimal.
[36m(WorkerDict pid=375751)[0m INFO 12-13 15:23:34 [platform.py:179] Compilation disabled, using eager mode by default
[36m(WorkerDict pid=375750)[0m INFO 12-13 15:23:34 [model.py:547] Resolved architecture: Qwen3ForCausalLM
[36m(WorkerDict pid=375750)[0m INFO 12-13 15:23:34 [model.py:1510] Using max model len 36864
[36m(WorkerDict pid=375750)[0m INFO 12-13 15:23:34 [arg_utils.py:1215] Using ray runtime env: {'env_vars': {'MASTER_ADDR': '192.168.0.163', 'MASTER_PORT': '38407', 'NCCL_CUMEM_ENABLE': '0', 'NCCL_DEBUG': 'WARN', 'RANK': '10', 'RAY_LOCAL_WORLD_SIZE': '16', 'TOKENIZERS_PARALLELISM': 'true', 'VLLM_ALLOW_RUNTIME_LORA_UPDATING': 'true', 'WG_BACKEND': 'ray', 'WG_PREFIX': 'Ux93Mb', 'WORLD_SIZE': '16'}}
[36m(WorkerDict pid=375750)[0m INFO 12-13 15:23:34 [parallel.py:380] Using external launcher for distributed inference.
[36m(WorkerDict pid=375750)[0m INFO 12-13 15:23:34 [parallel.py:420] Disabling V1 multiprocessing for external launcher.
[36m(WorkerDict pid=375750)[0m INFO 12-13 15:23:34 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=36864.
[36m(WorkerDict pid=375750)[0m INFO 12-13 15:23:34 [__init__.py:381] Cudagraph is disabled under eager mode
[36m(WorkerDict pid=375750)[0m INFO 12-13 15:23:34 [platform.py:141] Non-MLA LLMs forcibly disable the chunked prefill feature,as the performance of operators supporting this feature functionality is currently suboptimal.
[36m(WorkerDict pid=375750)[0m INFO 12-13 15:23:34 [platform.py:179] Compilation disabled, using eager mode by default
[36m(WorkerDict pid=375748)[0m INFO 12-13 15:23:34 [model.py:547] Resolved architecture: Qwen3ForCausalLM
[36m(WorkerDict pid=375748)[0m INFO 12-13 15:23:34 [model.py:1510] Using max model len 36864
[36m(WorkerDict pid=375748)[0m INFO 12-13 15:23:34 [arg_utils.py:1215] Using ray runtime env: {'env_vars': {'MASTER_ADDR': '192.168.0.163', 'MASTER_PORT': '38407', 'NCCL_CUMEM_ENABLE': '0', 'NCCL_DEBUG': 'WARN', 'RANK': '8', 'RAY_LOCAL_WORLD_SIZE': '16', 'TOKENIZERS_PARALLELISM': 'true', 'VLLM_ALLOW_RUNTIME_LORA_UPDATING': 'true', 'WG_BACKEND': 'ray', 'WG_PREFIX': 'Ux93Mb', 'WORLD_SIZE': '16'}}
[36m(WorkerDict pid=375748)[0m INFO 12-13 15:23:34 [parallel.py:380] Using external launcher for distributed inference.
[36m(WorkerDict pid=375748)[0m INFO 12-13 15:23:34 [parallel.py:420] Disabling V1 multiprocessing for external launcher.
[36m(WorkerDict pid=375748)[0m INFO 12-13 15:23:34 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=36864.
[36m(WorkerDict pid=375748)[0m INFO 12-13 15:23:34 [__init__.py:381] Cudagraph is disabled under eager mode
[36m(WorkerDict pid=375748)[0m INFO 12-13 15:23:34 [platform.py:141] Non-MLA LLMs forcibly disable the chunked prefill feature,as the performance of operators supporting this feature functionality is currently suboptimal.
[36m(WorkerDict pid=375748)[0m INFO 12-13 15:23:34 [platform.py:179] Compilation disabled, using eager mode by default
[36m(WorkerDict pid=375749)[0m INFO 12-13 15:23:34 [model.py:547] Resolved architecture: Qwen3ForCausalLM
[36m(WorkerDict pid=375749)[0m INFO 12-13 15:23:34 [model.py:1510] Using max model len 36864
[36m(WorkerDict pid=375749)[0m INFO 12-13 15:23:34 [arg_utils.py:1215] Using ray runtime env: {'env_vars': {'MASTER_ADDR': '192.168.0.163', 'MASTER_PORT': '38407', 'NCCL_CUMEM_ENABLE': '0', 'NCCL_DEBUG': 'WARN', 'RANK': '9', 'RAY_LOCAL_WORLD_SIZE': '16', 'TOKENIZERS_PARALLELISM': 'true', 'VLLM_ALLOW_RUNTIME_LORA_UPDATING': 'true', 'WG_BACKEND': 'ray', 'WG_PREFIX': 'Ux93Mb', 'WORLD_SIZE': '16'}}
[36m(WorkerDict pid=375753)[0m INFO 12-13 15:23:34 [model.py:547] Resolved architecture: Qwen3ForCausalLM
[36m(WorkerDict pid=375753)[0m INFO 12-13 15:23:34 [model.py:1510] Using max model len 36864
[36m(WorkerDict pid=375753)[0m INFO 12-13 15:23:34 [arg_utils.py:1215] Using ray runtime env: {'env_vars': {'MASTER_ADDR': '192.168.0.163', 'MASTER_PORT': '38407', 'NCCL_CUMEM_ENABLE': '0', 'NCCL_DEBUG': 'WARN', 'RANK': '14', 'RAY_LOCAL_WORLD_SIZE': '16', 'TOKENIZERS_PARALLELISM': 'true', 'VLLM_ALLOW_RUNTIME_LORA_UPDATING': 'true', 'WG_BACKEND': 'ray', 'WG_PREFIX': 'Ux93Mb', 'WORLD_SIZE': '16'}}
[36m(WorkerDict pid=375753)[0m INFO 12-13 15:23:34 [parallel.py:380] Using external launcher for distributed inference.
[36m(WorkerDict pid=375753)[0m INFO 12-13 15:23:34 [parallel.py:420] Disabling V1 multiprocessing for external launcher.
[36m(WorkerDict pid=375753)[0m INFO 12-13 15:23:34 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=36864.
[36m(WorkerDict pid=375753)[0m INFO 12-13 15:23:34 [__init__.py:381] Cudagraph is disabled under eager mode
[36m(WorkerDict pid=375753)[0m INFO 12-13 15:23:34 [platform.py:141] Non-MLA LLMs forcibly disable the chunked prefill feature,as the performance of operators supporting this feature functionality is currently suboptimal.
[36m(WorkerDict pid=375753)[0m INFO 12-13 15:23:34 [platform.py:179] Compilation disabled, using eager mode by default
[36m(WorkerDict pid=375754)[0m INFO 12-13 15:23:34 [model.py:547] Resolved architecture: Qwen3ForCausalLM
[36m(WorkerDict pid=375754)[0m INFO 12-13 15:23:34 [model.py:1510] Using max model len 36864
[36m(WorkerDict pid=375754)[0m INFO 12-13 15:23:34 [arg_utils.py:1215] Using ray runtime env: {'env_vars': {'MASTER_ADDR': '192.168.0.163', 'MASTER_PORT': '38407', 'NCCL_CUMEM_ENABLE': '0', 'NCCL_DEBUG': 'WARN', 'RANK': '13', 'RAY_LOCAL_WORLD_SIZE': '16', 'TOKENIZERS_PARALLELISM': 'true', 'VLLM_ALLOW_RUNTIME_LORA_UPDATING': 'true', 'WG_BACKEND': 'ray', 'WG_PREFIX': 'Ux93Mb', 'WORLD_SIZE': '16'}}
[36m(WorkerDict pid=375754)[0m INFO 12-13 15:23:34 [parallel.py:380] Using external launcher for distributed inference.
[36m(WorkerDict pid=375754)[0m INFO 12-13 15:23:34 [parallel.py:420] Disabling V1 multiprocessing for external launcher.
[36m(WorkerDict pid=375754)[0m INFO 12-13 15:23:34 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=36864.
[36m(WorkerDict pid=375754)[0m INFO 12-13 15:23:34 [__init__.py:381] Cudagraph is disabled under eager mode
[36m(WorkerDict pid=375754)[0m INFO 12-13 15:23:34 [platform.py:141] Non-MLA LLMs forcibly disable the chunked prefill feature,as the performance of operators supporting this feature functionality is currently suboptimal.
[36m(WorkerDict pid=375754)[0m INFO 12-13 15:23:34 [platform.py:179] Compilation disabled, using eager mode by default
[36m(WorkerDict pid=375758)[0m INFO 12-13 15:23:34 [model.py:547] Resolved architecture: Qwen3ForCausalLM
[36m(WorkerDict pid=375758)[0m INFO 12-13 15:23:34 [model.py:1510] Using max model len 36864
[36m(WorkerDict pid=375758)[0m INFO 12-13 15:23:34 [arg_utils.py:1215] Using ray runtime env: {'env_vars': {'MASTER_ADDR': '192.168.0.163', 'MASTER_PORT': '38407', 'NCCL_CUMEM_ENABLE': '0', 'NCCL_DEBUG': 'WARN', 'RANK': '12', 'RAY_LOCAL_WORLD_SIZE': '16', 'TOKENIZERS_PARALLELISM': 'true', 'VLLM_ALLOW_RUNTIME_LORA_UPDATING': 'true', 'WG_BACKEND': 'ray', 'WG_PREFIX': 'Ux93Mb', 'WORLD_SIZE': '16'}}
[36m(WorkerDict pid=375758)[0m INFO 12-13 15:23:34 [parallel.py:380] Using external launcher for distributed inference.
[36m(WorkerDict pid=375758)[0m INFO 12-13 15:23:34 [parallel.py:420] Disabling V1 multiprocessing for external launcher.
[36m(WorkerDict pid=375758)[0m INFO 12-13 15:23:34 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=36864.
[36m(WorkerDict pid=375758)[0m INFO 12-13 15:23:34 [__init__.py:381] Cudagraph is disabled under eager mode
[36m(WorkerDict pid=375758)[0m INFO 12-13 15:23:34 [platform.py:141] Non-MLA LLMs forcibly disable the chunked prefill feature,as the performance of operators supporting this feature functionality is currently suboptimal.
[36m(WorkerDict pid=375758)[0m INFO 12-13 15:23:34 [platform.py:179] Compilation disabled, using eager mode by default
[36m(WorkerDict pid=375742)[0m INFO 12-13 15:23:34 [platform.py:141] Non-MLA LLMs forcibly disable the chunked prefill feature,as the performance of operators supporting this feature functionality is currently suboptimal.
[36m(WorkerDict pid=375742)[0m INFO 12-13 15:23:34 [platform.py:179] Compilation disabled, using eager mode by default
[36m(WorkerDict pid=375745)[0m INFO 12-13 15:23:34 [arg_utils.py:1215] Using ray runtime env: {'env_vars': {'MASTER_ADDR': '192.168.0.163', 'MASTER_PORT': '38407', 'NCCL_CUMEM_ENABLE': '0', 'NCCL_DEBUG': 'WARN', 'RANK': '5', 'RAY_LOCAL_WORLD_SIZE': '16', 'TOKENIZERS_PARALLELISM': 'true', 'VLLM_ALLOW_RUNTIME_LORA_UPDATING': 'true', 'WG_BACKEND': 'ray', 'WG_PREFIX': 'Ux93Mb', 'WORLD_SIZE': '16'}}
[36m(WorkerDict pid=375745)[0m INFO 12-13 15:23:34 [parallel.py:380] Using external launcher for distributed inference.
[36m(WorkerDict pid=375745)[0m INFO 12-13 15:23:34 [parallel.py:420] Disabling V1 multiprocessing for external launcher.
[36m(WorkerDict pid=375745)[0m INFO 12-13 15:23:34 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=36864.
[36m(WorkerDict pid=375745)[0m INFO 12-13 15:23:34 [__init__.py:381] Cudagraph is disabled under eager mode
[36m(WorkerDict pid=375745)[0m INFO 12-13 15:23:34 [platform.py:141] Non-MLA LLMs forcibly disable the chunked prefill feature,as the performance of operators supporting this feature functionality is currently suboptimal.
[36m(WorkerDict pid=375745)[0m INFO 12-13 15:23:34 [platform.py:179] Compilation disabled, using eager mode by default
[36m(WorkerDict pid=375743)[0m INFO 12-13 15:23:34 [__init__.py:381] Cudagraph is disabled under eager mode
[36m(WorkerDict pid=375743)[0m INFO 12-13 15:23:34 [platform.py:141] Non-MLA LLMs forcibly disable the chunked prefill feature,as the performance of operators supporting this feature functionality is currently suboptimal.
[36m(WorkerDict pid=375743)[0m INFO 12-13 15:23:34 [platform.py:179] Compilation disabled, using eager mode by default
[36m(WorkerDict pid=375749)[0m INFO 12-13 15:23:34 [parallel.py:380] Using external launcher for distributed inference.
[36m(WorkerDict pid=375749)[0m INFO 12-13 15:23:34 [parallel.py:420] Disabling V1 multiprocessing for external launcher.
[36m(WorkerDict pid=375749)[0m INFO 12-13 15:23:34 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=36864.
[36m(WorkerDict pid=375749)[0m INFO 12-13 15:23:34 [__init__.py:381] Cudagraph is disabled under eager mode
[36m(WorkerDict pid=375749)[0m INFO 12-13 15:23:34 [platform.py:141] Non-MLA LLMs forcibly disable the chunked prefill feature,as the performance of operators supporting this feature functionality is currently suboptimal.
[36m(WorkerDict pid=375749)[0m INFO 12-13 15:23:34 [platform.py:179] Compilation disabled, using eager mode by default
[36m(WorkerDict pid=375742)[0m INFO 12-13 15:23:35 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='/home/data/Qwen3-32B', speculative_config=SpeculativeConfig(method='sam', model='/home/data/Qwen3-32B', num_spec_tokens=3), tokenizer='/home/data/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=36864, download_dir=None, load_format=dummy, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=npu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/data/Qwen3-32B, enable_prefix_caching=False, chunked_prefill_enabled=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[36m(WorkerDict pid=375740)[0m INFO 12-13 15:23:35 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='/home/data/Qwen3-32B', speculative_config=SpeculativeConfig(method='sam', model='/home/data/Qwen3-32B', num_spec_tokens=3), tokenizer='/home/data/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=36864, download_dir=None, load_format=dummy, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=npu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/data/Qwen3-32B, enable_prefix_caching=False, chunked_prefill_enabled=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[36m(WorkerDict pid=375746)[0m INFO 12-13 15:23:35 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='/home/data/Qwen3-32B', speculative_config=SpeculativeConfig(method='sam', model='/home/data/Qwen3-32B', num_spec_tokens=3), tokenizer='/home/data/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=36864, download_dir=None, load_format=dummy, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=npu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/data/Qwen3-32B, enable_prefix_caching=False, chunked_prefill_enabled=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[36m(WorkerDict pid=375743)[0m INFO 12-13 15:23:35 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='/home/data/Qwen3-32B', speculative_config=SpeculativeConfig(method='sam', model='/home/data/Qwen3-32B', num_spec_tokens=3), tokenizer='/home/data/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=36864, download_dir=None, load_format=dummy, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=npu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/data/Qwen3-32B, enable_prefix_caching=False, chunked_prefill_enabled=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[36m(WorkerDict pid=375744)[0m INFO 12-13 15:23:35 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='/home/data/Qwen3-32B', speculative_config=SpeculativeConfig(method='sam', model='/home/data/Qwen3-32B', num_spec_tokens=3), tokenizer='/home/data/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=36864, download_dir=None, load_format=dummy, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=npu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/data/Qwen3-32B, enable_prefix_caching=False, chunked_prefill_enabled=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[36m(WorkerDict pid=375747)[0m INFO 12-13 15:23:35 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='/home/data/Qwen3-32B', speculative_config=SpeculativeConfig(method='sam', model='/home/data/Qwen3-32B', num_spec_tokens=3), tokenizer='/home/data/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=36864, download_dir=None, load_format=dummy, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=npu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/data/Qwen3-32B, enable_prefix_caching=False, chunked_prefill_enabled=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[36m(WorkerDict pid=375751)[0m INFO 12-13 15:23:35 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='/home/data/Qwen3-32B', speculative_config=SpeculativeConfig(method='sam', model='/home/data/Qwen3-32B', num_spec_tokens=3), tokenizer='/home/data/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=36864, download_dir=None, load_format=dummy, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=npu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/data/Qwen3-32B, enable_prefix_caching=False, chunked_prefill_enabled=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[36m(WorkerDict pid=375750)[0m INFO 12-13 15:23:35 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='/home/data/Qwen3-32B', speculative_config=SpeculativeConfig(method='sam', model='/home/data/Qwen3-32B', num_spec_tokens=3), tokenizer='/home/data/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=36864, download_dir=None, load_format=dummy, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=npu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/data/Qwen3-32B, enable_prefix_caching=False, chunked_prefill_enabled=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[36m(WorkerDict pid=375748)[0m INFO 12-13 15:23:35 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='/home/data/Qwen3-32B', speculative_config=SpeculativeConfig(method='sam', model='/home/data/Qwen3-32B', num_spec_tokens=3), tokenizer='/home/data/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=36864, download_dir=None, load_format=dummy, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=npu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/data/Qwen3-32B, enable_prefix_caching=False, chunked_prefill_enabled=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[36m(WorkerDict pid=375734)[0m INFO 12-13 15:23:35 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='/home/data/Qwen3-32B', speculative_config=SpeculativeConfig(method='sam', model='/home/data/Qwen3-32B', num_spec_tokens=3), tokenizer='/home/data/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=36864, download_dir=None, load_format=dummy, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=npu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/data/Qwen3-32B, enable_prefix_caching=False, chunked_prefill_enabled=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[36m(WorkerDict pid=375758)[0m INFO 12-13 15:23:35 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='/home/data/Qwen3-32B', speculative_config=SpeculativeConfig(method='sam', model='/home/data/Qwen3-32B', num_spec_tokens=3), tokenizer='/home/data/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=36864, download_dir=None, load_format=dummy, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=npu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/data/Qwen3-32B, enable_prefix_caching=False, chunked_prefill_enabled=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[36m(WorkerDict pid=375745)[0m INFO 12-13 15:23:35 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='/home/data/Qwen3-32B', speculative_config=SpeculativeConfig(method='sam', model='/home/data/Qwen3-32B', num_spec_tokens=3), tokenizer='/home/data/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=36864, download_dir=None, load_format=dummy, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=npu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/data/Qwen3-32B, enable_prefix_caching=False, chunked_prefill_enabled=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[36m(WorkerDict pid=375752)[0m INFO 12-13 15:23:35 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='/home/data/Qwen3-32B', speculative_config=SpeculativeConfig(method='sam', model='/home/data/Qwen3-32B', num_spec_tokens=3), tokenizer='/home/data/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=36864, download_dir=None, load_format=dummy, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=npu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/data/Qwen3-32B, enable_prefix_caching=False, chunked_prefill_enabled=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[36m(WorkerDict pid=375749)[0m INFO 12-13 15:23:35 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='/home/data/Qwen3-32B', speculative_config=SpeculativeConfig(method='sam', model='/home/data/Qwen3-32B', num_spec_tokens=3), tokenizer='/home/data/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=36864, download_dir=None, load_format=dummy, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=npu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/data/Qwen3-32B, enable_prefix_caching=False, chunked_prefill_enabled=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[36m(WorkerDict pid=375753)[0m INFO 12-13 15:23:35 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='/home/data/Qwen3-32B', speculative_config=SpeculativeConfig(method='sam', model='/home/data/Qwen3-32B', num_spec_tokens=3), tokenizer='/home/data/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=36864, download_dir=None, load_format=dummy, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=npu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/data/Qwen3-32B, enable_prefix_caching=False, chunked_prefill_enabled=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[36m(WorkerDict pid=375754)[0m INFO 12-13 15:23:35 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='/home/data/Qwen3-32B', speculative_config=SpeculativeConfig(method='sam', model='/home/data/Qwen3-32B', num_spec_tokens=3), tokenizer='/home/data/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=36864, download_dir=None, load_format=dummy, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=npu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/data/Qwen3-32B, enable_prefix_caching=False, chunked_prefill_enabled=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[36m(WorkerDict pid=375740)[0m INFO 12-13 15:23:44 [model_runner_v1.py:2660] Starting to load model /home/data/Qwen3-32B...
[36m(WorkerDict pid=375743)[0m INFO 12-13 15:23:44 [model_runner_v1.py:2660] Starting to load model /home/data/Qwen3-32B...
[36m(WorkerDict pid=375752)[0m INFO 12-13 15:23:44 [model_runner_v1.py:2660] Starting to load model /home/data/Qwen3-32B...
[36m(WorkerDict pid=375740)[0m INFO 12-13 15:23:44 [model_runner_v1.py:2677] Loading drafter model...
[36m(WorkerDict pid=375743)[0m INFO 12-13 15:23:44 [model_runner_v1.py:2677] Loading drafter model...
[36m(WorkerDict pid=375752)[0m INFO 12-13 15:23:45 [model_runner_v1.py:2677] Loading drafter model...
[36m(WorkerDict pid=375749)[0m INFO 12-13 15:23:45 [model_runner_v1.py:2660] Starting to load model /home/data/Qwen3-32B...
[36m(WorkerDict pid=375745)[0m INFO 12-13 15:23:46 [model_runner_v1.py:2660] Starting to load model /home/data/Qwen3-32B...
[36m(WorkerDict pid=375740)[0m INFO 12-13 15:23:46 [model_runner_v1.py:2694] Loading model weights took 7.6393 GB
[36m(WorkerDict pid=375752)[0m INFO 12-13 15:23:46 [model_runner_v1.py:2694] Loading model weights took 7.6393 GB
[36m(WorkerDict pid=375743)[0m INFO 12-13 15:23:46 [model_runner_v1.py:2694] Loading model weights took 7.6393 GB
[36m(WorkerDict pid=375745)[0m INFO 12-13 15:23:47 [model_runner_v1.py:2677] Loading drafter model...
[36m(WorkerDict pid=375749)[0m INFO 12-13 15:23:47 [model_runner_v1.py:2677] Loading drafter model...
[36m(WorkerDict pid=375745)[0m INFO 12-13 15:23:48 [model_runner_v1.py:2694] Loading model weights took 7.6393 GB
[36m(WorkerDict pid=375749)[0m INFO 12-13 15:23:49 [model_runner_v1.py:2694] Loading model weights took 7.6393 GB
[36m(WorkerDict pid=375748)[0m INFO 12-13 15:23:56 [model_runner_v1.py:2660] Starting to load model /home/data/Qwen3-32B...
[36m(WorkerDict pid=375746)[0m INFO 12-13 15:23:56 [model_runner_v1.py:2660] Starting to load model /home/data/Qwen3-32B...
[36m(WorkerDict pid=375748)[0m INFO 12-13 15:23:57 [model_runner_v1.py:2677] Loading drafter model...
[36m(WorkerDict pid=375746)[0m INFO 12-13 15:23:57 [model_runner_v1.py:2677] Loading drafter model...
[36m(WorkerDict pid=375748)[0m INFO 12-13 15:23:57 [model_runner_v1.py:2694] Loading model weights took 7.6393 GB
[36m(WorkerDict pid=375753)[0m INFO 12-13 15:23:57 [model_runner_v1.py:2660] Starting to load model /home/data/Qwen3-32B...
[36m(WorkerDict pid=375746)[0m INFO 12-13 15:23:58 [model_runner_v1.py:2694] Loading model weights took 7.6393 GB
[36m(WorkerDict pid=375753)[0m INFO 12-13 15:23:58 [model_runner_v1.py:2677] Loading drafter model...
[36m(WorkerDict pid=375754)[0m INFO 12-13 15:23:58 [model_runner_v1.py:2660] Starting to load model /home/data/Qwen3-32B...
[36m(WorkerDict pid=375758)[0m INFO 12-13 15:23:59 [model_runner_v1.py:2660] Starting to load model /home/data/Qwen3-32B...
[36m(WorkerDict pid=375744)[0m INFO 12-13 15:23:59 [model_runner_v1.py:2660] Starting to load model /home/data/Qwen3-32B...
[36m(WorkerDict pid=375744)[0m INFO 12-13 15:23:59 [model_runner_v1.py:2677] Loading drafter model...
[36m(WorkerDict pid=375753)[0m INFO 12-13 15:23:59 [model_runner_v1.py:2694] Loading model weights took 7.6393 GB
[36m(WorkerDict pid=375758)[0m INFO 12-13 15:23:59 [model_runner_v1.py:2677] Loading drafter model...
[36m(WorkerDict pid=375754)[0m INFO 12-13 15:23:59 [model_runner_v1.py:2677] Loading drafter model...
[36m(WorkerDict pid=375747)[0m INFO 12-13 15:23:59 [model_runner_v1.py:2660] Starting to load model /home/data/Qwen3-32B...
[36m(WorkerDict pid=375750)[0m INFO 12-13 15:23:59 [model_runner_v1.py:2660] Starting to load model /home/data/Qwen3-32B...
[36m(WorkerDict pid=375751)[0m INFO 12-13 15:24:00 [model_runner_v1.py:2660] Starting to load model /home/data/Qwen3-32B...
[36m(WorkerDict pid=375758)[0m INFO 12-13 15:24:00 [model_runner_v1.py:2694] Loading model weights took 7.6393 GB
[36m(WorkerDict pid=375744)[0m INFO 12-13 15:24:00 [model_runner_v1.py:2694] Loading model weights took 7.6393 GB
[36m(WorkerDict pid=375734)[0m INFO 12-13 15:24:00 [model_runner_v1.py:2660] Starting to load model /home/data/Qwen3-32B...
[36m(WorkerDict pid=375747)[0m INFO 12-13 15:24:00 [model_runner_v1.py:2677] Loading drafter model...
[36m(WorkerDict pid=375750)[0m INFO 12-13 15:24:00 [model_runner_v1.py:2677] Loading drafter model...
[36m(WorkerDict pid=375754)[0m INFO 12-13 15:24:00 [model_runner_v1.py:2694] Loading model weights took 7.6393 GB
[36m(WorkerDict pid=375751)[0m INFO 12-13 15:24:00 [model_runner_v1.py:2677] Loading drafter model...
[36m(WorkerDict pid=375734)[0m INFO 12-13 15:24:01 [model_runner_v1.py:2677] Loading drafter model...
[36m(WorkerDict pid=375750)[0m INFO 12-13 15:24:01 [model_runner_v1.py:2694] Loading model weights took 7.6393 GB
[36m(WorkerDict pid=375751)[0m INFO 12-13 15:24:01 [model_runner_v1.py:2694] Loading model weights took 7.6393 GB
[36m(WorkerDict pid=375742)[0m INFO 12-13 15:24:01 [model_runner_v1.py:2660] Starting to load model /home/data/Qwen3-32B...
[36m(WorkerDict pid=375747)[0m INFO 12-13 15:24:01 [model_runner_v1.py:2694] Loading model weights took 7.6393 GB
[36m(WorkerDict pid=375734)[0m INFO 12-13 15:24:01 [model_runner_v1.py:2694] Loading model weights took 7.6393 GB
[36m(WorkerDict pid=375742)[0m INFO 12-13 15:24:02 [model_runner_v1.py:2677] Loading drafter model...
[36m(WorkerDict pid=375742)[0m INFO 12-13 15:24:02 [model_runner_v1.py:2694] Loading model weights took 7.6393 GB
[36m(WorkerDict pid=375740)[0m INFO 12-13 15:24:10 [worker_v1.py:234] Available memory: 42063567564, total memory: 65796046848
[36m(WorkerDict pid=375747)[0m INFO 12-13 15:24:10 [worker_v1.py:234] Available memory: 42063772364, total memory: 65796046848
[36m(WorkerDict pid=375752)[0m INFO 12-13 15:24:10 [worker_v1.py:234] Available memory: 42065410764, total memory: 65796046848
[36m(WorkerDict pid=375748)[0m INFO 12-13 15:24:10 [worker_v1.py:234] Available memory: 41811893760, total memory: 65787658240
[36m(WorkerDict pid=375734)[0m INFO 12-13 15:24:10 [worker_v1.py:234] Available memory: 41803095552, total memory: 65787658240
[36m(WorkerDict pid=375746)[0m INFO 12-13 15:24:10 [worker_v1.py:234] Available memory: 41800560128, total memory: 65787658240
[36m(WorkerDict pid=375743)[0m INFO 12-13 15:24:10 [worker_v1.py:234] Available memory: 42056911564, total memory: 65796046848
[36m(WorkerDict pid=375750)[0m INFO 12-13 15:24:10 [worker_v1.py:234] Available memory: 41810562560, total memory: 65787658240
[36m(WorkerDict pid=375753)[0m INFO 12-13 15:24:10 [worker_v1.py:234] Available memory: 41805016576, total memory: 65787658240
[36m(WorkerDict pid=375749)[0m INFO 12-13 15:24:11 [worker_v1.py:234] Available memory: 42057202380, total memory: 65796046848
[36m(WorkerDict pid=375745)[0m INFO 12-13 15:24:11 [worker_v1.py:234] Available memory: 42056633036, total memory: 65796046848
[36m(WorkerDict pid=375758)[0m INFO 12-13 15:24:11 [worker_v1.py:234] Available memory: 41801166336, total memory: 65787658240
[36m(WorkerDict pid=375744)[0m INFO 12-13 15:24:11 [worker_v1.py:234] Available memory: 41805291008, total memory: 65787658240
[36m(WorkerDict pid=375754)[0m INFO 12-13 15:24:11 [worker_v1.py:234] Available memory: 42069080780, total memory: 65796046848
[36m(WorkerDict pid=375751)[0m INFO 12-13 15:24:11 [worker_v1.py:234] Available memory: 42059573964, total memory: 65796046848
[36m(WorkerDict pid=375742)[0m INFO 12-13 15:24:11 [worker_v1.py:234] Available memory: 41804799488, total memory: 65787658240
[36m(WorkerDict pid=375742)[0m INFO 12-13 15:24:11 [kv_cache_utils.py:1087] GPU KV cache size: 1,275,648 tokens
[36m(WorkerDict pid=375742)[0m INFO 12-13 15:24:11 [kv_cache_utils.py:1091] Maximum concurrency for 36,864 tokens per request: 34.60x
[36m(WorkerDict pid=375734)[0m INFO 12-13 15:24:11 [kv_cache_utils.py:1087] GPU KV cache size: 1,275,648 tokens
[36m(WorkerDict pid=375734)[0m INFO 12-13 15:24:11 [kv_cache_utils.py:1091] Maximum concurrency for 36,864 tokens per request: 34.60x
[36m(WorkerDict pid=375745)[0m INFO 12-13 15:24:11 [kv_cache_utils.py:1087] GPU KV cache size: 1,275,648 tokens
[36m(WorkerDict pid=375745)[0m INFO 12-13 15:24:11 [kv_cache_utils.py:1091] Maximum concurrency for 36,864 tokens per request: 34.60x
[36m(WorkerDict pid=375740)[0m INFO 12-13 15:24:11 [kv_cache_utils.py:1087] GPU KV cache size: 1,275,648 tokens
[36m(WorkerDict pid=375740)[0m INFO 12-13 15:24:11 [kv_cache_utils.py:1091] Maximum concurrency for 36,864 tokens per request: 34.60x
[36m(WorkerDict pid=375746)[0m INFO 12-13 15:24:11 [kv_cache_utils.py:1087] GPU KV cache size: 1,275,648 tokens
[36m(WorkerDict pid=375746)[0m INFO 12-13 15:24:11 [kv_cache_utils.py:1091] Maximum concurrency for 36,864 tokens per request: 34.60x
[36m(WorkerDict pid=375743)[0m INFO 12-13 15:24:11 [kv_cache_utils.py:1087] GPU KV cache size: 1,275,648 tokens
[36m(WorkerDict pid=375743)[0m INFO 12-13 15:24:11 [kv_cache_utils.py:1091] Maximum concurrency for 36,864 tokens per request: 34.60x
[36m(WorkerDict pid=375744)[0m INFO 12-13 15:24:11 [kv_cache_utils.py:1087] GPU KV cache size: 1,275,648 tokens
[36m(WorkerDict pid=375744)[0m INFO 12-13 15:24:11 [kv_cache_utils.py:1091] Maximum concurrency for 36,864 tokens per request: 34.60x
[36m(WorkerDict pid=375747)[0m INFO 12-13 15:24:11 [kv_cache_utils.py:1087] GPU KV cache size: 1,275,648 tokens
[36m(WorkerDict pid=375747)[0m INFO 12-13 15:24:11 [kv_cache_utils.py:1091] Maximum concurrency for 36,864 tokens per request: 34.60x
[36m(WorkerDict pid=375752)[0m INFO 12-13 15:24:11 [kv_cache_utils.py:1087] GPU KV cache size: 1,275,648 tokens
[36m(WorkerDict pid=375752)[0m INFO 12-13 15:24:11 [kv_cache_utils.py:1091] Maximum concurrency for 36,864 tokens per request: 34.60x
[36m(WorkerDict pid=375751)[0m INFO 12-13 15:24:11 [kv_cache_utils.py:1087] GPU KV cache size: 1,275,648 tokens
[36m(WorkerDict pid=375751)[0m INFO 12-13 15:24:11 [kv_cache_utils.py:1091] Maximum concurrency for 36,864 tokens per request: 34.60x
[36m(WorkerDict pid=375750)[0m INFO 12-13 15:24:11 [kv_cache_utils.py:1087] GPU KV cache size: 1,275,648 tokens
[36m(WorkerDict pid=375750)[0m INFO 12-13 15:24:11 [kv_cache_utils.py:1091] Maximum concurrency for 36,864 tokens per request: 34.60x
[36m(WorkerDict pid=375748)[0m INFO 12-13 15:24:11 [kv_cache_utils.py:1087] GPU KV cache size: 1,275,648 tokens
[36m(WorkerDict pid=375748)[0m INFO 12-13 15:24:11 [kv_cache_utils.py:1091] Maximum concurrency for 36,864 tokens per request: 34.60x
[36m(WorkerDict pid=375749)[0m INFO 12-13 15:24:11 [kv_cache_utils.py:1087] GPU KV cache size: 1,275,648 tokens
[36m(WorkerDict pid=375749)[0m INFO 12-13 15:24:11 [kv_cache_utils.py:1091] Maximum concurrency for 36,864 tokens per request: 34.60x
[36m(WorkerDict pid=375753)[0m INFO 12-13 15:24:11 [kv_cache_utils.py:1087] GPU KV cache size: 1,275,648 tokens
[36m(WorkerDict pid=375753)[0m INFO 12-13 15:24:11 [kv_cache_utils.py:1091] Maximum concurrency for 36,864 tokens per request: 34.60x
[36m(WorkerDict pid=375754)[0m INFO 12-13 15:24:11 [kv_cache_utils.py:1087] GPU KV cache size: 1,275,648 tokens
[36m(WorkerDict pid=375754)[0m INFO 12-13 15:24:11 [kv_cache_utils.py:1091] Maximum concurrency for 36,864 tokens per request: 34.60x
[36m(WorkerDict pid=375758)[0m INFO 12-13 15:24:11 [kv_cache_utils.py:1087] GPU KV cache size: 1,275,648 tokens
[36m(WorkerDict pid=375758)[0m INFO 12-13 15:24:11 [kv_cache_utils.py:1091] Maximum concurrency for 36,864 tokens per request: 34.60x
[36m(WorkerDict pid=375742)[0m INFO 12-13 15:24:11 [core.py:210] init engine (profile, create kv cache, warmup model) took 8.85 seconds
[36m(WorkerDict pid=375734)[0m INFO 12-13 15:24:11 [core.py:210] init engine (profile, create kv cache, warmup model) took 9.85 seconds
[36m(WorkerDict pid=375745)[0m INFO 12-13 15:24:11 [core.py:210] init engine (profile, create kv cache, warmup model) took 23.67 seconds
[36m(WorkerDict pid=375740)[0m INFO 12-13 15:24:11 [core.py:210] init engine (profile, create kv cache, warmup model) took 25.39 seconds
[36m(WorkerDict pid=375746)[0m INFO 12-13 15:24:11 [core.py:210] init engine (profile, create kv cache, warmup model) took 13.29 seconds
[36m(WorkerDict pid=375743)[0m INFO 12-13 15:24:11 [core.py:210] init engine (profile, create kv cache, warmup model) took 24.83 seconds
[36m(WorkerDict pid=375744)[0m INFO 12-13 15:24:11 [core.py:210] init engine (profile, create kv cache, warmup model) took 11.39 seconds
[36m(WorkerDict pid=375747)[0m INFO 12-13 15:24:11 [core.py:210] init engine (profile, create kv cache, warmup model) took 10.05 seconds
[36m(WorkerDict pid=375752)[0m INFO 12-13 15:24:11 [core.py:210] init engine (profile, create kv cache, warmup model) took 25.42 seconds
[36m(WorkerDict pid=375751)[0m INFO 12-13 15:24:11 [core.py:210] init engine (profile, create kv cache, warmup model) took 10.31 seconds
[36m(WorkerDict pid=375750)[0m INFO 12-13 15:24:11 [core.py:210] init engine (profile, create kv cache, warmup model) took 10.58 seconds
[36m(WorkerDict pid=375748)[0m INFO 12-13 15:24:11 [core.py:210] init engine (profile, create kv cache, warmup model) took 13.86 seconds
[36m(WorkerDict pid=375749)[0m INFO 12-13 15:24:11 [core.py:210] init engine (profile, create kv cache, warmup model) took 22.05 seconds
[36m(WorkerDict pid=375753)[0m INFO 12-13 15:24:11 [core.py:210] init engine (profile, create kv cache, warmup model) took 12.13 seconds
[36m(WorkerDict pid=375754)[0m INFO 12-13 15:24:11 [core.py:210] init engine (profile, create kv cache, warmup model) took 11.11 seconds
[36m(WorkerDict pid=375758)[0m INFO 12-13 15:24:11 [core.py:210] init engine (profile, create kv cache, warmup model) took 11.45 seconds
[36m(WorkerDict pid=375740)[0m WARNING 12-13 15:24:12 [core.py:112] Using configured V1 scheduler class vllm_ascend.core.scheduler.AscendScheduler. This scheduler interface is not public and compatibility may not be maintained.
[36m(WorkerDict pid=375740)[0m INFO 12-13 15:24:12 [llm.py:307] Supported_tasks: ('generate',)
[36m(WorkerDict pid=375743)[0m WARNING 12-13 15:24:12 [core.py:112] Using configured V1 scheduler class vllm_ascend.core.scheduler.AscendScheduler. This scheduler interface is not public and compatibility may not be maintained.
[36m(WorkerDict pid=375743)[0m INFO 12-13 15:24:12 [llm.py:307] Supported_tasks: ('generate',)
[36m(WorkerDict pid=375752)[0m WARNING 12-13 15:24:12 [core.py:112] Using configured V1 scheduler class vllm_ascend.core.scheduler.AscendScheduler. This scheduler interface is not public and compatibility may not be maintained.
[36m(WorkerDict pid=375752)[0m INFO 12-13 15:24:12 [llm.py:307] Supported_tasks: ('generate',)
[36m(WorkerDict pid=375745)[0m WARNING 12-13 15:24:12 [core.py:112] Using configured V1 scheduler class vllm_ascend.core.scheduler.AscendScheduler. This scheduler interface is not public and compatibility may not be maintained.
[36m(WorkerDict pid=375745)[0m INFO 12-13 15:24:12 [llm.py:307] Supported_tasks: ('generate',)
[36m(WorkerDict pid=375746)[0m WARNING 12-13 15:24:12 [core.py:112] Using configured V1 scheduler class vllm_ascend.core.scheduler.AscendScheduler. This scheduler interface is not public and compatibility may not be maintained.
[36m(WorkerDict pid=375746)[0m INFO 12-13 15:24:12 [llm.py:307] Supported_tasks: ('generate',)
[36m(WorkerDict pid=375747)[0m WARNING 12-13 15:24:12 [core.py:112] Using configured V1 scheduler class vllm_ascend.core.scheduler.AscendScheduler. This scheduler interface is not public and compatibility may not be maintained.
[36m(WorkerDict pid=375747)[0m INFO 12-13 15:24:12 [llm.py:307] Supported_tasks: ('generate',)
[36m(WorkerDict pid=375750)[0m WARNING 12-13 15:24:12 [core.py:112] Using configured V1 scheduler class vllm_ascend.core.scheduler.AscendScheduler. This scheduler interface is not public and compatibility may not be maintained.
[36m(WorkerDict pid=375750)[0m INFO 12-13 15:24:12 [llm.py:307] Supported_tasks: ('generate',)
[36m(WorkerDict pid=375748)[0m WARNING 12-13 15:24:12 [core.py:112] Using configured V1 scheduler class vllm_ascend.core.scheduler.AscendScheduler. This scheduler interface is not public and compatibility may not be maintained.
[36m(WorkerDict pid=375748)[0m INFO 12-13 15:24:12 [llm.py:307] Supported_tasks: ('generate',)
[36m(WorkerDict pid=375758)[0m WARNING 12-13 15:24:12 [core.py:112] Using configured V1 scheduler class vllm_ascend.core.scheduler.AscendScheduler. This scheduler interface is not public and compatibility may not be maintained.
[36m(WorkerDict pid=375758)[0m INFO 12-13 15:24:12 [llm.py:307] Supported_tasks: ('generate',)
[36m(WorkerDict pid=375742)[0m WARNING 12-13 15:24:12 [core.py:112] Using configured V1 scheduler class vllm_ascend.core.scheduler.AscendScheduler. This scheduler interface is not public and compatibility may not be maintained.
[36m(WorkerDict pid=375742)[0m INFO 12-13 15:24:12 [llm.py:307] Supported_tasks: ('generate',)
[36m(WorkerDict pid=375734)[0m WARNING 12-13 15:24:12 [core.py:112] Using configured V1 scheduler class vllm_ascend.core.scheduler.AscendScheduler. This scheduler interface is not public and compatibility may not be maintained.
[36m(WorkerDict pid=375734)[0m INFO 12-13 15:24:12 [llm.py:307] Supported_tasks: ('generate',)
[36m(WorkerDict pid=375744)[0m WARNING 12-13 15:24:12 [core.py:112] Using configured V1 scheduler class vllm_ascend.core.scheduler.AscendScheduler. This scheduler interface is not public and compatibility may not be maintained.
[36m(WorkerDict pid=375744)[0m INFO 12-13 15:24:12 [llm.py:307] Supported_tasks: ('generate',)
[36m(WorkerDict pid=375751)[0m WARNING 12-13 15:24:12 [core.py:112] Using configured V1 scheduler class vllm_ascend.core.scheduler.AscendScheduler. This scheduler interface is not public and compatibility may not be maintained.
[36m(WorkerDict pid=375751)[0m INFO 12-13 15:24:12 [llm.py:307] Supported_tasks: ('generate',)
[36m(WorkerDict pid=375749)[0m WARNING 12-13 15:24:12 [core.py:112] Using configured V1 scheduler class vllm_ascend.core.scheduler.AscendScheduler. This scheduler interface is not public and compatibility may not be maintained.
[36m(WorkerDict pid=375749)[0m INFO 12-13 15:24:12 [llm.py:307] Supported_tasks: ('generate',)
[36m(WorkerDict pid=375754)[0m WARNING 12-13 15:24:12 [core.py:112] Using configured V1 scheduler class vllm_ascend.core.scheduler.AscendScheduler. This scheduler interface is not public and compatibility may not be maintained.
[36m(WorkerDict pid=375754)[0m INFO 12-13 15:24:12 [llm.py:307] Supported_tasks: ('generate',)
[36m(WorkerDict pid=375753)[0m WARNING 12-13 15:24:12 [core.py:112] Using configured V1 scheduler class vllm_ascend.core.scheduler.AscendScheduler. This scheduler interface is not public and compatibility may not be maintained.
[36m(WorkerDict pid=375753)[0m INFO 12-13 15:24:12 [llm.py:307] Supported_tasks: ('generate',)
[36m(WorkerDict pid=375740)[0m [Rank 1 | Local Rank 0] 2025-12-13 15:24:21,684 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py:265] => The 'logprobs' parameter is incompatible with Speculative Decoding and has been disabled.
[36m(WorkerDict pid=375740)[0m kwargs: {'n': 1, 'max_tokens': 34816, 'repetition_penalty': 1.0, 'detokenize': False, 'temperature': 0.9, 'top_k': -1, 'top_p': 1.0, 'ignore_eos': False}
[36m(WorkerDict pid=375747)[0m [Rank 7 | Local Rank 0] 2025-12-13 15:24:21,593 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py:265] => The 'logprobs' parameter is incompatible with Speculative Decoding and has been disabled.
[36m(WorkerDict pid=375747)[0m kwargs: {'n': 1, 'max_tokens': 34816, 'repetition_penalty': 1.0, 'detokenize': False, 'temperature': 0.9, 'top_k': -1, 'top_p': 1.0, 'ignore_eos': False}
[36m(WorkerDict pid=375752)[0m [Rank 15 | Local Rank 0] 2025-12-13 15:24:21,594 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py:265] => The 'logprobs' parameter is incompatible with Speculative Decoding and has been disabled.
[36m(WorkerDict pid=375752)[0m kwargs: {'n': 1, 'max_tokens': 34816, 'repetition_penalty': 1.0, 'detokenize': False, 'temperature': 0.9, 'top_k': -1, 'top_p': 1.0, 'ignore_eos': False}
[36m(WorkerDict pid=375743)[0m [Rank 3 | Local Rank 0] 2025-12-13 15:24:21,706 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py:265] => The 'logprobs' parameter is incompatible with Speculative Decoding and has been disabled.
[36m(WorkerDict pid=375743)[0m kwargs: {'n': 1, 'max_tokens': 34816, 'repetition_penalty': 1.0, 'detokenize': False, 'temperature': 0.9, 'top_k': -1, 'top_p': 1.0, 'ignore_eos': False}
[36m(WorkerDict pid=375754)[0m [Rank 13 | Local Rank 0] 2025-12-13 15:24:21,853 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py:265] => The 'logprobs' parameter is incompatible with Speculative Decoding and has been disabled.
[36m(WorkerDict pid=375754)[0m kwargs: {'n': 1, 'max_tokens': 34816, 'repetition_penalty': 1.0, 'detokenize': False, 'temperature': 0.9, 'top_k': -1, 'top_p': 1.0, 'ignore_eos': False}
[36m(WorkerDict pid=375749)[0m [Rank 9 | Local Rank 0] 2025-12-13 15:24:22,116 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py:265] => The 'logprobs' parameter is incompatible with Speculative Decoding and has been disabled.
[36m(WorkerDict pid=375749)[0m kwargs: {'n': 1, 'max_tokens': 34816, 'repetition_penalty': 1.0, 'detokenize': False, 'temperature': 0.9, 'top_k': -1, 'top_p': 1.0, 'ignore_eos': False}
[36m(WorkerDict pid=375745)[0m [Rank 5 | Local Rank 0] 2025-12-13 15:24:22,278 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py:265] => The 'logprobs' parameter is incompatible with Speculative Decoding and has been disabled.
[36m(WorkerDict pid=375745)[0m kwargs: {'n': 1, 'max_tokens': 34816, 'repetition_penalty': 1.0, 'detokenize': False, 'temperature': 0.9, 'top_k': -1, 'top_p': 1.0, 'ignore_eos': False}
[36m(WorkerDict pid=375746)[0m [Rank 6 | Local Rank 0] 2025-12-13 15:24:22,355 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py:265] => The 'logprobs' parameter is incompatible with Speculative Decoding and has been disabled.
[36m(WorkerDict pid=375746)[0m kwargs: {'n': 1, 'max_tokens': 34816, 'repetition_penalty': 1.0, 'detokenize': False, 'temperature': 0.9, 'top_k': -1, 'top_p': 1.0, 'ignore_eos': False}
[36m(WorkerDict pid=375750)[0m [Rank 10 | Local Rank 0] 2025-12-13 15:24:22,392 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py:265] => The 'logprobs' parameter is incompatible with Speculative Decoding and has been disabled.
[36m(WorkerDict pid=375750)[0m kwargs: {'n': 1, 'max_tokens': 34816, 'repetition_penalty': 1.0, 'detokenize': False, 'temperature': 0.9, 'top_k': -1, 'top_p': 1.0, 'ignore_eos': False}
[36m(WorkerDict pid=375748)[0m [Rank 8 | Local Rank 0] 2025-12-13 15:24:22,397 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py:265] => The 'logprobs' parameter is incompatible with Speculative Decoding and has been disabled.
[36m(WorkerDict pid=375748)[0m kwargs: {'n': 1, 'max_tokens': 34816, 'repetition_penalty': 1.0, 'detokenize': False, 'temperature': 0.9, 'top_k': -1, 'top_p': 1.0, 'ignore_eos': False}
[36m(WorkerDict pid=375758)[0m [Rank 12 | Local Rank 0] 2025-12-13 15:24:22,357 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py:265] => The 'logprobs' parameter is incompatible with Speculative Decoding and has been disabled.
[36m(WorkerDict pid=375758)[0m kwargs: {'n': 1, 'max_tokens': 34816, 'repetition_penalty': 1.0, 'detokenize': False, 'temperature': 0.9, 'top_k': -1, 'top_p': 1.0, 'ignore_eos': False}
[36m(WorkerDict pid=375742)[0m [Rank 2 | Local Rank 0] 2025-12-13 15:24:22,426 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py:265] => The 'logprobs' parameter is incompatible with Speculative Decoding and has been disabled.
[36m(WorkerDict pid=375742)[0m kwargs: {'n': 1, 'max_tokens': 34816, 'repetition_penalty': 1.0, 'detokenize': False, 'temperature': 0.9, 'top_k': -1, 'top_p': 1.0, 'ignore_eos': False}
[36m(WorkerDict pid=375751)[0m [Rank 11 | Local Rank 0] 2025-12-13 15:24:22,683 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py:265] => The 'logprobs' parameter is incompatible with Speculative Decoding and has been disabled.
[36m(WorkerDict pid=375751)[0m kwargs: {'n': 1, 'max_tokens': 34816, 'repetition_penalty': 1.0, 'detokenize': False, 'temperature': 0.9, 'top_k': -1, 'top_p': 1.0, 'ignore_eos': False}
[36m(WorkerDict pid=375734)[0m [Rank 0 | Local Rank 0] 2025-12-13 15:24:22,847 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py:265] => The 'logprobs' parameter is incompatible with Speculative Decoding and has been disabled.
[36m(WorkerDict pid=375734)[0m kwargs: {'n': 1, 'max_tokens': 34816, 'repetition_penalty': 1.0, 'detokenize': False, 'temperature': 0.9, 'top_k': -1, 'top_p': 1.0, 'ignore_eos': False}
[36m(WorkerDict pid=375734)[0m Overridden TransformerConfig init config: {'num_layers': 64, 'hidden_size': 5120, 'num_attention_heads': 64, 'num_query_groups': 8, 'ffn_hidden_size': 25600, 'attention_dropout': 0.0, 'hidden_dropout': 0.0, 'kv_channels': 128, 'layernorm_epsilon': 1e-06, 'add_bias_linear': False, 'activation_func': <function silu at 0xffff00741ee0>, 'normalization': 'RMSNorm', 'gated_linear_unit': True, 'pipeline_dtype': torch.bfloat16, 'params_dtype': torch.bfloat16, 'bf16': True, 'tensor_model_parallel_size': 8, 'pipeline_model_parallel_size': 2, 'expert_model_parallel_size': 1, 'expert_tensor_parallel_size': 1, 'virtual_pipeline_model_parallel_size': None, 'context_parallel_size': 1, 'overlap_p2p_comm': False, 'batch_p2p_comm': False, 'sequence_parallel': True, 'variable_seq_lengths': True, 'masked_softmax_fusion': True, 'moe_token_dispatcher_type': 'alltoall', 'use_cpu_initialization': False, 'add_qkv_bias': False, 'qk_layernorm': True}
[36m(WorkerDict pid=375744)[0m [Rank 4 | Local Rank 0] 2025-12-13 15:24:22,857 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py:265] => The 'logprobs' parameter is incompatible with Speculative Decoding and has been disabled.
[36m(WorkerDict pid=375744)[0m kwargs: {'n': 1, 'max_tokens': 34816, 'repetition_penalty': 1.0, 'detokenize': False, 'temperature': 0.9, 'top_k': -1, 'top_p': 1.0, 'ignore_eos': False}
[36m(WorkerDict pid=375753)[0m [Rank 14 | Local Rank 0] 2025-12-13 15:24:22,875 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py:265] => The 'logprobs' parameter is incompatible with Speculative Decoding and has been disabled.
[36m(WorkerDict pid=375753)[0m kwargs: {'n': 1, 'max_tokens': 34816, 'repetition_penalty': 1.0, 'detokenize': False, 'temperature': 0.9, 'top_k': -1, 'top_p': 1.0, 'ignore_eos': False}
[36m(TaskRunner pid=348037)[0m Saving tensorboard log to tensorboard_log/verl_grpo_example_deepscaler/qwen3_32b_verl_true_weights.
[36m(TaskRunner pid=348037)[0m Checkpoint tracker file does not exist: /workspace/cann-recipes-train/llm_rl/qwen3/checkpoints/verl_grpo_example_deepscaler/qwen3_32b_verl_true_weights/latest_checkpointed_iteration.txt
[36m(TaskRunner pid=348037)[0m Training from scratch
[36m(WorkerDict pid=375740)[0m INFO 12-13 15:24:33 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375743)[0m INFO 12-13 15:24:33 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375752)[0m INFO 12-13 15:24:33 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375749)[0m INFO 12-13 15:24:33 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375740)[0m WARNING 12-13 15:24:33 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[36m(WorkerDict pid=375749)[0m WARNING 12-13 15:24:33 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[36m(WorkerDict pid=375742)[0m INFO 12-13 15:24:34 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375734)[0m INFO 12-13 15:24:34 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375754)[0m INFO 12-13 15:24:34 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375744)[0m INFO 12-13 15:24:35 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375744)[0m WARNING 12-13 15:24:35 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[36m(WorkerDict pid=375753)[0m INFO 12-13 15:24:35 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375758)[0m INFO 12-13 15:24:35 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375758)[0m WARNING 12-13 15:24:35 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[36m(WorkerDict pid=375742)[0m WARNING 12-13 15:24:35 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[36m(WorkerDict pid=375745)[0m INFO 12-13 15:24:35 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375750)[0m INFO 12-13 15:24:35 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375750)[0m WARNING 12-13 15:24:35 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[36m(WorkerDict pid=375745)[0m WARNING 12-13 15:24:35 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[36m(WorkerDict pid=375754)[0m WARNING 12-13 15:24:35 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[36m(WorkerDict pid=375747)[0m INFO 12-13 15:24:35 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375747)[0m WARNING 12-13 15:24:35 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[36m(WorkerDict pid=375752)[0m WARNING 12-13 15:24:35 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[36m(WorkerDict pid=375748)[0m INFO 12-13 15:24:36 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375734)[0m WARNING 12-13 15:24:36 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[36m(WorkerDict pid=375746)[0m INFO 12-13 15:24:36 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375748)[0m WARNING 12-13 15:24:36 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[36m(WorkerDict pid=375746)[0m WARNING 12-13 15:24:36 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[36m(WorkerDict pid=375751)[0m INFO 12-13 15:24:36 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375753)[0m WARNING 12-13 15:24:36 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[36m(WorkerDict pid=375743)[0m WARNING 12-13 15:24:36 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[36m(WorkerDict pid=375751)[0m WARNING 12-13 15:24:36 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[36m(WorkerDict pid=375740)[0m ninja: no work to do.
[36m(WorkerDict pid=375752)[0m ninja: no work to do.
[36m(WorkerDict pid=375752)[0m [Rank 15 | Local Rank 0] 2025-12-13 16:05:17,890 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/actor/megatron_actor.py:328] => For memory-efficient computation, enable fused kernels via `actor_rollout_ref.model.use_fused_kernels=True`. The current `clone()` operation ensures correctness but increases memory usage.
[36m(WorkerDict pid=375751)[0m [Rank 11 | Local Rank 0] 2025-12-13 16:05:17,889 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/actor/megatron_actor.py:328] => For memory-efficient computation, enable fused kernels via `actor_rollout_ref.model.use_fused_kernels=True`. The current `clone()` operation ensures correctness but increases memory usage.
[36m(WorkerDict pid=375750)[0m [Rank 10 | Local Rank 0] 2025-12-13 16:05:17,890 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/actor/megatron_actor.py:328] => For memory-efficient computation, enable fused kernels via `actor_rollout_ref.model.use_fused_kernels=True`. The current `clone()` operation ensures correctness but increases memory usage.
[36m(WorkerDict pid=375748)[0m [Rank 8 | Local Rank 0] 2025-12-13 16:05:17,895 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/actor/megatron_actor.py:328] => For memory-efficient computation, enable fused kernels via `actor_rollout_ref.model.use_fused_kernels=True`. The current `clone()` operation ensures correctness but increases memory usage.
[36m(WorkerDict pid=375749)[0m [Rank 9 | Local Rank 0] 2025-12-13 16:05:17,893 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/actor/megatron_actor.py:328] => For memory-efficient computation, enable fused kernels via `actor_rollout_ref.model.use_fused_kernels=True`. The current `clone()` operation ensures correctness but increases memory usage.
[36m(WorkerDict pid=375753)[0m [Rank 14 | Local Rank 0] 2025-12-13 16:05:17,891 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/actor/megatron_actor.py:328] => For memory-efficient computation, enable fused kernels via `actor_rollout_ref.model.use_fused_kernels=True`. The current `clone()` operation ensures correctness but increases memory usage.
[36m(WorkerDict pid=375754)[0m [Rank 13 | Local Rank 0] 2025-12-13 16:05:17,897 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/actor/megatron_actor.py:328] => For memory-efficient computation, enable fused kernels via `actor_rollout_ref.model.use_fused_kernels=True`. The current `clone()` operation ensures correctness but increases memory usage.
[36m(WorkerDict pid=375758)[0m [Rank 12 | Local Rank 0] 2025-12-13 16:05:17,892 WARNING [/workspace/cann-recipes-train/llm_rl/qwen3/verl/workers/actor/megatron_actor.py:328] => For memory-efficient computation, enable fused kernels via `actor_rollout_ref.model.use_fused_kernels=True`. The current `clone()` operation ensures correctness but increases memory usage.
[36m(WorkerDict pid=375754)[0m ninja: no work to do.
[36m(TaskRunner pid=348037)[0m Dumped generations to /workspace/data/dump/1.jsonl
[36m(TaskRunner pid=348037)[0m step:1 - actor/entropy:0.25305086374282837 - actor/kl_loss:-2.3983833144435333e-10 - actor/kl_coef:0.0010000000000000002 - actor/pg_loss:-0.0015798468687912312 - actor/pg_clipfrac:0.0 - actor/ppo_kl:-4.5308364138347523e-11 - actor/pg_clipfrac_lower:0.0 - actor/grad_norm:0.02568925674542942 - perf/mfu/actor:0.40370956749869125 - perf/max_memory_allocated_gb:47.246397972106934 - perf/max_memory_reserved_gb:47.375 - perf/cpu_memory_used_gb:1011.8418006896973 - actor/lr:1e-06 - training/global_step:1 - training/epoch:0 - critic/score/mean:0.908203125 - critic/score/max:1.0 - critic/score/min:0.0 - critic/rewards/mean:0.908203125 - critic/rewards/max:1.0 - critic/rewards/min:0.0 - critic/advantages/mean:0.0016146909911185503 - critic/advantages/max:0.8539109230041504 - critic/advantages/min:-1.6770472526550293 - critic/returns/mean:0.0016146909911185503 - critic/returns/max:0.8539109230041504 - critic/returns/min:-1.6770472526550293 - response_length/mean:5776.513671875 - response_length/max:34816.0 - response_length/min:985.0 - response_length/clip_ratio:0.009765625 - response_length_non_aborted/mean:5776.513671875 - response_length_non_aborted/max:34816.0 - response_length_non_aborted/min:985.0 - response_length_non_aborted/clip_ratio:0.009765625 - response/aborted_ratio:0.0 - prompt_length/mean:78.15625 - prompt_length/max:191.0 - prompt_length/min:34.0 - prompt_length/clip_ratio:0.0 - timing_s/start_profile:0.0003745919093489647 - timing_s/generate_sequences:2412.23193359375 - timing_s/generation_timing/max:2413.6298828125 - timing_s/generation_timing/min:2410.863525390625 - timing_s/generation_timing/topk_ratio:0.125 - timing_s/gen:2443.715494927019 - timing_s/reward:0.8871983319986612 - timing_s/old_log_prob:118.25098190992139 - timing_s/ref:108.34448423795402 - timing_s/adv:0.3574728991370648 - timing_s/update_actor:360.31887704809196 - timing_s/dump_rollout_generations:2.0172869791276753 - timing_s/step:3033.9088543490507 - timing_s/stop_profile:7.829000242054462e-05 - timing_per_token_ms/adv:0.00011925339352068538 - timing_per_token_ms/gen:0.8262564752971671 - timing_per_token_ms/update_actor:0.1202028152099776 - timing_per_token_ms/ref:0.0361438515921465 - perf/total_num_tokens:2997591 - perf/time_per_step:3033.9088543490507 - perf/throughput:61.75183451257546
[36m(WorkerDict pid=375745)[0m INFO 12-13 16:15:08 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375740)[0m INFO 12-13 16:15:08 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375743)[0m INFO 12-13 16:15:08 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375752)[0m INFO 12-13 16:15:08 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375754)[0m INFO 12-13 16:15:09 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375744)[0m INFO 12-13 16:15:09 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375758)[0m INFO 12-13 16:15:09 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375753)[0m INFO 12-13 16:15:09 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375750)[0m INFO 12-13 16:15:10 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375749)[0m INFO 12-13 16:15:11 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375747)[0m INFO 12-13 16:15:11 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375751)[0m INFO 12-13 16:15:11 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375742)[0m INFO 12-13 16:15:12 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375734)[0m INFO 12-13 16:15:12 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375748)[0m INFO 12-13 16:15:12 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375746)[0m INFO 12-13 16:15:12 [block_pool.py:378] Successfully reset prefix cache
[36m(TaskRunner pid=348037)[0m Dumped generations to /workspace/data/dump/2.jsonl
[36m(TaskRunner pid=348037)[0m step:2 - actor/entropy:0.2549678087234497 - actor/kl_loss:0.0009091913097905984 - actor/kl_coef:0.0010000000000000002 - actor/pg_loss:-0.0010577174461228243 - actor/pg_clipfrac:0.0 - actor/ppo_kl:-7.412158782170347e-10 - actor/pg_clipfrac_lower:0.0 - actor/grad_norm:0.015846343354729783 - perf/mfu/actor:0.40244165593798015 - perf/max_memory_allocated_gb:47.873727798461914 - perf/max_memory_reserved_gb:48.01953125 - perf/cpu_memory_used_gb:1021.5050010681152 - actor/lr:1e-06 - training/global_step:2 - training/epoch:1 - critic/score/mean:0.9140625 - critic/score/max:1.0 - critic/score/min:0.0 - critic/rewards/mean:0.9140625 - critic/rewards/max:1.0 - critic/rewards/min:0.0 - critic/advantages/mean:0.0009553040144965053 - critic/advantages/max:1.0978854894638062 - critic/advantages/min:-2.015559434890747 - critic/returns/mean:0.0009553040144965053 - critic/returns/max:1.0978854894638062 - critic/returns/min:-2.015559434890747 - response_length/mean:5734.072265625 - response_length/max:34816.0 - response_length/min:916.0 - response_length/clip_ratio:0.00390625 - response_length_non_aborted/mean:5734.072265625 - response_length_non_aborted/max:34816.0 - response_length_non_aborted/min:916.0 - response_length_non_aborted/clip_ratio:0.00390625 - response/aborted_ratio:0.0 - prompt_length/mean:78.15625 - prompt_length/max:191.0 - prompt_length/min:34.0 - prompt_length/clip_ratio:0.0 - timing_s/start_profile:0.0004179819952696562 - timing_s/generate_sequences:2338.47802734375 - timing_s/generation_timing/max:2340.267333984375 - timing_s/generation_timing/min:2336.7431640625 - timing_s/generation_timing/topk_ratio:0.0625 - timing_s/gen:2364.0681638119277 - timing_s/reward:0.9670183679554611 - timing_s/old_log_prob:113.38400065083988 - timing_s/ref:104.84151498111896 - timing_s/adv:0.26587690808810294 - timing_s/update_actor:354.8935557389632 - timing_s/dump_rollout_generations:1.9916060611139983 - timing_s/step:2940.420103024924 - timing_s/stop_profile:7.298006676137447e-05 - timing_per_token_ms/adv:8.934453191466367e-05 - timing_per_token_ms/gen:0.8052428393910195 - timing_per_token_ms/update_actor:0.11925743700359767 - timing_per_token_ms/ref:0.035230649207445834 - perf/total_num_tokens:2975861 - perf/time_per_step:2940.420103024924 - perf/throughput:63.25331278638162
[36m(WorkerDict pid=375743)[0m INFO 12-13 17:04:10 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375740)[0m INFO 12-13 17:04:10 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375749)[0m INFO 12-13 17:04:10 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375745)[0m INFO 12-13 17:04:10 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375752)[0m INFO 12-13 17:04:10 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375754)[0m INFO 12-13 17:04:11 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375744)[0m INFO 12-13 17:04:11 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375758)[0m INFO 12-13 17:04:11 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375753)[0m INFO 12-13 17:04:12 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375742)[0m INFO 12-13 17:04:14 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375751)[0m INFO 12-13 17:04:14 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375747)[0m INFO 12-13 17:04:14 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375748)[0m INFO 12-13 17:04:14 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375746)[0m INFO 12-13 17:04:15 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375750)[0m INFO 12-13 17:04:15 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375734)[0m INFO 12-13 17:04:15 [block_pool.py:378] Successfully reset prefix cache
[36m(TaskRunner pid=348037)[0m Dumped generations to /workspace/data/dump/3.jsonl
[36m(TaskRunner pid=348037)[0m step:3 - actor/entropy:0.25454431772232056 - actor/kl_loss:0.0009163867560192853 - actor/kl_coef:0.0010000000000000002 - actor/pg_loss:0.0026237578523984873 - actor/pg_clipfrac:0.0 - actor/ppo_kl:9.50715648275142e-12 - actor/pg_clipfrac_lower:0.0 - actor/grad_norm:0.02887606196274927 - perf/mfu/actor:0.4090767583263936 - perf/max_memory_allocated_gb:47.87722396850586 - perf/max_memory_reserved_gb:48.0390625 - perf/cpu_memory_used_gb:1022.7907447814941 - actor/lr:1e-06 - training/global_step:3 - training/epoch:2 - critic/score/mean:0.90625 - critic/score/max:1.0 - critic/score/min:0.0 - critic/rewards/mean:0.90625 - critic/rewards/max:1.0 - critic/rewards/min:0.0 - critic/advantages/mean:-0.002743752673268318 - critic/advantages/max:1.2499974966049194 - critic/advantages/min:-2.561730146408081 - critic/returns/mean:-0.002743752673268318 - critic/returns/max:1.2499974966049194 - critic/returns/min:-2.561730146408081 - response_length/mean:5854.453125 - response_length/max:34816.0 - response_length/min:959.0 - response_length/clip_ratio:0.009765625 - response_length_non_aborted/mean:5854.453125 - response_length_non_aborted/max:34816.0 - response_length_non_aborted/min:959.0 - response_length_non_aborted/clip_ratio:0.009765625 - response/aborted_ratio:0.0 - prompt_length/mean:78.15625 - prompt_length/max:191.0 - prompt_length/min:34.0 - prompt_length/clip_ratio:0.0 - timing_s/start_profile:0.000749293016269803 - timing_s/generate_sequences:2465.55810546875 - timing_s/generation_timing/max:2467.947509765625 - timing_s/generation_timing/min:2463.295166015625 - timing_s/generation_timing/topk_ratio:0.125 - timing_s/gen:2494.811162999831 - timing_s/reward:0.9308732750359923 - timing_s/old_log_prob:116.54668432613835 - timing_s/ref:108.6811447229702 - timing_s/adv:0.16687801480293274 - timing_s/update_actor:362.0583582317922 - timing_s/dump_rollout_generations:2.0006985359359533 - timing_s/step:3085.204071372049 - timing_s/stop_profile:8.066999725997448e-05 - timing_per_token_ms/adv:5.4939336480750174e-05 - timing_per_token_ms/gen:0.8323028553984785 - timing_per_token_ms/update_actor:0.11919632428546152 - timing_per_token_ms/ref:0.03577984784933715 - perf/total_num_tokens:3037496 - perf/time_per_step:3085.204071372049 - perf/throughput:61.53353088101332
[36m(WorkerDict pid=375743)[0m INFO 12-13 17:55:37 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375752)[0m INFO 12-13 17:55:37 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375754)[0m INFO 12-13 17:55:37 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375745)[0m INFO 12-13 17:55:37 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375740)[0m INFO 12-13 17:55:37 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375744)[0m INFO 12-13 17:55:38 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375749)[0m INFO 12-13 17:55:38 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375753)[0m INFO 12-13 17:55:38 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375758)[0m INFO 12-13 17:55:38 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375734)[0m INFO 12-13 17:55:41 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375746)[0m INFO 12-13 17:55:41 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375751)[0m INFO 12-13 17:55:41 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375742)[0m INFO 12-13 17:55:41 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375747)[0m INFO 12-13 17:55:41 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375750)[0m INFO 12-13 17:55:41 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375748)[0m INFO 12-13 17:55:41 [block_pool.py:378] Successfully reset prefix cache
[36m(TaskRunner pid=348037)[0m Dumped generations to /workspace/data/dump/4.jsonl
[36m(TaskRunner pid=348037)[0m step:4 - actor/entropy:0.25227051973342896 - actor/kl_loss:0.000904944381767475 - actor/kl_coef:0.0010000000000000002 - actor/pg_loss:-0.0005156928105506955 - actor/pg_clipfrac:0.0 - actor/ppo_kl:-9.753782799909885e-10 - actor/pg_clipfrac_lower:0.0 - actor/grad_norm:0.024656168221083036 - perf/mfu/actor:0.4053846873876581 - perf/max_memory_allocated_gb:47.88142395019531 - perf/max_memory_reserved_gb:48.0390625 - perf/cpu_memory_used_gb:1030.079174041748 - actor/lr:1e-06 - training/global_step:4 - training/epoch:3 - critic/score/mean:0.908203125 - critic/score/max:1.0 - critic/score/min:0.0 - critic/rewards/mean:0.908203125 - critic/rewards/max:1.0 - critic/rewards/min:0.0 - critic/advantages/mean:0.0005127850454300642 - critic/advantages/max:0.9682439565658569 - critic/advantages/min:-3.7499847412109375 - critic/returns/mean:0.0005127850454300642 - critic/returns/max:0.9682439565658569 - critic/returns/min:-3.7499847412109375 - response_length/mean:5773.951171875 - response_length/max:34816.0 - response_length/min:860.0 - response_length/clip_ratio:0.0078125 - response_length_non_aborted/mean:5773.951171875 - response_length_non_aborted/max:34816.0 - response_length_non_aborted/min:860.0 - response_length_non_aborted/clip_ratio:0.0078125 - response/aborted_ratio:0.0 - prompt_length/mean:78.15625 - prompt_length/max:191.0 - prompt_length/min:34.0 - prompt_length/clip_ratio:0.0 - timing_s/start_profile:0.000508622033521533 - timing_s/generate_sequences:2370.098388671875 - timing_s/generation_timing/max:2372.25537109375 - timing_s/generation_timing/min:2368.012939453125 - timing_s/generation_timing/topk_ratio:0.125 - timing_s/gen:2401.61453688005 - timing_s/reward:0.9284361249301583 - timing_s/old_log_prob:117.20182203804143 - timing_s/ref:108.4032236658968 - timing_s/adv:0.044318462954834104 - timing_s/update_actor:360.9351098621264 - timing_s/dump_rollout_generations:1.9803860730025917 - timing_s/step:2991.117342836922 - timing_s/stop_profile:0.0001024911180138588 - timing_per_token_ms/adv:1.4791166962367024e-05 - timing_per_token_ms/gen:0.8123818946014106 - timing_per_token_ms/update_actor:0.12046111522395825 - timing_per_token_ms/ref:0.03617928225839343 - perf/total_num_tokens:2996279 - perf/time_per_step:2991.117342836922 - perf/throughput:62.60785386720616
[36m(WorkerDict pid=375740)[0m INFO 12-13 18:45:31 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375749)[0m INFO 12-13 18:45:31 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375754)[0m INFO 12-13 18:45:31 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375743)[0m INFO 12-13 18:45:31 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375752)[0m INFO 12-13 18:45:32 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375745)[0m INFO 12-13 18:45:32 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375747)[0m INFO 12-13 18:45:32 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375744)[0m INFO 12-13 18:45:32 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375753)[0m INFO 12-13 18:45:33 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375758)[0m INFO 12-13 18:45:33 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375751)[0m INFO 12-13 18:45:35 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375742)[0m INFO 12-13 18:45:35 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375748)[0m INFO 12-13 18:45:35 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375734)[0m INFO 12-13 18:45:35 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375746)[0m INFO 12-13 18:45:35 [block_pool.py:378] Successfully reset prefix cache
[36m(WorkerDict pid=375750)[0m INFO 12-13 18:45:35 [block_pool.py:378] Successfully reset prefix cache
[36m(TaskRunner pid=348037)[0m Dumped generations to /workspace/data/dump/5.jsonl
[36m(TaskRunner pid=348037)[0m step:5 - actor/entropy:0.2528623342514038 - actor/kl_loss:0.0009074063644834222 - actor/kl_coef:0.0010000000000000002 - actor/pg_loss:0.0016869380845721945 - actor/pg_clipfrac:0.0 - actor/ppo_kl:2.575387866695564e-11 - actor/pg_clipfrac_lower:0.0 - actor/grad_norm:0.02586604532024688 - perf/mfu/actor:0.40575694379103344 - perf/max_memory_allocated_gb:47.88142395019531 - perf/max_memory_reserved_gb:48.0390625 - perf/cpu_memory_used_gb:1034.4334564208984 - actor/lr:1e-06 - training/global_step:5 - training/epoch:4 - critic/score/mean:0.919921875 - critic/score/max:1.0 - critic/score/min:0.0 - critic/rewards/mean:0.919921875 - critic/rewards/max:1.0 - critic/rewards/min:0.0 - critic/advantages/mean:-0.0016389766242355108 - critic/advantages/max:0.7499985098838806 - critic/advantages/min:-3.7499847412109375 - critic/returns/mean:-0.0016389766242355108 - critic/returns/max:0.7499985098838806 - critic/returns/min:-3.7499847412109375 - response_length/mean:5830.390625 - response_length/max:34816.0 - response_length/min:947.0 - response_length/clip_ratio:0.0078125 - response_length_non_aborted/mean:5830.390625 - response_length_non_aborted/max:34816.0 - response_length_non_aborted/min:947.0 - response_length_non_aborted/clip_ratio:0.0078125 - response/aborted_ratio:0.0 - prompt_length/mean:78.15625 - prompt_length/max:191.0 - prompt_length/min:34.0 - prompt_length/clip_ratio:0.0 - timing_s/start_profile:0.000562052009627223 - timing_s/generate_sequences:2435.6103515625 - timing_s/generation_timing/max:2437.71484375 - timing_s/generation_timing/min:2433.416259765625 - timing_s/generation_timing/topk_ratio:0.125 - timing_s/gen:2469.6699847490527 - timing_s/reward:0.9874934579711407 - timing_s/old_log_prob:119.51065301918425 - timing_s/ref:113.70895282411948 - timing_s/adv:0.260988850845024 - timing_s/update_actor:364.4425419860054 - timing_s/dump_rollout_generations:1.9933389897923917 - timing_s/step:3070.582186840009 - timing_s/stop_profile:8.787005208432674e-05 - timing_per_token_ms/adv:8.627228658597846e-05 - timing_per_token_ms/gen:0.82731578365952 - timing_per_token_ms/update_actor:0.12046986422806652 - timing_per_token_ms/ref:0.03758754955880897 - perf/total_num_tokens:3025176 - perf/time_per_step:3070.582186840009 - perf/throughput:61.57578221170459
[36m(TaskRunner pid=348037)[0m 'Final validation metrics: None'
